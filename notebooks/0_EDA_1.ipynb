{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fb9adfb-b1a4-4951-9e72-dfa83a8f2253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Credit Card Churn Analysis\n",
    "---  \n",
    "\n",
    "## Project Description \n",
    "In this project, I will be working with a dataset provided by **Kaggle**, where I will develop a churn-rate analysis. The goal is to identify the causes and reasons for customer churn from a banking institution in relation to credit card services. After understanding these causes and reasons, some machine learning models will be developed to predict potential customers who will be abandoning the credit card service of this institution. With these predictions, I will seek to develop solutions to prevent or reverse the churn of these customers.  \n",
    "\n",
    "---  \n",
    "\n",
    "### CRISP-DM Methodology  \n",
    "The project will follow the CRISP-DM (*Cross-Industry Standard Process for Data Mining*) framework:  \n",
    "\n",
    "| **Stage** | **Objective** | **Key Actions** |  \n",
    "|-----------|---------------|------------------|  \n",
    "| **1. Business Understanding** | Define the impact of churn prediction on customer retention. | - Identify costs of false negatives.<br>- Align metrics with business KPIs. |  \n",
    "| **2. Data Understanding** | Analyze data structure, quality, and variable relationships. | - Exploratory Data Analysis (EDA).<br>- Outlier and correlation detection. |  \n",
    "| **3. Data Preparation** | Prepare data for model training. | - Split training and test data.<br>- Remove redundant variables. |  \n",
    "| **4. Modeling** | Train and compare classical models and neural networks. | - Random Forest/Logistic Regression (baseline).<br>- PyTorch neural network (focus on generalization). |  \n",
    "| **5. Evaluation** | Validate performance with business-oriented metrics. | - AUC-ROC, confusion matrix.<br>- Simulate financial impact. |  \n",
    "| **6. Deployment** | Deploy the model for production use. | - Build a final churn prediction model with customer behavior indicators. |  \n",
    "\n",
    "*This notebook covers the Business Understanding, Data Understanding, and Data Preparation.*  \n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31754a4-5703-476b-abd9-71eaf357e584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d1172b-be9d-4df7-b86d-f69f2525d410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data analizing/preparation:\n",
    "# SRC/Data\n",
    "import sys\n",
    "sys.path.append('./src/data.py')\n",
    "# PySpark.SQL\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Numpy \n",
    "import numpy as np\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Sklearning features\n",
    "from sklearn.feature_selection import chi2\n",
    "# Scipy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Graphics:\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "417026e1-48ae-44a3-9ff2-01f8e65c0fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56443ea3-2aa3-434a-b5b0-49e41d02f26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Dataset Manipulation in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b23ae1e-06b6-4835-beec-9cb7f70d4ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataSpark:\n",
    "\n",
    "    # Init\n",
    "    def __init__(\n",
    "        self, \n",
    "        file_location: str,\n",
    "        dataframe = False, \n",
    "    ):\n",
    "  \n",
    "        try:\n",
    "            if dataframe: \n",
    "                # Checks if it is a PySpark DataFrame\n",
    "                if not isinstance(dataframe, DataFrame):\n",
    "                    raise TypeError('The input must be a PySpark DataFrame.')\n",
    "\n",
    "                # Check if the DataFrame is empty\n",
    "                if dataframe.rdd.isEmpty():\n",
    "                    raise ValueError('The provided DataFrame is empty.')\n",
    "\n",
    "                self.dataframe = dataframe\n",
    "            \n",
    "            self.file_location = file_location\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load DataFrame {dataframe} or file path {file_location}: {e}\")\n",
    "\n",
    "    # Save Data\n",
    "    def save_data(\n",
    "        self,\n",
    "        file_type: str = 'parquet',\n",
    "        mode: str = 'overwrite',\n",
    "        delimiter: str = ',',\n",
    "        header: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves the DataFrame to disk at the path defined by `self.file_location`.\n",
    "\n",
    "        Supports saving as CSV or Parquet. The output format is determined by the `file_type` argument.\n",
    "        This method uses the PySpark DataFrameWriter with the specified options.\n",
    "\n",
    "        Args:\n",
    "            file_type (str, optional): Format to save the file. Must be 'csv' or 'parquet'. Default is 'parquet'.\n",
    "            mode (str, optional): Write mode, such as 'overwrite', 'append', 'ignore', or 'error'. Default is 'overwrite'.\n",
    "            delimiter (str, optional): Field delimiter to use when writing CSV files. Default is ','.\n",
    "            header (bool, optional): Whether to include a header row in CSV files. Default is True.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unsupported file type is provided.\n",
    "            Exception: If the save operation fails for any other reason.\n",
    "\n",
    "        Example:\n",
    "            >>> spark_df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "            >>> ds = DataSpark(file_location='/tmp/output/', dataframe=spark_df)\n",
    "            >>> ds.save_data(file_type='csv')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Mode\n",
    "            writer = self.dataframe.write.mode(mode)\n",
    "\n",
    "            # Parquet file\n",
    "            if file_type == 'parquet':\n",
    "                writer = writer.format('parquet')\n",
    "\n",
    "            # CSV File\n",
    "            elif file_type == 'csv':\n",
    "                writer = writer.format('csv') \\\n",
    "                    .option('delimiter', delimiter) \\\n",
    "                    .option('header', str(header)) \\\n",
    "                    .option('encoding', 'UTF-8') \\\n",
    "                    .option('escape', '\"') \\\n",
    "                    .option('multiline', 'true')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type '{file_type}'. Use 'csv' or 'parquet'.\")\n",
    "\n",
    "            writer.save(self.file_location)\n",
    "            print(f\"✅ Data saved successfully to: {self.file_location}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Unable to save data to '{self.file_location}': {e}\")\n",
    "\n",
    "    # Load Data\n",
    "    def load_data(\n",
    "        self,\n",
    "        spark = spark,\n",
    "        file_type: str = 'csv',\n",
    "        infer_schema: bool = True,\n",
    "        header: bool = True,\n",
    "        delimiter: str = ',',\n",
    "        encoding: str = 'UTF-8',\n",
    "        multiline: bool = True,\n",
    "        escape: str = '\"'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Loads data from disk into `self.dataframe` using the specified format and options.\n",
    "\n",
    "        Supports loading from CSV or Parquet files. Uses the path defined in `self.file_location`.\n",
    "        For CSV files, several options like schema inference, delimiter, and multiline reading are configurable.\n",
    "\n",
    "        Args:\n",
    "            spark (SparkSession): The active Spark session used to read the data.\n",
    "            file_type (str, optional): The format of the input file. Options are 'csv' or 'parquet'. Default is 'csv'.\n",
    "            infer_schema (bool, optional): Whether to infer the schema when reading CSV files. Default is True.\n",
    "            header (bool, optional): Whether the CSV file has a header row. Default is True.\n",
    "            delimiter (str, optional): Field delimiter for CSV files. Default is ','.\n",
    "            encoding (str, optional): Character encoding of the file. Default is 'UTF-8'.\n",
    "            multiline (bool, optional): Whether to support multiline fields in CSV. Default is True.\n",
    "            escape (str, optional): Character used to escape quotes in CSV. Default is '\"'.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A PySpark DataFrame containing the loaded data.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unsupported file type is provided.\n",
    "            FileNotFoundError: If the file does not exist at the specified location.\n",
    "            Exception: For any other issues during file loading.\n",
    "\n",
    "        Example:\n",
    "            >>> ds = DataSpark(file_location='/tmp/data.csv')\n",
    "            >>> df = ds.load_data(spark=spark, file_type='csv')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file_type == 'csv':\n",
    "                df = spark.read.format('csv') \\\n",
    "                    .option('inferSchema', str(infer_schema)) \\\n",
    "                    .option('header', str(header)) \\\n",
    "                    .option('delimiter', delimiter) \\\n",
    "                    .option('encoding', encoding) \\\n",
    "                    .option('multiline', str(multiline)) \\\n",
    "                    .option('escape', escape) \\\n",
    "                    .load(self.file_location)\n",
    "\n",
    "            elif file_type == 'parquet':\n",
    "                df = spark.read.format('parquet').load(self.file_location)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type '{file_type}'. Use 'csv' or 'parquet'.\")\n",
    "\n",
    "            print(f'✅ File loaded successfully from: {self.file_location}')\n",
    "            self.dataframe = df\n",
    "            return self.dataframe\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[ERROR] File not found at: '{self.file_location}'\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"[ERROR] {ve}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error loading file '{self.file_location}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbb6e132-029c-4231-b349-4e9985b7c4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a08e93-1ba4-4447-9c9f-bb7bf4150976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GraphicsData:\n",
    "\n",
    "    # Init\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: pd.DataFrame,\n",
    "        ):\n",
    "\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if data.empty:\n",
    "                raise ValueError('The provided DataFrame is empty.')\n",
    "\n",
    "            self.data = data\n",
    "\n",
    "        except Exception  as e:\n",
    "            print(f'[Error] Failed to load Dataframe : {str(e)}')\n",
    "    \n",
    "\n",
    "    ###_initializer_subplot_grid\n",
    "    def _initializer_subplot_grid(\n",
    "        self, \n",
    "        num_columns, \n",
    "        figsize_per_row\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes and returns a standardized matplotlib subplot grid layout.\n",
    "\n",
    "        This utility method calculates the required number of rows based on \n",
    "        the number of variables in the dataset and the desired number of \n",
    "        columns per row. It then creates a grid of subplots accordingly and \n",
    "        applies a consistent styling.\n",
    "\n",
    "        Args:\n",
    "            num_columns (int): Number of subplots per row.\n",
    "            figsize_per_row (int): Vertical size (height) per row in the final figure.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - fig (matplotlib.figure.Figure): The full matplotlib figure object.\n",
    "                - ax (np.ndarray of matplotlib.axes._subplots.AxesSubplot): Flattened array of subplot axes.\n",
    "        \"\"\"\n",
    "        num_vars = len(self.data.columns)\n",
    "        num_rows = (num_vars + num_columns - 1) // num_columns\n",
    "\n",
    "        plt.rc('font', size = 12)\n",
    "        fig, ax = plt.subplots(num_rows, num_columns, figsize = (30, num_rows * figsize_per_row))\n",
    "        ax = ax.flatten()\n",
    "        sns.set(style = 'whitegrid')\n",
    "\n",
    "        return fig, ax\n",
    "\n",
    "    ###_finalize_subplot_layout\n",
    "    def _finalize_subplot_layout(\n",
    "        self,\n",
    "        fig,\n",
    "        ax,\n",
    "        i: int,\n",
    "        title: str = None,\n",
    "        fontsize: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finalizes and displays a matplotlib figure by adjusting layout and removing unused subplots.\n",
    "\n",
    "        This method is used after plotting multiple subplots to:\n",
    "        - Remove any unused axes in the grid.\n",
    "        - Set a central title for the entire figure.\n",
    "        - Automatically adjust spacing and layout for better readability.\n",
    "        - Display the resulting plot.\n",
    "\n",
    "        Args:\n",
    "            fig (matplotlib.figure.Figure): The matplotlib figure object containing the subplots.\n",
    "            ax (np.ndarray of matplotlib.axes.Axes): Array of axes (flattened) for all subplots.\n",
    "            i (int): Index of the last used subplot (all subplots after this will be removed).\n",
    "            title (str, optional): Title to be displayed at the top of the entire figure.\n",
    "            fontsize (int, optional): Font size of the overall title. Default is 30.\n",
    "        \"\"\"\n",
    "        for j in range(i + 1, len(ax)):\n",
    "                fig.delaxes(ax[j])\n",
    "        \n",
    "        plt.suptitle(title, fontsize = fontsize, fontweight = 'bold')\n",
    "        plt.tight_layout(rect = [0, 0, 1, 0.97])\n",
    "        plt.show()\n",
    "    \n",
    "    ###_format_single_ax\n",
    "    def _format_single_ax(\n",
    "        self, \n",
    "        ax,\n",
    "        title: str = None,\n",
    "        fontsize: int = 20,\n",
    "        linewidth: float = 0.9\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Applies standard formatting to a single subplot axis.\n",
    "\n",
    "        This method configures a single axis by:\n",
    "        - Setting the title with specified font size and bold style.\n",
    "        - Hiding the x and y axis labels.\n",
    "        - Adding dashed grid lines for both axes with configurable line width.\n",
    "\n",
    "        Args:\n",
    "            ax (matplotlib.axes.Axes): The axis to be formatted.\n",
    "            title (str, optional): Title text for the axis. Defaults to None.\n",
    "            fontsize (int, optional): Font size for the title. Defaults to 20.\n",
    "            linewidth (float, optional): Width of the dashed grid lines. Defaults to 0.9.\n",
    "        \"\"\"\n",
    "        ax.set_title(title, fontsize = fontsize, fontweight = 'bold')\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_ylabel(None)\n",
    "        ax.grid(axis = 'y', which = 'major', linestyle = '--', linewidth = linewidth)\n",
    "        ax.grid(axis = 'x', which = 'major', linestyle = '--', linewidth = linewidth)\n",
    "\n",
    "    ### Plot Variable Type\n",
    "    def plot_variable_type(\n",
    "        self,\n",
    "        count_col: str,\n",
    "        label_col: str, \n",
    "        title = 'Distribution of Variable Types'\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Plots a pie chart to display the proportion of each variable type in the dataset.\n",
    "\n",
    "        This method uses a pie chart to visualize the distribution of different types of variables\n",
    "        (e.g., categorical, numerical) based on the values provided in `count_col` and `label_col`.\n",
    "\n",
    "        Args:\n",
    "            count_col (str): Name of the column containing the counts for each variable type.\n",
    "            label_col (str): Name of the column containing the labels/categories of variable types.\n",
    "            title (str, optional): Title of the pie chart. Defaults to 'Distribution of Variable Types'.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `count_col` or `label_col` is not found in the DataFrame.\n",
    "            Exception: For any other error that occurs during plotting.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if count_col not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{count_col}' does not exist in the DataFrame.\")\n",
    "\n",
    "            if label_col not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{label_col}' does not exist in the DataFrame.\")\n",
    "\n",
    "            # Define AX and Fig\n",
    "            plt.rc('font', size = 14)\n",
    "            fig, ax = plt.subplots(figsize = (7, 7))\n",
    "\n",
    "            ax.pie(\n",
    "                self.data[count_col],\n",
    "                labels = self.data[label_col],\n",
    "                colors = sns.color_palette('Set3', len(self.data)),\n",
    "                autopct = '%1.1f%%',\n",
    "                startangle = 120,\n",
    "                explode=[0.05 if i >= len(self.data) - 2 else 0 for i in range(len(self.data))],\n",
    "                shadow = False,\n",
    "            )\n",
    "            # Config Ax's and Show Graphics\n",
    "            ax.set_title(title, fontsize = 15, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception  as e:\n",
    "            print(f'[Error] Failed to generate variable distribution plot: {str(e)}')\n",
    "\n",
    "    ### Numerical histograms\n",
    "    def numerical_histograms(\n",
    "        self, \n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        color: str = '#a2bffe',\n",
    "        hue: str = None,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        title: str = 'Histograms of Numerical Variables',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots histograms with KDE (Kernel Density Estimation) for all numerical columns in the dataset.\n",
    "\n",
    "        Optionally groups the histograms by a categorical target variable using different colors (hue).\n",
    "        Useful for visualizing the distribution of numerical features and how they differ between groups.\n",
    "\n",
    "        Args:\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height of each row in inches (controls vertical spacing).\n",
    "            color (str): Default color for histograms when `hue` is not specified.\n",
    "            hue (str, optional): Name of the column used for grouping (e.g., 'churn_target'). Must be categorical.\n",
    "            palette (list): List of colors for hue levels. Only used if `hue` is provided.\n",
    "            title (str): Title of the entire figure layout.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If plotting fails due to missing columns, incorrect types, or rendering errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "                sns.histplot(\n",
    "                    data = self.data,\n",
    "                    x = column,\n",
    "                    kde = True,\n",
    "                    hue = hue,\n",
    "                    palette = palette if hue else None,\n",
    "                    edgecolor = 'black',\n",
    "                    alpha = 0.4 if hue else 0.7,\n",
    "                    color = None if hue else color,\n",
    "                    ax = ax[i],\n",
    "                )\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], title = f'Histogram of variable: {column}')\n",
    "                \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to generate numeric histograms: {str(e)}')\n",
    "\n",
    "    ### Numerical Boxplots\n",
    "    def numerical_boxplots(\n",
    "        self, \n",
    "        hue: str = None, \n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        color: str = '#a2bffe',\n",
    "        showfliers: bool = False,\n",
    "        title: str = 'Boxplots of Numerical Variables',\n",
    "        legend: list = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots boxplots for each numerical variable in the dataset.\n",
    "\n",
    "        Optionally groups the boxplots by a categorical hue variable (e.g., churn target), \n",
    "        allowing for comparison of distributions between groups. Helps identify outliers, \n",
    "        skewness, and variability in each feature.\n",
    "\n",
    "        Args:\n",
    "            hue (str, optional): Column name to group the boxplots (e.g., 'churn_target').\n",
    "                                If None, individual boxplots are created without grouping.\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height (in inches) of each row of plots.\n",
    "            palette (list): Color palette to use when `hue` is provided.\n",
    "            color (str): Single color to use when `hue` is not specified.\n",
    "            showfliers (bool): Whether to display outlier points in the boxplots (default: False).\n",
    "            title (str): Overall title for the subplot grid.\n",
    "            legend (list): Custom legend labels to replace default tick labels when `hue` is present.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the hue column is not found in the DataFrame.\n",
    "            Exception: If plotting fails due to unexpected issues.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not in the DataFrame.\")\n",
    "\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "                    sns.boxplot(\n",
    "                        data = self.data,\n",
    "                        x = hue if hue else column,\n",
    "                        y = column if hue else None,\n",
    "                        hue = hue if hue else None,\n",
    "                        palette = palette if hue else None,\n",
    "                        color = None if hue else color,\n",
    "                        showfliers = showfliers,\n",
    "                        legend = False,\n",
    "                        ax = ax[i]\n",
    "                    )\n",
    "\n",
    "                    # Config Ax's\n",
    "                    if len(legend) > 0:\n",
    "                        ax[i].set_xticks([l for l in range(0, len(legend))])\n",
    "                        ax[i].set_xticklabels(legend, fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "                    self._format_single_ax(ax[i], f'Box plot of variable: {column}')\n",
    "                    ax[i].set_yticklabels([])\n",
    "                    sns.despine(ax = ax[i], top = True, right = True, left = True, bottom = True)\n",
    "            \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e: \n",
    "            print(f'[ERROR] Failed to generate numerical boxplots: {str(e)}')\n",
    "\n",
    "    ### Categorical Countplots\n",
    "    def categorical_countplots(\n",
    "        self,\n",
    "        hue: str = None,\n",
    "        num_columns: int = 2,\n",
    "        figsize_per_row: int = 7,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        color: str = '#a2bffe',\n",
    "        title: str = 'Countplots of Categorical Variables '\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots countplots for all categorical variables in the dataset.\n",
    "\n",
    "        Optionally groups the bars using a hue column (e.g., 'churn_target'), allowing \n",
    "        visual comparison of class distributions between different categories. Annotates\n",
    "        each bar with its percentage frequency.\n",
    "\n",
    "        Args:\n",
    "            hue (str, optional): Name of the column used to group bars (e.g., target variable).\n",
    "                                If None, no grouping is applied.\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height (in inches) of each subplot row.\n",
    "            palette (list): List of colors to use when `hue` is specified.\n",
    "            color (str): Default color to use when `hue` is not provided.\n",
    "            title (str): General title for the entire plot grid.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the hue column is not found in the DataFrame.\n",
    "            Exception: If the plot generation fails for unexpected reasons.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not found in the DataFrame.\")\n",
    "\n",
    "            categorical_cols = self.data.select_dtypes(include = ['object', 'category']).columns.tolist()\n",
    "            if hue and hue in categorical_cols:\n",
    "                categorical_cols.remove(hue)\n",
    "            \n",
    "            # Config Ax's\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(categorical_cols):\n",
    "                sns.countplot(\n",
    "                    data = self.data,\n",
    "                    x = column,\n",
    "                    hue = hue if hue else None,\n",
    "                    palette = palette if hue else None,\n",
    "                    color = None if hue else color,\n",
    "                    edgecolor = 'white' if hue else 'black',\n",
    "                    saturation = 1,\n",
    "                    legend = False,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "                \n",
    "                total = len(self.data[column])\n",
    "                for p in ax[i].patches:\n",
    "                    height = p.get_height()\n",
    "                    if height == 0:\n",
    "                        continue\n",
    "                    percentage = f'{100 * height / total:.1f}%'\n",
    "                    x = p.get_x() + p.get_width() / 1.95\n",
    "                    y = height\n",
    "                    ax[i].annotate(\n",
    "                        percentage,\n",
    "                        (x, y),\n",
    "                        ha = 'center',\n",
    "                        va = 'bottom',\n",
    "                        fontsize = 16,\n",
    "                        color = 'black'\n",
    "                    )\n",
    "\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], f'Countplot of variable: {column}')\n",
    "                ax[i].set_xticks(range(len(ax[i].get_xticklabels())))\n",
    "                ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize = 16)\n",
    "                \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate categorical countplots: {str(e)}')\n",
    "\n",
    "    ### Numerical Barplots\n",
    "    def numerical_barplots(\n",
    "        self,\n",
    "        hue: str = None,\n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        errorbar = ('ci', 90),\n",
    "        title: str = 'Barplots of Numerical Variables',\n",
    "        legend: list = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots barplots for each numerical variable, optionally grouped by a hue variable.\n",
    "\n",
    "        This method creates barplots to visualize the mean (or other estimator) of numerical\n",
    "        variables in the dataset. It supports grouping by a categorical variable (`hue`)\n",
    "        and displays error bars (e.g., confidence intervals).\n",
    "\n",
    "        Args:\n",
    "            hue (str, optional): Column name to group the barplots (e.g., 'churn_target').\n",
    "                If None, no grouping is applied. Defaults to None.\n",
    "            num_columns (int): Number of subplots per row in the grid.\n",
    "            figsize_per_row (int): Height (in inches) allocated per row of subplots.\n",
    "            palette (list, optional): List of colors to use when `hue` is specified.\n",
    "                Defaults to ['#b0ff9d', '#db5856'].\n",
    "            errorbar (tuple or str, optional): Error bar representation passed to seaborn.barplot.\n",
    "                Defaults to ('ci', 90) for 90% confidence intervals.\n",
    "            title (str): Overall title for the figure.\n",
    "            legend (list, optional): Custom labels to replace default hue legend labels.\n",
    "                Defaults to an empty list.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the `hue` column is specified but not found in the DataFrame.\n",
    "            Exception: For other errors during plotting.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not found in the DataFrame.\")\n",
    "\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "            \n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "                sns.barplot(\n",
    "                    data = self.data,\n",
    "                    x = hue,\n",
    "                    y = column,\n",
    "                    hue = hue,\n",
    "                    errorbar = errorbar,\n",
    "                    dodge = False,\n",
    "                    palette = palette,\n",
    "                    edgecolor = 'white',\n",
    "                    legend = False,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "\n",
    "                # Config Ax's\n",
    "                if len(legend) > 1:\n",
    "                    ax[i].set_xticks(list(range(len(legend))))\n",
    "                    ax[i].set_xticklabels(legend, fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "                self._format_single_ax(ax[i], f'Barplot of variable: {column}')\n",
    "                ax[i].set_yticklabels([])\n",
    "                sns.despine(ax = ax[i], top = True, right = True, left = True, bottom = True)\n",
    "            \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate numerical barplots: {str(e)}')\n",
    "\n",
    "    ### Barplot Target\n",
    "    def barplot_target(\n",
    "        self,\n",
    "        target_col: str,\n",
    "        percentage_col: str,\n",
    "        title: str,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots a bar chart showing the churn rate from a pre-aggregated DataFrame.\n",
    "\n",
    "        This method visualizes the percentage distribution of the churn target classes,\n",
    "        using bars colored by the target class and annotated with percentage values.\n",
    "\n",
    "        Args:\n",
    "            target_col (str): Name of the column representing the churn target classes\n",
    "                (e.g., 0 = non-churner, 1 = churner).\n",
    "            percentage_col (str): Name of the column containing percentage values for each class.\n",
    "            title (str): Title of the plot.\n",
    "            palette (list, optional): List of colors for the bars. Defaults to ['#b0ff9d', '#db5856'].\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `target_col` or `percentage_col` are not found in the DataFrame.\n",
    "            Exception: For any other error occurring during plotting.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if target_col not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{target_col}' not found in the DataFrame.\")\n",
    "\n",
    "            if percentage_col not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{percentage_col}' not found in the DataFrame.\")\n",
    "            \n",
    "            # Define AX and Fig\n",
    "            plt.rc('font', size = 20, weight = 'bold')\n",
    "            fig, ax = plt.subplots(figsize = (8, 6))\n",
    "\n",
    "            barplot = sns.barplot(\n",
    "                data = self.data,\n",
    "                x = target_col,\n",
    "                y = percentage_col,\n",
    "                hue = target_col,\n",
    "                dodge = False,\n",
    "                palette = palette,\n",
    "                edgecolor = 'black',\n",
    "                saturation = 1,\n",
    "                legend = False,\n",
    "                ax = ax\n",
    "            )\n",
    "\n",
    "            # Annotate bars\n",
    "            for v in barplot.patches:\n",
    "                barplot.annotate(\n",
    "                    f'{v.get_height():.2f}%',\n",
    "                    (v.get_x() + v.get_width() / 2., v.get_height() / 1.06),\n",
    "                    ha = 'center',\n",
    "                    va = 'top',\n",
    "                    fontsize = 16,\n",
    "                    fontweight = 'bold',\n",
    "                    color = 'black'\n",
    "                )\n",
    "\n",
    "            # Config Ax's and Show Graphics\n",
    "            ax.set_yticklabels([])\n",
    "            sns.despine(ax = ax, top = True, right = True, left = True, bottom = False)\n",
    "            self._format_single_ax(ax, title = title, linewidth = 0.5)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to generate Barplot target: {str(e)}')\n",
    "\n",
    "    ### Correlation Heatmap\n",
    "    def correlation_heatmap(\n",
    "        self,\n",
    "        title: str = None,\n",
    "        cmap: str = 'coolwarm'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots a heatmap showing the correlation matrix among the numerical columns.\n",
    "\n",
    "        This method computes the correlation matrix of the dataset and displays it as a heatmap,\n",
    "        with annotations showing the correlation coefficients.\n",
    "\n",
    "        Args:\n",
    "            title (str, optional): Title for the heatmap plot. Defaults to None.\n",
    "            cmap (str, optional): Colormap to use for the heatmap. Defaults to 'coolwarm'.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the heatmap generation or plotting fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select only the desired columns\n",
    "            corr_data = self.data.corr()\n",
    "\n",
    "            # Define AX and Fig\n",
    "            plt.rc('font', size = 15)\n",
    "            fig, ax = plt.subplots(figsize = (20, 15))\n",
    "\n",
    "            sns.heatmap(\n",
    "                corr_data,\n",
    "                annot = True,\n",
    "                cmap = cmap,\n",
    "                fmt = '.2f',\n",
    "                linewidths = 0.5,\n",
    "                ax = ax\n",
    "            )\n",
    "            # Config Ax's and Show Graphics\n",
    "            ax.set_title(title, fontsize = 20, fontweight = 'bold')\n",
    "            plt.tight_layout(rect = [0, 0, 1, 0.97])\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to generate correlation heatmap: {str(e)}')\n",
    "\n",
    "    ### Scatterplots vs Reference\n",
    "    def scatterplots_vs_reference(\n",
    "        self, \n",
    "        x_reference: str,\n",
    "        hue: str = None,\n",
    "        exclude_cols: list = [],\n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        title: str = 'Scatterplot of Numerical Variables vs Reference'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots scatterplots comparing numerical variables against a reference variable,\n",
    "        optionally grouped by a hue variable.\n",
    "\n",
    "        This method creates scatterplots of all numerical columns (excluding specified ones)\n",
    "        against a single reference numerical column on the X-axis. Points can be colored by\n",
    "        a categorical hue variable.\n",
    "\n",
    "        Args:\n",
    "            x_reference (str): Column name to be used as X-axis in all scatterplots.\n",
    "            hue (str, optional): Column name used for grouping/coloring points. Defaults to None.\n",
    "            exclude_cols (list, optional): List of columns to exclude from Y-axis candidates,\n",
    "                in addition to `x_reference` and `hue`. Defaults to empty list.\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height (in inches) allocated per subplot row.\n",
    "            palette (list, optional): List of colors for the hue categories. Defaults to ['#b0ff9d', '#db5856'].\n",
    "            title (str): Overall title for the figure.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `x_reference` or `hue` (when specified) are not found in the DataFrame.\n",
    "            Exception: For any other errors during plotting.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if x_reference not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{x_reference}' not found in the DataFrame.\")\n",
    "        \n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not found in the DataFrame.\")\n",
    "\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            for col in [x_reference, hue] + exclude_cols:\n",
    "                if col in numeric_cols:\n",
    "                    numeric_cols.remove(col)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "                sns.scatterplot(\n",
    "                    data = self.data,\n",
    "                    x = x_reference,\n",
    "                    y = column,\n",
    "                    hue = hue,\n",
    "                    palette = palette if hue else None,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], f'{column} x {x_reference}')\n",
    "                ax[i].set_xticklabels([])\n",
    "                ax[i].set_yticklabels([])\n",
    "                sns.despine(ax = ax[i], top = True, right = True, left = True, bottom = True)\n",
    "\n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate scatterplots vs reference: {str(e)}')  \n",
    "\n",
    "    # Categorical Bar Percentages\n",
    "    def categorical_bar_percentages(\n",
    "        self,\n",
    "        hue: str ,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        num_columns: int = 2,\n",
    "        figsize_per_row: int = 8,\n",
    "        title: str = 'Barplots Of The Individual Rate Percentages Of Each Column Class'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots barplots of churn percentages per class of each categorical variable.\n",
    "\n",
    "        This method calculates the percentage distribution of a binary target (`hue`)\n",
    "        within each category of all categorical columns in the dataset, and visualizes\n",
    "        these percentages as barplots.\n",
    "\n",
    "        Args:\n",
    "            hue (str): Name of the binary target column (e.g., 'churn_target').\n",
    "            palette (list, optional): List of colors for the hue classes.\n",
    "                Defaults to ['#b0ff9d', '#db5856'].\n",
    "            num_columns (int): Number of subplots per row in the grid.\n",
    "            figsize_per_row (int): Height (in inches) allocated per subplot row.\n",
    "            title (str): Overall title for the figure.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `hue` is not found in the DataFrame.\n",
    "            Exception: For other errors during computation or plotting.\n",
    "\n",
    "        Returns:\n",
    "            None: Displays the plot directly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not found in the DataFrame.\")\n",
    "            categorical_cols = self.data.select_dtypes(include = ['object', 'category']).columns.tolist()\n",
    "            if hue and hue in categorical_cols:\n",
    "                categorical_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(categorical_cols):\n",
    "                \n",
    "                total_churn_per_class = self.data.groupby(column)[hue].count().reset_index(name = f'total_count_class')\n",
    "\n",
    "                result = (\n",
    "                    self.data.groupby([column, hue])[hue]\n",
    "                    .count()\n",
    "                    .reset_index(name = 'frequency')\n",
    "                    .merge(total_churn_per_class, on = column)\n",
    "                )\n",
    "                result['percentage_per_class'] = round((result['frequency'] / result['total_count_class']) * 100, 2)\n",
    "\n",
    "                sns.barplot(\n",
    "                    data=result,\n",
    "                    x = column,\n",
    "                    y = 'percentage_per_class',\n",
    "                    hue = hue,\n",
    "                    palette = palette,\n",
    "                    edgecolor = 'white',\n",
    "                    saturation = 1,\n",
    "                    legend = False,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "\n",
    "                # Annotate bars\n",
    "                for p in ax[i].patches:\n",
    "                    height = p.get_height()\n",
    "                    percentage = f'{height:.1f}%'\n",
    "                    x = p.get_x() + p.get_width() / 2\n",
    "                    ax[i].annotate(\n",
    "                        percentage,\n",
    "                        (x, height),\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        fontsize=14,\n",
    "                        color='black'\n",
    "                    )\n",
    "\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], f'Barplot of variable: {column}')\n",
    "                ax[i].set_xticks(range(len(ax[i].get_xticklabels())))\n",
    "                ax[i].set_xticklabels(ax[i].get_xticklabels(), fontsize = 16)\n",
    "            \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate percentage barplots: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab269f91-6c80-4e90-b99f-48b1b2c79f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Find Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4605f80f-3d96-48e8-ab22-deb6a8cb62d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_outliers(df_num):\n",
    "    \"\"\"\n",
    "    Calculates and displays the percentage of outliers in each column of a PySpark DataFrame.\n",
    "\n",
    "    This function identifies outliers using the IQR method (Q3 - Q1).\n",
    "    It displays the percentage of outliers per column.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_num : pyspark.sql.DataFrame\n",
    "        PySpark DataFrame with numeric columns.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Checks if it is a PySpark DataFrame\n",
    "        if not isinstance(df_num, DataFrame):\n",
    "            raise TypeError('The input must be a PySpark DataFrame.')\n",
    "        \n",
    "        # Check if the DataFrame is empty\n",
    "        if df_num.rdd.isEmpty():\n",
    "            raise ValueError('The provided DataFrame is empty.')\n",
    "        \n",
    "        # List to save data\n",
    "        out_col, num_outliers = [], []\n",
    "\n",
    "        # Total size of the dataframe\n",
    "        size_df = df_num.count()\n",
    "        if size_df == 0:\n",
    "            raise ValueError('The DataFrame has no rows.')\n",
    "        \n",
    "        for column in df_num.columns:\n",
    "            try:\n",
    "                # Calculation of quartiles (may fail if not numeric)\n",
    "                quantiles = df_num.approxQuantile(column, [0.25, 0.75], 0)\n",
    "                if not quantiles or len(quantiles) < 2:\n",
    "                    print(f\"[Warning] Could not calculate quantiles for column: {column}\")\n",
    "                    continue\n",
    "                \n",
    "                Q1, Q3 = quantiles # Lower quartile and Upper quartile\n",
    "                IQR = Q3 - Q1 # Difference between the third quartile and the first quartile\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                # Filter nulls and create temporary column for outliers\n",
    "                df_filtered = df_num.filter(F.col(column).isNotNull())\n",
    "                df_filtered = df_filtered.withColumn(\n",
    "                    f'{column}_out',\n",
    "                    F.when((F.col(column) < lower_bound) | (F.col(column) > upper_bound), True).otherwise(False)\n",
    "                )\n",
    "\n",
    "                # Count outliers\n",
    "                n_outliers = df_filtered.filter(F.col(f'{column}_out') == True).count()\n",
    "                percentage_out = round((n_outliers / size_df) * 100, 2)\n",
    "\n",
    "                # # Stores the data\n",
    "                out_col.append(column)\n",
    "                num_outliers.append(percentage_out)\n",
    "            \n",
    "            except Exception as inner_e:\n",
    "                print(f\"[Warning] Failed to process column '{column}': {inner_e}\")\n",
    "\n",
    "        # Show Results\n",
    "        if out_col:\n",
    "            print('\\n✅ Percentage of Outliers by Column:')\n",
    "            percentage_out_data = spark.createDataFrame([tuple(num_outliers)], out_col)\n",
    "            percentage_out_data.display()\n",
    "        else:\n",
    "            print('⚠️ No outliers could be computed.')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to compute outliers: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c4f98fc-d551-4053-9b92-c55d2cfdaaca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e91db7-7425-4faa-8d4c-d67280d2af83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_ttest_between_groups(\n",
    "    data: pd.DataFrame, \n",
    "    numerical_col: str, \n",
    "    group_col: str, \n",
    "    group1_val  = 1, \n",
    "    group2_val = 0, \n",
    "    alpha: float = 0.05,\n",
    "    print_summary: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a t-test comparing a numerical variable between two specified groups.\n",
    "\n",
    "    This function conducts an independent two-sample t-test (Welch’s t-test) \n",
    "    on the specified numerical column between two groups defined by values in a grouping column.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset containing the variables.\n",
    "        numerical_col (str): Name of the numeric column to compare.\n",
    "        group_col (str): Name of the column representing groups.\n",
    "        group1_val (any): Value in `group_col` representing the first group.\n",
    "        group2_val (any): Value in `group_col` representing the second group.\n",
    "        alpha (float, optional): Significance level for hypothesis testing. Default is 0.05.\n",
    "        print_summary (bool, optional): Whether to print the test summary. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            t_stat (float): The computed t-statistic.\n",
    "            p_value (float): The p-value of the test.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during the test execution.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        group1 = data[data[group_col] == group1_val][numerical_col]\n",
    "        group2 = data[data[group_col] == group2_val][numerical_col]\n",
    "\n",
    "        t_stat, p_value = ttest_ind(group1, group2, equal_var = False)\n",
    "\n",
    "        if print_summary:\n",
    "            print(f'\\n🟢 t-statistic: {t_stat:.5f}')\n",
    "            print(f'🔵 p-value: {p_value:.5f}')\n",
    "            print('-------' * 10)\n",
    "            if p_value <= alpha:\n",
    "                print(f'\\n✅ Null Hypothesis (H0) Rejected!')\n",
    "                print(f\"There is a significant difference in '{numerical_col}'\") \n",
    "                print(f'between the two groups ({group1_val} vs {group2_val}).')\n",
    "            else:\n",
    "                print(f'\\n⛔ Null Hypothesis (H0) Accepted!') \n",
    "                print(f\"There is no significant difference in '{numerical_col}'\") \n",
    "                print(f'between the two groups ({group1_val} vs {group2_val})')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Failed to perform t-test: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a6d9dc-277d-45cb-925c-45883f735039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1 - Business Understanding  \n",
    "---\n",
    "\n",
    "### General Problem Context  \n",
    "#### What is Churn Rate, and What Are the Solutions to This Problem? \n",
    "Many companies struggle with customer churn and often find it challenging to reverse this trend. The metric that measures this scenario is called **churn rate**, which indicates when strategic solutions are needed to address the issue.  \n",
    "\n",
    "In 2020, Bryce Baer published a guide on churn rate on the [Zendesk website](https://www.zendesk.com.br/blog/customer-churn-rate/?_ga=2.155312252.614584228.1623244699-1365810980.1622555740#) – a company specializing in corporate software development. The guide highlights that businesses implementing strategies to reduce churn can increase their **profitability** by nearly 40%.  \n",
    "\n",
    "---\n",
    "\n",
    "#### How to Calculate Churn Rate?\n",
    "##### Churn Rate Formula:  \n",
    "$$\\text{Churn Rate} = \\frac{\\text{Number of customers lost during a period}}{\\text{Total number of customers at the start of the period}} \\times 100$$  \n",
    "\n",
    "---\n",
    "\n",
    "#### Impacts of a High Churn Rate\n",
    "While reducing churn to zero is practically impossible, acceptable rates (4% to 5%) minimize financial impacts. Some companies operate at higher rates (5% to 7%) without significant revenue loss, depending on industry dynamics. **Key factors to define \"acceptable\" churn**:  \n",
    "- Industry standards (e.g., SaaS vs. retail).  \n",
    "- Customer lifetime value (CLV).  \n",
    "- Customer acquisition cost (CAC).  \n",
    "\n",
    "---\n",
    "\n",
    "#### Reasons for Customer Churn\n",
    "1. **Lack of Perceived Value**:  \n",
    "   - Occurs when there’s a growing gap between customer expectations and actual delivery. Clear communication about product/service benefits is critical.  \n",
    "2. **Poor Customer Experience**:  \n",
    "   - Negative interactions (e.g., bad support, complex processes, product failures) drive churn.  \n",
    "3. **Competitor Offers**:  \n",
    "   - Attractive promotions or pricing from competitors can lure customers away.  \n",
    "4. **Changing Customer Needs**:  \n",
    "   - Failure to adapt products/services to evolving demands leads to turnover.  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Challenge: \n",
    "The bank’s manager has observed a rising number of customers abandoning credit card services. Stakeholders aim to:  \n",
    "1. **Analyze historical data** to identify root causes of churn.  \n",
    "2. **Develop a machine learning model** to predict customer churn probability.  \n",
    "3. **Implement strategic actions** to retain high-risk customers.  \n",
    "\n",
    "---\n",
    "\n",
    "## KPIs for the Churn Prediction Project:  \n",
    "1. **Churn Rate**:  \n",
    "   - *Definition*: Percentage of customers who discontinue credit card services within a specific period.  \n",
    "   - *Goal*: Reduce this metric through targeted retention strategies.  \n",
    "\n",
    "2. **Retention Rate**:  \n",
    "   - *Definition*: Percentage of customers retained after a period.  \n",
    "   - *Importance*: Directly reflects the success of retention efforts.  \n",
    "\n",
    "3. **Customer Acquisition Cost (CAC) vs. Retention Cost**:  \n",
    "   - *Definition*: Ratio of costs to acquire new customers vs. retaining existing ones.  \n",
    "   - *Insight*: Retention is typically **5-7x cheaper** than acquisition.  \n",
    "\n",
    "4. **AUC-ROC (Area Under the Receiver Operating Characteristic Curve)**:  \n",
    "   - *Definition*: Measures the model’s ability to distinguish between churners and non-churners.  \n",
    "   - *Target*: AUC-ROC > 0.85.  \n",
    "\n",
    "5. **Recall**:  \n",
    "   - *Definition*: Proportion of actual churners correctly identified by the model.  \n",
    "   - *Importance*: High recall ensures fewer **false negatives** (missed churners), which is critical because a false negative could result in losing a customer. Retaining existing customers through targeted strategies is significantly cheaper than acquiring new ones.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc06202e-9e92-4afd-a7e5-4e93c722af35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2 - Data Understanding\n",
    "\n",
    "---\n",
    "\n",
    "* This dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc.\n",
    "\n",
    "---\n",
    "\n",
    "- **Data file**: - BankChurners.csv\n",
    "\n",
    "---\n",
    "\n",
    "- **Target dependent variable**: - 'Attrition_Flag', categorical column with binary classification, i.e. 'Existing Customer'(No-churner) or 'Attrited Customer'(Churner).\n",
    "\n",
    "---\n",
    "\n",
    "- **The dataset colleted from kaggle**: https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers?sort=votes&select=BankChurners.csv\n",
    "\n",
    "---\n",
    "- **The dataset origin from this site**: https://leaps.analyttica.com/home\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bc2667e-7b88-494c-ae2f-e6721608a488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Dataset\n",
    "\n",
    "I will use Medallion Architecture to organize and classify the data. This approach organizes the data into different levels of processing and refinement, making it easier to manage and analyze the data. Here is a summary of each layer:\n",
    "\n",
    "---\n",
    "\n",
    "- **Bronze Layer:**\n",
    "\n",
    "  Description: Stores the raw data, exactly as it was collected from different sources.\n",
    "  \n",
    "  Objective: Preserve the integrity of the original data, without any transformation.\n",
    "---\n",
    "- **Silver Layer:**\n",
    "\n",
    "  Description: Contains pre-processed and cleaned data.\n",
    "  \n",
    "  Objective: Perform basic transformations, such as data cleaning, standardization, and type correction.\n",
    "---\n",
    "- **Gold Layer:**\n",
    "\n",
    "  Description: Stores the refined data, ready for analysis and final consumption.\n",
    "  \n",
    "  Objective: Apply specific corrections and improvements according to business needs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20ffb426-ba11-47a7-aea9-7aa92a22cf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Data Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dede1a14-12d2-4ee5-bb03-bef040301eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a directory to store the files\n",
    "dbutils.fs.mkdirs('dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Bronze/')\n",
    "###### >>>>>>> Note: At this point, upload the files present in the notebook repository folder to this directory\n",
    "\n",
    "# Viewing the location of files\n",
    "display(dbutils.fs.ls('dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Bronze/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9913fa-e3a1-4342-9c8e-ac788080acbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Bronze/BankChurners.csv'\n",
    "file_type = 'csv'\n",
    "# Load Data\n",
    "df_csv = DataSpark(file_location = file_location).load_data(file_type = file_type)\n",
    "# Show Data\n",
    "df_csv.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b92250b-7253-43a1-b793-f24057e51353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving dataset in Parquet format for more performance at consultations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6abad6-cf24-45d2-9d11-1b2e532fdb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Bronze/BankChurners-parquet'\n",
    "file_type = 'parquet'\n",
    "\n",
    "# Save Data\n",
    "DataSpark(dataframe = df_csv, file_location = file_location).save_data(file_type = file_type)\n",
    "# Load Data\n",
    "df = DataSpark(file_location = file_location).load_data(file_type = file_type)\n",
    "# Show Data\n",
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083c8839-7e7f-4f8b-9de5-da1f9af6d1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dictionary of Dataset\n",
    "---\n",
    "\n",
    "**CLIENTNUM**: Client number. Unique identifier for the customer holding the account\n",
    "\n",
    "---\n",
    "\n",
    "**Attrition_Flag**: Internal event (customer activity) variable - if the account is closed then 'Attrited Customer' else 'Existing Customer'\n",
    "\n",
    "---\n",
    "\n",
    "**Customer_Age**: Demographic variable - Customer's Age in Years\n",
    "\n",
    "---\n",
    "\n",
    "**Gender**: Demographic variable - M=Male, F=Female\n",
    "\n",
    "---\n",
    "\n",
    "**Dependent_count**: Demographic variable - Number of dependents\n",
    "\n",
    "---\n",
    "\n",
    "**Education_Level**: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "**Marital_Status**: Demographic variable - Married, Single, Divorced, Unknown\n",
    "\n",
    "---\n",
    "\n",
    "**Income_Category**: Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, >$120K)\n",
    "\n",
    "---\n",
    "\n",
    "**Card_Category**: Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n",
    "\n",
    "---\n",
    "\n",
    "**Months_on_book**: Period of relationship with bank\n",
    "\n",
    "---\n",
    "\n",
    "**Total_Relationship_Count**: Total no. of products held by the customer\n",
    "\n",
    "---\n",
    "\n",
    "**Months_Inactive_12_mon**: No. of months inactive in the last 12 months\n",
    "\n",
    "---\n",
    "\n",
    "**Contacts_Count_12_mon**: No. of Contacts in the last 12 months\n",
    "\n",
    "---\n",
    "\n",
    "**Credit_Limit**: Credit Limit on the Credit Card\n",
    "\n",
    "---\n",
    "\n",
    "**Total_Revolving_Bal**: Total Revolving Balance on the Credit Card\n",
    "\n",
    "---\n",
    "\n",
    "**Avg_Open_To_Buy**: Open to Buy Credit Line (Average of last 12 months)\n",
    "\n",
    "---\n",
    "\n",
    "**Total_Amt_Chng_Q4_Q1**: Change in Transaction Amount (Q4 over Q1)\n",
    "\n",
    "---\n",
    "\n",
    "**Total_Trans_Amt**: Total Transaction Amount (Last 12 months)\n",
    "\n",
    "---\n",
    "\n",
    "**Total_Trans_Ct**: Total Transaction Count (Last 12 months)\n",
    "\n",
    "---\n",
    "\n",
    "**Total_Ct_Chng_Q4_Q1**: Change in Transaction Count (Q4 over Q1)\n",
    "\n",
    "---\n",
    "\n",
    "**Avg_Utilization_Ratio**: Average Card Utilization Ratio\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34ea1a53-deaf-4a2b-b561-75b495195ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The Size Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a62937-6133-4117-843c-8ce04d52727b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of registers: {df.count()}\\nNumber of columns: {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b91f352-50c8-49bf-9ea9-cfd64c597aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop Redundantes Columns:\n",
    "\n",
    "According to the documentation of this dataset that was made available by Kaggle, we were given the recommendation to remove the columns:\n",
    "\n",
    "---\n",
    "- Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\n",
    "\n",
    "---\n",
    "- Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\n",
    "\n",
    "---\n",
    "Therefore, I will be applying this recommendation.\n",
    "\n",
    "Will also be removing the **CLIENTNUM** column, which refers to the registration number of the customers of this banking institution. It is possible to conclude that this data will not add any relevant information to the resolution of the problems and questions to be answered with this analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90848043-6c97-4273-be90-1e94080c6174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "redundants_cols = [\n",
    "  'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1', 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n",
    "  'CLIENTNUM'\n",
    "]\n",
    "df = df.drop(* redundants_cols)\n",
    "\n",
    "# Check size Dataset\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7580adb9-79a6-4639-867c-720b62b2dea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checking data and its characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d8e41a-5aa0-43df-aeaa-d911e3f351f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87afcc5-9496-47cf-bd14-b3626c823e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0d9ba69-5be2-495d-b609-69aa4648060c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking for null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da13d9f-3ea9-4ac1-a847-8cfbbc83c1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Size df\n",
    "size_df = df.count()\n",
    "\n",
    "# Check null data\n",
    "df.agg(*[F.round(((F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)) / size_df) * 100), 2).alias(c) for c in df.columns]) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a9729bf-399c-4f24-96e4-c484a6ee80fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking data duplicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7cf798-7129-4f40-86d2-3a4e1eb6f022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(df.columns) \\\n",
    "    .count() \\\n",
    "    .filter(F.col('count') > 1) \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6721a7-8cff-49b0-991b-e40c4ff120c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Classifying variables\n",
    "#### Concepts for Classification of variables according to statistics:\n",
    "\n",
    "---\n",
    "\n",
    "**Quantitative or numerical variables**:\n",
    "\n",
    "* *Discrete*: only take integer values\n",
    "\n",
    "* *Continuous*: assumes any value in the range of real numbers\n",
    "\n",
    "---\n",
    "\n",
    "**Qualitative or categorical variables**:\n",
    "* *Nominals*: when categories do not have a natural order\n",
    "\n",
    "* *Ordinals*: when categories can be ordered.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f8a18cc-3c58-4508-a2b4-b5b1484bbda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Adjusting column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73770f47-94ed-4640-a1af-d3062b7deb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    # Renaming columns with only lowercase letters    \n",
    "    df = df.withColumnRenamed(column, column.lower()) \\\n",
    "\n",
    "df.limit(5).display()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42691f1f-200f-4043-932b-01102121a939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Numerical Variables\n",
    "\n",
    "discrete_numerical = [\n",
    "    'customer_age', 'dependent_count', 'months_on_book', 'total_relationship_count', 'months_inactive_12_mon', 'contacts_count_12_mon', 'total_trans_ct'\n",
    "    ]\n",
    "\n",
    "continuos_numerical = [\n",
    "    'credit_limit','total_revolving_bal', 'avg_open_to_buy', 'total_amt_chng_q4_q1', 'total_trans_amt', 'total_ct_chng_q4_q1',\n",
    "    'avg_utilization_ratio'\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "# Categorical Variables\n",
    "\n",
    "nominal_categorical = [\n",
    "    'attrition_flag', 'gender', 'marital_status', \n",
    "    ]\n",
    "\n",
    "ordinal_categorical  = [\n",
    "    'education_level', 'income_category', 'card_category', \n",
    "\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be5cdb1-1773-4055-a784-bf1c2fdb2404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset with column types\n",
    "column = ['ct_type_cols']\n",
    "data = [ (name_col, ) for name_col in df.columns]\n",
    "type_columns = spark.createDataFrame(data, column)\n",
    "\n",
    "# Adding types\n",
    "type_columns = type_columns \\\n",
    "  .withColumn('ct_type_cols', F.when(F.col('ct_type_cols').isin(nominal_categorical), 'Nominal Categorical').otherwise(F.col('ct_type_cols'))) \\\n",
    "  .withColumn('ct_type_cols', F.when(F.col('ct_type_cols').isin(ordinal_categorical), 'Ordinal Categorical').otherwise(F.col('ct_type_cols'))) \\\n",
    "  .withColumn('ct_type_cols', F.when(F.col('ct_type_cols').isin(continuos_numerical), 'Continuos Numerical').otherwise(F.col('ct_type_cols'))) \\\n",
    "  .withColumn('ct_type_cols', F.when(F.col('ct_type_cols').isin(discrete_numerical), 'Discrete Numerical').otherwise(F.col('ct_type_cols'))) \\\n",
    "\n",
    "# Colleting data\n",
    "groupy_type_columns = type_columns.groupBy('ct_type_cols') \\\n",
    "  .agg(F.count('ct_type_cols').alias('count_types')) \\\n",
    "  .withColumn('percentage', F.round((F.col('count_types') / len(df.columns)) * 100, 2)) \\\n",
    "  .orderBy('count_types') \\\n",
    "\n",
    "# Graphic\n",
    "# Data\n",
    "data_ax = groupy_type_columns.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b18f685-80b7-4d28-8558-a8aad19e2bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pie_type_var = GraphicsData(data = data_ax)\n",
    "pie_type_var.plot_variable_type(count_col='count_types', label_col='ct_type_cols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b539810e-780a-4a3c-bbe9-ed84770d2dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Checking the data initially to verify its characteristics and structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528ee44a-1cde-4466-aa73-e6590d2fffbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Checking some statistical data from the numerical columns of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395cc66a-a838-4f56-98c6-ea60afcfbc9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_describe = df.select(*discrete_numerical, *continuos_numerical) \\\n",
    "    .describe()\n",
    "df_describe.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac3b2ba8-f7c8-4516-b25f-64bbc94c792e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Mean of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00965527-163c-4105-b919-10fd0b99eebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_describe.filter(F.col('summary') == 'mean').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e937244-33a3-4132-8dd7-f85bcbb1bcc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Check categorial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0acbee2-f540-48e2-8505-67745d61bf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    # Iterating over dataset columns\n",
    "    for column in  df.select(*nominal_categorical, *ordinal_categorical).columns:\n",
    "\n",
    "        # Grouping columns by frequency and percentage\n",
    "        df.groupBy(column) \\\n",
    "            .agg(F.count(column).alias('frequency')) \\\n",
    "            .withColumn('percentage', F.round((F.col('frequency') / df.count()) * 100, 2)) \\\n",
    "            .orderBy('frequency', ascending = False) \\\n",
    "            .display()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e376e7-a7c6-413d-91e4-95f488eb5041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d82e556-e0c0-481e-8871-c462b0b0247a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The min values ​​of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5b4f6fc-d371-4ae1-a8dc-42457fa12bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_describe.filter(F.col('summary') == 'min').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b996ce2-9af0-4e88-a222-37e746b6e7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The max values ​​of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd4e1a7-cd2c-4ae4-97ef-c54cae7f85d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_describe.filter(F.col('summary') == 'max').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c60b0957-f325-4a7a-9036-6341a632c436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Distribution of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050626fa-1deb-4591-9a47-415284a962fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Collect\n",
    "data_ax = df.select(*discrete_numerical, *continuos_numerical).toPandas()\n",
    "# Histoplots of Numerical Variables\n",
    "GraphicsData(data_ax).numerical_histograms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcae42dd-80dd-4f26-a439-be11d6e41889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Boxplots of Numerical Variables\n",
    "GraphicsData(data_ax).numerical_boxplots(showfliers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4287d38b-f998-424b-865d-f083e2e45dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking the percentage of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1f18b7-3d20-4142-8d82-501071a124d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "find_outliers(df.select(*discrete_numerical, *continuos_numerical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd55a415-855d-4133-b8e8-dbdae8f91158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Initial observations and insights:\n",
    "---\n",
    "\n",
    "- 1 - This dataset has a little over **10,000 records**, so it is a small dataset. This brings us some limitations regarding the statistical inferences that will be made in the analyses, and also regarding the training of models, it will be a little more difficult to train them and make them have a good generalization of the data.\n",
    "---\n",
    "- 2 - There is no **null data** or **duplicate data** in this dataset, which is a very positive thing.\n",
    "---\n",
    "- 3 - This dataset has as its main source of data **numerical variables**, corresponding to **70% of the data**, therefore, only **30% of the data** is **categorical**.\n",
    "---\n",
    "- 4 - The average age of customers is **46 years**, which indicates that we have a more mature profile and customer experience. Therefore, it is possible to hypothesize that, due to this factor, they tend to be more demanding regarding the services provided by the bank through credit cards.\n",
    "---\n",
    "- 5 - Customers have, on average, **2 dependents**, which can be a significant factor in understanding the profile of this banking institution's customers.\n",
    "---\n",
    "- 6 - On average, customers have been in a relationship with the bank for **35 months (about 3 years)**, which initially seems to be a positive thing, but it could be a factor that needs to be improved.\n",
    "---\n",
    "- 7 - Customers, on average, maintain approximately **4 products** offered by the bank. Therefore, this could be a factor that should be questioned and analyzed more carefully, so that it is possible to determine the influence of this factor on the turnover rate of this institution's customers.\n",
    "---\n",
    "- 8 - Most customers are **married, about 46%**, which can provide us with more information about the products and services to be offered to this customer profile.\n",
    "---\n",
    "- 9 - Most customers are **graduates**, around 30%.\n",
    "---\n",
    "- 10 - Most customers earn less than **$40,000** per year.\n",
    "---\n",
    "- 11 - On average, customers tend to use **27%** of their credit card limit.\n",
    "---\n",
    "- 12 - The **Blue** card is the most common category of credit cards, with around **93%** of participation.\n",
    "---\n",
    "- 13 - There is a balance between the number of male and female customers, with a slight majority of women.\n",
    "---\n",
    "- 14 - Initially, I considered the **IQR** as a parameter to define the existence of **outliers** in this dataset. Some numeric variables have data that, statistically, can be considered outliers. The **Credit_Limit** variable, for example, has 9.72% of data that can be considered outliers. However, all data and numeric columns will be analyzed to verify whether they are part of the **natural distribution** of this dataset or whether they are **incorrect data**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d163ccbc-7ae6-4b3a-8efc-bba14cdfd7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3 - Data Preparation\n",
    "---\n",
    "\n",
    "- In this step, I will initially divide the training and testing data so that the testing data does not interfere in the analyses, so that the model does not have any bias from the testing data, and only with the training data is it capable of generating good classifications with good generalization.\n",
    "---\n",
    "- Next, an EDA will be conducted to verify the data and its main characteristics. In this EDA, the main objective will be to understand the relationship of the data with the churn rate of this banking institution.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "147f95d7-51bd-46ef-a8ae-2e6cdba2b44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adjusting dataset targets\n",
    "---\n",
    "\n",
    "I will adjust the target column, which is attrition_flag. Since it is a categorical column with binary classification:\n",
    "\n",
    "---\n",
    "\n",
    "- **Existing Customer (Non-churner)**: will receive the **value 0**. \n",
    "\n",
    "  These are customers who still use the credit card services provided by the bank.\n",
    "\n",
    "---\n",
    "\n",
    "- **Attrited Customer (Churner)**: will receive the **value 1**. \n",
    "\n",
    "  These are customers who have stopped using the credit card services.\n",
    "\n",
    "---\n",
    "\n",
    "This approach will make it easier to calculate correlation statistics between variables since the target variable is already indexed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9068a5ce-e8d9-4878-8c89-56c15911bd24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Indexing and adjusting the target column name\n",
    "df_clean = df.withColumn('churn_target', F.when(F.col('attrition_flag') == 'Existing Customer', 0).otherwise(1).cast(IntegerType()))\n",
    "\n",
    "# Saving previously cleaned and adjusted data to a new dataframe\n",
    "df_clean = df_clean.drop('attrition_flag')\n",
    "\n",
    "# Adjusting the list of nominal variables\n",
    "nominal_categorical.remove('attrition_flag')\n",
    "\n",
    "# Check new df\n",
    "df_clean.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4d4ea1-1e6a-49bb-aea4-3d6daf777d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Silver Data Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef445a2-932e-4058-a529-6386e60201fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# File location and type\n",
    "file_location = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Silver/Clean-data'\n",
    "file_type = 'parquet'\n",
    "\n",
    "# Save  Clean Dataset\n",
    "DataSpark(dataframe = df_clean, file_location = file_location).save_data(file_type = file_type)\n",
    "\n",
    "# Reading Dataset\n",
    "df = DataSpark(file_location = file_location).load_data(file_type = file_type)\n",
    "\n",
    "# Check Dataset\n",
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c53ac2c-2246-45db-98c1-dda809488c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Splitting the Training and Testing data\n",
    "---\n",
    "* From here on, all analyses will be based on the **training dataset**, which will represent **80%** of the total dataset. I chose to separate the data from this point on to avoid **leaking test data**, since this data cannot be influenced by the analysis or modeling applied to the training data.\n",
    "\n",
    "---\n",
    "\n",
    "* I will be using pyspark randomSplit function, I will be looking for an equal division in the distribution of targets in this data set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ef60e0-b4c4-4c61-a4e7-608b17e05943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Df data\n",
    "df.groupBy('churn_target') \\\n",
    "    .agg(F.count('churn_target').alias('frequency')) \\\n",
    "    .withColumn('percentage', F.round((F.col('frequency') / df.count()) * 100, 2)) \\\n",
    "    .orderBy('frequency', ascending = False) \\\n",
    "    .display()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107e6860-4222-458c-89d3-179fd626ba60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed = 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f247ef0-549d-4cb8-9ed1-3380443ff448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "train.groupBy('churn_target') \\\n",
    "    .agg(F.count('churn_target').alias('frequency')) \\\n",
    "    .withColumn('percentage', F.round((F.col('frequency') / train.count()) * 100, 2)) \\\n",
    "    .orderBy('frequency', ascending = False) \\\n",
    "    .display()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89c83cf-fda0-4525-933f-a3da493ea22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test data\n",
    "test.groupBy('churn_target') \\\n",
    "    .agg(F.count('churn_target').alias('frequency')) \\\n",
    "    .withColumn('percentage', F.round((F.col('frequency') / test.count()) * 100, 2)) \\\n",
    "    .orderBy('frequency', ascending = False) \\\n",
    "    .display()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7304a6-bd6c-486c-ad1c-acd19ade461f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de2334d-1392-42b0-b920-dc83834fa1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the churn rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e6cf5d7-9359-4ae7-85ba-71ed76c0b96f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "train_churn = train.groupBy('churn_target') \\\n",
    "    .agg(F.count('churn_target').alias('frequency')) \\\n",
    "    .withColumn('percentage', F.round((F.col('frequency') / train.count()) * 100, 2)) \\\n",
    "    .withColumn('churn_target',F.when(F.col('churn_target') == 1, 'Churners').otherwise('Non-Churners')) \\\n",
    "    .orderBy('frequency', ascending = False)\n",
    "\n",
    "# Data of graphics\n",
    "data_ax = train_churn.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409d433d-3f37-49aa-b261-748b4ecd6582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data_ax).barplot_target(target_col = 'churn_target', percentage_col = 'percentage', title =  'Churn Rate of Training Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b89ff6-d552-4fae-8e05-e0bc06d5a9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In this dataset, the rate of customers who abandon credit card services is **16.02%**, while **83.98%** of customers continue to use the bank's services.\n",
    "---\n",
    "- Taking into account a basic principle of simple statistics, it is possible to observe that, for every **100 customers**, at least **16** of them discontinued credit card services.\n",
    "---\n",
    "- This dataset has imbalanced classes, which can be a factor to be considered when training machine learning models. Datasets with imbalanced classes make it more difficult to train and generalize model classifications, especially for **minority** classes. In general, models tend to learn more easily to predict the **majority class**, while they have more difficulty in detecting **minority classes**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41d2478-f606-4998-80b1-0ecaa3bfa71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the distributions of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12cae84-9841-49af-b638-9b5ab3b9e3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_ax = train.select(*discrete_numerical, *continuos_numerical).toPandas()\n",
    "GraphicsData(data_ax).numerical_histograms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e02b20e5-90fb-4f70-a528-bacb9c754241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Observations and insights regarding numerical data\n",
    "---\n",
    "- 1 - The **age of customers** is more widely distributed between the **40** and **50 age groups**, with **49** being the most frequent age group in these data.\n",
    "---\n",
    "- 2 - Most customers have between **2** and **3 dependents**, with a minority having 5 dependents.\n",
    "---\n",
    "- 3 - The length of the **customer's relationship** with the bank varies from **13** to **56 months**, with **36 months** being the most frequent.\n",
    "---\n",
    "- 4 - The **number of products** maintained by the customer is generally above **3 products**, with few customers maintaining only **1** or **2 products**.\n",
    "---\n",
    "- 5 - Most **customers remain inactive** for a maximum of **3 months**, with only a small fraction remaining inactive for **4** to **6 months**.\n",
    "---\n",
    "- 6 - The **number of contacts** in the last 12 months was, in most cases, **2** to **3 contacts**.\n",
    "---\n",
    "- 7 - The **number of transactions** is mostly distributed between **60** and **80 transactions**, with a very small portion of customers making less than **20** or more than **100 transactions**.\n",
    "---\n",
    "- 8 - Most customers have a **credit limit** of less than **5,000 dollars**, although there is a relatively significant portion of customers with a limit of **35,000 dollars**.\n",
    "---\n",
    "- 9 -Most customers have a **zero credit card revolving balance**, which is relatively positive, indicating that most customers are up to date with their bill payments.\n",
    "---\n",
    "- 10 - The **average open to buy**  is below **5,000 dollars**.\n",
    "---\n",
    "- 11 - Most credit **card limit utilization** is below **20%**, with a small portion of customers using more than **80%** of their credit card limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ca360a1-1873-40a1-8c2c-1192c720a241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the distribution of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad36e2e8-5d48-4dd0-9a00-0593bf87b5e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "data_ax = train.select(*nominal_categorical, *ordinal_categorical).toPandas()\n",
    "# Categorical Countplots\n",
    "GraphicsData(data_ax).categorical_countplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea1e335d-5f12-4323-b5f6-f6d8566ffe59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Observations and insights regarding categorical data\n",
    "---\n",
    "\n",
    "- 1 - The majority of clients are women, with a percentage of **52.8%**.\n",
    "---\n",
    "- 2 - **46.3%** of clients are **married**, while **38.8%** are **single**. There is a small portion of **7.5%** of **divorced** clients and another portion of **7.4%** of clients who do **not fit** into any of the above categories.\n",
    "---\n",
    "- 3 - The majority of clients have a **Graduate** level of education, with **30.8%**. This status refers to people who have already graduated and completed a specialization in the area they studied.\n",
    "\n",
    "  **High School** represents **20%**. This status refers to people who have already graduated and completed high school.\n",
    "\n",
    "  **Unknown** represents **15%**. This status refers to people who possibly did not fill out the form or did not fit into any of the above classifications.\n",
    "\n",
    "  **Uneducated** represents **14.7%**. This status refers to people who have not had access to formal education or have not completed a significant level of study.\n",
    "\n",
    "  **College** represents **10%**. This status refers to higher education.\n",
    "\n",
    "  Finally, the **Postgraduate** and **Doctorate** statuses have the smallest shares. **Postgraduate** is basically a synonym for **Graduate**, both referring to the same status, while **Doctorate** refers to the highest level of study.\n",
    "---\n",
    "- 4 - The majority of clients, **34.9%**, have an income below **40k**.\n",
    "\n",
    "  **17.6%** of clients have an income between **40k and 60k**.\n",
    "\n",
    "  **15.5%** have an income between **80k and 120k**.\n",
    "\n",
    "  **13.7%** have an income between **60k and 80k**.\n",
    "\n",
    "  **11%** did not fill out this information or do not fit into any of the categories above.\n",
    "\n",
    "  A smaller portion, **7.3%**, has an income above **120k** per year.\n",
    "---\n",
    "- 5 - **93.2%** of customers have a **Blue** credit card, which is the dominant class. Next comes the **Silver** credit card with **5.6%**, and the **Gold** and **Platinum** cards with a small share of participation that is practically nil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a11804-743f-462d-a7ac-ba6f0ee19992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the correlation of numerical variables with the cause of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81fe737-5806-43c6-bdb9-d39b517a1a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "data_ax = train.select(*continuos_numerical, *discrete_numerical, 'churn_target').toPandas()\n",
    "# Correlation Heatmap\n",
    "GraphicsData(data_ax).correlation_heatmap(title = 'Correlation Matrix Of Variables with Churn Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d308db-1108-4880-8e75-e51c0abd128c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Variables ordered by correlation with the churn target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b0d963-0088-4396-8baa-0343c109c37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_ax.corr()['churn_target'].abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b36991-d279-4fea-8118-7458dbbdce1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Observations and insights on the correlation of numeric variables with the 'churn_target' variable.\n",
    "---\n",
    "**Considering that a customer who churns has a value of 1 and a customer who does not churn has a value of 0:**\n",
    "\n",
    "---\n",
    "- 1 - **total_trans_ct** is the variable with the highest correlation with the **churn_target** variable, with a negative correlation of **-0.37**. This indicates that the lower the number of transactions in the last 12 months, the greater the likelihood that customers will stop using credit card services.\n",
    "---\n",
    "- 2 - **total_ct_chng_q4_q1** had a correlation of **-0.29**. This column represents the change in the number of transactions between the fourth quarter (Q4) of the previous year and the first quarter (Q1). The lower this index, the greater the possibility of the customer stopping using the credit card.\n",
    "---\n",
    "- 3 - Customers with a lower revolving balance** are more likely to become **inactive**, while customers with a higher revolving balance tend to continue using the bank's credit card services. It is interesting to note that, even with a **higher revolving balance**, which can lead to possible future debt, these customers tend to remain **active customers** of the institution.\n",
    "---\n",
    "- 4 - The **number of contacts** made in the last 12 months showed a **positive correlation**, indicating that a higher number of contacts is statistically associated with the rate of **inactive customers**.\n",
    "---\n",
    "- 5 - The **use of the credit card limit** has a **negative correlation** with customers who have abandoned the credit card service, that is, the **lower the use of the card**, the greater the possibility of them becoming **inactive**. On the other hand, the greater the use of the credit card limit, the greater the possibility of the customer continuing to use the service.\n",
    "---\n",
    "- 6 - The **number of services** has a negative correlation with customers who have stopped using the credit card; the **fewer the number of services**, the greater the possibility of the customer becoming **inactive**.\n",
    "---\n",
    "- 7 - The **number of inactive months** has a **positive correlation** with inactive customers, since the higher this number, the greater the possibility of the customer becoming inactive.\n",
    "---\n",
    "- 8 - **total_amt_chng_q4_q1** has a **negative correlation** with inactive customers; the lower the total value of the variations between the fourth quarter (Q4) of the previous year and the first quarter (Q1), the greater the possibility of the customer becoming inactive.\n",
    "---\n",
    "- 9 - The variables ​​​​**avg_open_to_buy** and **credit_limit** have a **perfect correlation**, indicating that these two variables are passing the same information to future machine learning models.\n",
    "---\n",
    "- 10 - The **other variables** do not have a very relevant correlation with the **churn_target** variable.\n",
    "---\n",
    "- 11 - Initially, it is possible to conclude that the **number of transactions** carried out by customers in recent months has a **very strong and important correlation** with the problem in question, which is the abandonment of credit card customers. The **two variables** that most clearly relate to the **churn_target** variable are variables that provide information about the **number of transactions carried out by customers**. Therefore, it is now possible to question and create a hypothesis about this fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "693e9726-9e91-40bc-9819-0165341e1af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "data_ax = train.select(*continuos_numerical, *discrete_numerical, 'churn_target').toPandas()\n",
    "# Numerical Histograms By Churn Rate\n",
    "GraphicsData(data = data_ax).numerical_histograms(\n",
    "    hue = 'churn_target',\n",
    "    title='Histoplots Of Numerical Variables Grouped By Churn Rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611744b7-e262-4b08-a77c-f690ff239823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data_ax).numerical_barplots(hue = 'churn_target', legend = ['Non-Churner', 'Churner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a77d7029-5155-4406-98df-50632c4532aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data_ax).numerical_boxplots(hue = 'churn_target', legend = ['Non-churner', 'Churner'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bc1cf2-fbfd-4be7-8183-4154541d9859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Observations and insights into the of numeric variables with the 'churn_target' variable.\n",
    "---\n",
    "- 1 - In the variable **total_revolving_bal**, it is possible to observe that a greater distribution of customers who stopped using their credit card is in the **lowest revolving balance values**. A significant portion of these customers have a revolving balance **below \\$500**.\n",
    "---\n",
    "- 2 - In the variable **total_trans_amt**, it is possible to observe that customers who stopped using their credit card have a greater distribution in the **lower transfer values**. Most of these customers made a total of **transfers below \\$2750**.\n",
    "---\n",
    "- 3 - In the variable **total_ct_chng_q4_q1**, it is possible to observe that most customers who kept their credit card service active had an increase of at least **50%** in the number of transactions carried out in relation to Q4 and Q1.\n",
    "---\n",
    "- 4 - In the **avg_utilization_ratio** variable, it is possible to observe that most customers who stopped using their credit card have practically **not used their credit card limit** in the last few months.\n",
    "---\n",
    "- 5 - In the **contacts_count_12_mon** variable, it is possible to observe that most customers who stopped using their credit card have a **number of contacts greater than or equal to 3**.\n",
    "---\n",
    "- 6 - In the **total_trans_ct** variable, it is possible to observe that most customers who stopped using the credit card have a number **below 80 transactions in the last 12 months**. And all customers who have **95 transactions or more** continued to use the credit card.\n",
    "---\n",
    "- 7 - The value of the revolving balances of customers who stopped using their credit cards is relatively lower, around **45% less**, than that of customers who continued using their credit cards.\n",
    "---\n",
    "- 8 - The total transfer values ​​in recent months are lower for customers who stopped using their credit cards.\n",
    "---\n",
    "- 9 - Customers who continued using their credit cards have a reasonably higher number of services.\n",
    "---\n",
    "- 10 - Customers who stopped using their credit cards have a higher number of inactive months and a higher number of contacts in the last 12 months.\n",
    "---\n",
    "- 11 - Customers who stopped using their credit cards have around **34% fewer transactions** in the last 12 months.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b359d3-c1ef-410c-a3b0-5ea8bd7cebd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the relationship of the numeric variables together with the total_trans_ct variable with the churn_target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b63ecd6-9e2f-4377-81f5-9536d0cbcd3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplots vs total_trans_ct Grouped By Churn Rate\n",
    "GraphicsData(data_ax).scatterplots_vs_reference(\n",
    "    x_reference = 'total_trans_ct',\n",
    "    hue = 'churn_target',\n",
    "    title = 'Scatterplot Of Numeric Variables x total_trans_ct Grouped By Churn Rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c61f77-1dcd-4530-a1b2-ca4ae69e8953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Observations and insights into the of numeric variables x  total_trans_ct with the 'churn_target' variable.\n",
    "---\n",
    "- 1 - The variable **total_trans_ct** has a very strong correlation with the variable **churn_target**. Therefore, I chose to check its dispersion with the other variables, grouped by the target variable **churn_target**.\n",
    "\n",
    "The combinations that best defined a good separation between **Non_churners** and **Churners** customers were:\n",
    "\n",
    "- **total_trans_ct** x **total_trans_amt**\n",
    "- **total_trans_ct** x **total_ct_chng_q4_q1**\n",
    "- **total_trans_ct** x **total_ct_chng_q4_q1**\n",
    "\n",
    "These variables refer to the quantity or total value of transactions, reinforcing the previous observations that the number of transactions and their total value reflect, in a certain way, the possible behavior of the customer, indicating whether he or she will continue to use the credit card or stop using it.\n",
    "\n",
    "---\n",
    "- 2 - The variables **total_revolving_bal** and **avg_utilization_ratio**, together with the variable **total_trans_ct**, had a reasonable separation of the **Churn** and **Non-churn** classes, although there was a greater dispersion in the graphs of these two variables.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eecb4d1-6b4d-474e-bf02-1bad6423c39a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the relationship of categorical variables with the cause of the problem\n",
    "---\n",
    "\n",
    "- Checking the statistical relationship of categorical variables using the Chi-Square Test\n",
    "\n",
    "- **Note**:\n",
    "To submit categorical variables to the Chi-Square Test it will be necessary to index them first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144a3e66-0641-4503-89ba-06b31c347d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_idx = train.select(*ordinal_categorical, *nominal_categorical, 'churn_target') \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'Unknown', 6)\n",
    "                .otherwise(F.col('education_level'))) \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'Uneducated', 5)\n",
    "                .otherwise(F.col('education_level'))) \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'High School', 4)\n",
    "                .otherwise(F.col('education_level'))) \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'College', 3)\n",
    "                .otherwise(F.col('education_level'))) \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'Graduate', 2)\n",
    "                .otherwise(F.col('education_level'))) \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'Post-Graduate', 1)\n",
    "                .otherwise(F.col       ('education_level'))) \\\n",
    "    .withColumn('education_level', F.when(F.col('education_level') == 'Doctorate', 0)\n",
    "                .otherwise(F.col('education_level').cast(IntegerType()))) \\\n",
    "    .withColumn('income_category', F.when(F.col('income_category') == 'Unknown', 0)\n",
    "                .otherwise(F.col('income_category'))) \\\n",
    "    .withColumn('income_category', F.when(F.col('income_category') == 'Less than $40K', 1)\n",
    "                .otherwise(F.col('income_category'))) \\\n",
    "    .withColumn('income_category', F.when(F.col('income_category') == '$40K - $60K', 2)\n",
    "                .otherwise(F.col('income_category'))) \\\n",
    "    .withColumn('income_category', F.when(F.col('income_category') == '$60K - $80K', 3)\n",
    "                .otherwise(F.col('income_category'))) \\\n",
    "    .withColumn('income_category', F.when(F.col('income_category') == '$80K - $120K', 4)\n",
    "                .otherwise(F.col('income_category'))) \\\n",
    "    .withColumn('income_category', F.when(F.col('income_category') == '$120K +', 5)\n",
    "                .otherwise(F.col('income_category').cast(IntegerType()))) \\\n",
    "    .withColumn('card_category', F.when(F.col('card_category') == 'Blue', 0)\n",
    "                .otherwise(F.col('card_category'))) \\\n",
    "    .withColumn('card_category', F.when(F.col('card_category') == 'Silver', 1)\n",
    "                .otherwise(F.col('card_category'))) \\\n",
    "    .withColumn('card_category', F.when(F.col('card_category') == 'Gold', 2)\n",
    "                .otherwise(F.col('card_category'))) \\\n",
    "    .withColumn('card_category', F.when(F.col('card_category') == 'Platinum', 3)\n",
    "                .otherwise(F.col('card_category').cast(IntegerType()))) \\\n",
    "    .withColumn('gender', F.when(F.col('gender') == 'F', 0)\n",
    "                .otherwise(F.col('gender'))) \\\n",
    "    .withColumn('gender', F.when(F.col('gender') == 'M', 1)\n",
    "                .otherwise(F.col('gender').cast(IntegerType()))) \\\n",
    "    .withColumn('marital_status', F.when(F.col('marital_status') == 'Unknown', 0)\n",
    "                .otherwise(F.col('marital_status'))) \\\n",
    "    .withColumn('marital_status', F.when(F.col('marital_status') == 'Married', 1)\n",
    "                .otherwise(F.col('marital_status'))) \\\n",
    "    .withColumn('marital_status', F.when(F.col('marital_status') == 'Divorced', 2)\n",
    "                .otherwise(F.col('marital_status'))) \\\n",
    "    .withColumn('marital_status', F.when(F.col('marital_status') == 'Single', 3)\n",
    "                .otherwise(F.col('marital_status').cast(IntegerType()))) \\\n",
    " \n",
    "train_idx.limit(3).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4faed3-147e-4334-8bf2-426d9278fc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_idx = train_idx.toPandas()\n",
    "x = train_idx[['education_level', 'income_category', 'card_category', 'gender','marital_status']]\n",
    "y = train_idx['churn_target']\n",
    "\n",
    "chi_stat, p_value = chi2(x, y)\n",
    "chi_results = pd.DataFrame({\n",
    "    'cat_variables': x.columns,\n",
    "    'chi_score': chi_stat, \n",
    "    'p_value': p_value\n",
    "  })\n",
    "chi_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "764bec5f-56f6-41fb-9728-ad0ccdb5382c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Observations and insights on the Chi-Square Test of category variables with the 'churn_target' variable.\n",
    "---\n",
    "- Initially, the statistical tests used on the categorical variables showed that they did not have much of a relationship with the churn rate.\n",
    "---\n",
    "- The variable **gender** showed a significant relationship if we consider its p_value, but its chi_score is too low to be considered statistically relevant.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b658f292-bf64-411f-8fcd-7570ac43cf71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "data_ax = train.select(*ordinal_categorical, *nominal_categorical, 'churn_target').toPandas()\n",
    "# Categorical Countplots by Churn\n",
    "GraphicsData(data_ax).categorical_countplots(\n",
    "    hue = 'churn_target',\n",
    "    title = 'Countplot of categorical variables by Churn Rate',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720e1f07-447b-4712-9e59-6da108e28b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data_ax).categorical_bar_percentages(\n",
    "    hue = 'churn_target',\n",
    "    title = 'Barplots Of The Individual Churn Rate Percentages Of Each Column Class'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb221f13-5854-4683-a23e-bac3353d91ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Observations and insights on categorical variables with the 'churn_target' variable\n",
    "---\n",
    "- 1 - The **level of education** does not demonstrate a very strong relationship with the rate of customers who stopped using credit card services. Considering that this variable is classified according to the levels of education, it was expected that the higher or lower the level of education, the more likely these customers would choose to continue using the credit card service.\n",
    "\n",
    "- However, it is possible to draw some observations regarding this data, as it directly affects the institution's possible decision-making.\n",
    "- The level of education with the highest churn rate is the **Doctorate**, with around **22%**. The lowest rates are the **Graduate** levels, with **15%**, and **High School**, with **14.40%** of churn rate.\n",
    "---\n",
    "- 2 - **Customers' annual income** does not have a significant influence on the rate of customers who stopped using their credit cards, as all salary ranges follow a practically similar distribution in relation to the churn rate index. Only customers with a salary range of **60k - 80k** had a churn rate of **13.5%**, which is slightly lower compared to the other salary ranges.\n",
    "---\n",
    "- 3 - The **credit card category** shows a significant relationship with the rate of customers who stopped using their credit cards. The **Gold** and **Platinum** categories had a higher-than-average rate of credit card service cancellations compared to the other categories. However, these two categories represent a very small percentage of this data set; together, they do not even have a **2%** share in relation to the other categories.\n",
    "---\n",
    "- 4 - The **Silver** category is the category with the lowest rate of credit card service cancellations **with a 15% churn rate**.\n",
    "---\n",
    "- 5 - Considering the data from this banking institution, it is possible to conclude that the rate of customers with the **Silver**, **Gold** and **Platinum** card brands is very low. We have **93%** of customers with the initial brand, which is the basic **Azul** card. It would be of great value for this institution to invest in a more flexible policy in its card categories. Offering more benefits to its customers and differentiated services through the **Silver**, **Gold** and **Platinum** brands can increase the loyalty rate of its customers.\n",
    "---\n",
    "- 6 - The **gender** of customers declared to have a specific relationship with the churn rate. Women reported having a higher rate of cancellation of the card service than men.\n",
    "---\n",
    "- 7 - The **relationship status** is graphically revealed to have a specific relationship with the churn rate indexes. **Married customers** have a **slightly lower** churn rate index than **single customers**.\n",
    "---\n",
    "- 8 - Initially considering the statistical data and graphs of these categorical variables, it is possible to conclude that they do not have a satisfactory relevance in solving the problem of this institution, which would be the turnover rate. However, we have some variables that somehow present some differences in their classes regarding the churn rate index, which directly affect these variables is the distorted distribution of these variables such as **marital_status and card_category**.\n",
    "---\n",
    "- 9 - At first, I will be keeping these variables and I will analyze their performance during the pre-training process of the models and I will see if it is viable to keep them or discard them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbab4683-b7cc-4fc6-93a8-d7995212271b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Hypothesis testing H0 and H1\n",
    "\n",
    "- I will be considering the main variables that in the analyses demonstrated to have a significant relationship with the churn rate indices of this banking institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b2de7a-4aad-4d72-8eb0-402415d02dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_h_test = train.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dd3083-1571-44bb-9585-2bc778d305bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Hypothesis test with the variable: total_trans_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5324821c-4136-4740-b968-4e28be89b783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_ttest_between_groups(\n",
    "    data = train_h_test,\n",
    "    numerical_col = 'total_trans_ct',\n",
    "    group_col = 'churn_target',   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d825b1a-2505-43f7-931c-4a06994ed79f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Hypothesis test with the variable: total_trans_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd93e7d-908b-4e5d-85b1-710733c2092d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_ttest_between_groups(\n",
    "    data = train_h_test,\n",
    "    numerical_col = 'total_trans_amt',\n",
    "    group_col = 'churn_target',   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22c08d41-e34e-4693-b88b-b53ef703c652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Hypothesis test with the variable: total_revolving_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f4881d-6036-4645-a7e4-68cd24370b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_ttest_between_groups(\n",
    "    data = train_h_test,\n",
    "    numerical_col = 'total_revolving_bal',\n",
    "    group_col = 'churn_target',   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61421bd1-3c21-40dc-b2ea-7626e072f019",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Hypothesis test with the variable: avg_utilization_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9fb249c-dfeb-40c0-ae67-da783a0e56a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_ttest_between_groups(\n",
    "    data = train_h_test,\n",
    "    numerical_col = 'avg_utilization_ratio',\n",
    "    group_col = 'churn_target',   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e0997c5-ab22-45a8-a7b8-9fff92a8a233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Hypothesis test with the variable: total_relationship_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fded507-ea2d-4eaf-a38c-cf814b3eb258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_ttest_between_groups(\n",
    "    data = train_h_test,\n",
    "    numerical_col = 'total_relationship_count',\n",
    "    group_col = 'churn_target',   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2324a99-7522-48f0-9a7d-994303544554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Hypothesis test with the variable: months_inactive_12_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4639b3d9-be61-4e2f-95e0-8d4fc1e57430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_ttest_between_groups(\n",
    "    data = train_h_test,\n",
    "    numerical_col = 'months_inactive_12_mon',\n",
    "    group_col = 'churn_target',   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2527de24-036b-4e94-9c17-2399af387b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving training and testing data\n",
    "---\n",
    "\n",
    "- Removing the **avg_open_to_buy** column as it has a perfect fit with **credit_limit**. I will choose to continue with credit_limit as it has shown to be slightly more associated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a040dac4-6ae8-4838-beea-888d6c58f0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop redundantes continuos variables\n",
    "# Train data\n",
    "train =  train.drop('avg_open_to_buy')\n",
    "# Test data\n",
    "test =  test.drop('avg_open_to_buy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93167132-1ea0-407b-b893-9ea8a0afd5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Data Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80e995c-f30a-468b-8e06-db9fa1219fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# File location and type\n",
    "file_location = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Gold/train'\n",
    "file_type = 'parquet'\n",
    "\n",
    "# Save  Train Dataset\n",
    "DataSpark(dataframe = train, file_location = file_location).save_data(file_type = file_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358cb541-262f-4f9a-8afd-9cc899c296eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Gold/test'\n",
    "file_type = 'parquet'\n",
    "\n",
    "# Save  Test Dataset\n",
    "DataSpark(dataframe = test, file_location = file_location).save_data(file_type = file_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0bcd811-2328-469f-8043-1de5d26cf9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EDA Conclusions\n",
    "---\n",
    "\n",
    "- Through the hypothesis test, it is possible to conclude that the number of transactions made by customers using their credit cards has a significant impact on the churn rate. In the distribution graphs, it is possible to see that the number of transactions for customers who stopped using the credit card services of this institution is much lower. The average number of transactions made by **non-churner customers is almost 69**, while that of **churner customers is 45 transactions**.\n",
    "---\n",
    "- Also through the hypothesis test, it is possible to conclude that the total value of transactions made by customers using their credit cards has a significant impact on the churn rate. Most customers who stopped using credit card services had a total value of **transactions equal to or below \\$2500**.\n",
    "---\n",
    "- The credit card limit utilization rate demonstrated to have a very high significance in relation to the churn rate of this institution, and this fact was validated through the hypothesis test. In this data, there is an **asymmetric distribution with a very long tail to the right**, indicating that most of the credit card limit usage values ​​are in the **lowest values**. This is something to be explored by the stakeholders of this institution, since most of its customers use a **limit much lower than expected**, which can be a problem, since most of the bank's **profits from the credit card** are associated with customer usage. Regarding this problem, we have **non-churner** customers with an **average of 30%** limit usage and **churners** with an **average of 20%** limit usage.\n",
    "---\n",
    "- The number of services maintained on their card by the customer has been shown to have a significant impact on the churn rate of this institution, and this was also proven in the hypothesis tests.\n",
    "---\n",
    "- The **credit limit of credit cards** has an **asymmetric distribution**, with most of the limits **below \\$5000**. This is linked to the customer profile, as most have an annual income **below $40k**. However, the possibility of having a more flexible policy regarding limit increases can be assessed together with the institution's stakeholders, considering the default rate of its customers.\n",
    "---\n",
    "- The **card categories** need to be restructured with more flexible policies regarding the **Silver, Gold and Platinum categories**, as these three categories together represent **less than 7% of the total number of customers**. The **Blue card** category has **93.20% of the total number of customers**, indicating that most customers only keep the initial credit card brand. This may not be a positive factor for the bank, considering that the benefits and advantages tend to be greater than those of cards above the Blue category, and with these advantages it is natural for there to be greater and more continuous customer loyalty.\n",
    "---\n",
    "- Churners had a rate of **20% more contacts with the bank**, it is important for the bank to understand and seek to meet the requests of these customers.\n",
    "---\n",
    "- Most of the customers of this institution were inactive for more than 3 months, and we have **churners** with 15% more months inactive** in relation to **non-churners** customers who continued with the credit card service, indicating that customer inactivity is a problem to be solved and explored by the bank in possible solutions to this problem.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_EDA_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
