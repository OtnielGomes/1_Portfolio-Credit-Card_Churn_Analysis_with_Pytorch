{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04542d55-b694-4ec5-b42f-741fb6aa5974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Credit Card Churn Prediction\n",
    "---  \n",
    "\n",
    "## Project Description  \n",
    "In this project, I will develop a machine learning model â€“ specifically, **a custom neural network architecture built with PyTorch** â€“ to predict the probability of a customer canceling their credit card service (*churn*). The model will follow a **supervised learning** approach, using a labeled dataset where:  \n",
    "- **Customers who left the service** (*churn*) are labeled as **1**.  \n",
    "- **Active customers** (*non-churn*) are labeled as **0**.  \n",
    "\n",
    "---  \n",
    "\n",
    "### CRISP-DM Methodology  \n",
    "The project will follow the CRISP-DM (*Cross-Industry Standard Process for Data Mining*) framework:  \n",
    "\n",
    "| **Stage** | **Objective** | **Key Actions** |  \n",
    "|-----------|---------------|------------------|  \n",
    "| **1. Business Understanding** | Define the impact of churn prediction on customer retention. | - Identify costs of false negatives.<br>- Align metrics with business KPIs. |  \n",
    "| **2. Data Understanding** | Analyze data structure, quality, and variable relationships. | - Exploratory Data Analysis (EDA).<br>- Outlier and correlation detection. |  \n",
    "| **3. Data Preparation** | Prepare data for model training. | - Split training and test data.<br>- Remove redundant variables. |  \n",
    "| **4. Modeling** | Train and compare classical models and neural networks. | - Random Forest/Logistic Regression (baseline).<br>- PyTorch neural network (focus on generalization). |  \n",
    "| **5. Evaluation** | Validate performance with business-oriented metrics. | - AUC-ROC, confusion matrix.<br>- Simulate financial impact. |  \n",
    "| **6. Deployment** | Deploy the model for production use. | - Build a final churn prediction model with customer behavior indicators. |  \n",
    "\n",
    "*This notebook covers the Modeling, Evaluation, and Deployment stages.*  \n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec52ede6-598d-4668-ad6d-48eb63b7ea39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Installs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "887e39d2-8428-4256-aebf-1d3367ed9931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Install in the cluster > Libraries > Install new > PyPI  insert: >>>:\n",
    "\n",
    "- scikit-learn==1.7.0\n",
    "\n",
    "- torch==2.7.1\n",
    "\n",
    "- torchmetrics==1.7.3\n",
    "\n",
    "- tqdm == 4.67.1\n",
    "\n",
    "- ray[tune]==2.47.1\n",
    "\n",
    "- seaborn == 0.13.2\n",
    "\n",
    "- threadpoolctl==3.6.0\n",
    "\n",
    "- optuna==4.4.0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9506a854-afd6-4843-a5b2-1205f5788b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898af20c-3355-40b0-943f-d87b6e2453d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Loading and Modeling:\n",
    "# Pyspark.Api Pandas\n",
    "from pyspark import pandas as ps\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Models of Machine Learning:\n",
    "# Scikit-Learn Preprocessing / Metrics\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# Scikit-Learn Models\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# Torch Metrics\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC, BinaryRecall, BinaryF1Score, BinaryConfusionMatrix, BinaryPrecision, BinaryNegativePredictiveValue, BinaryROC, BinaryPrecisionRecallCurve, BinaryAveragePrecision\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "# Hypertunning Pytorch:\n",
    "# Ray Tunner/Optuna\n",
    "from ray import tune\n",
    "from ray.tune import Checkpoint, Tuner, TuneConfig, RunConfig\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "import ray.cloudpickle as pickle\n",
    "# Tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Graphics:\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Python:\n",
    "# Time\n",
    "import time\n",
    "# Random\n",
    "import random\n",
    "# Partial\n",
    "from functools import partial\n",
    "# OS\n",
    "import os\n",
    "# Tempfile\n",
    "import tempfile\n",
    "# Path\n",
    "from pathlib import Path\n",
    "# Warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f85939-a58c-4dce-b185-eed8137408aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c4d43b-8fd0-4188-b917-76cdd86f88ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c10d684-b74a-4962-8d69-a9286b1fac9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataSparkPS:\n",
    "    \"\"\"\n",
    "    A helper class for managing data loading using PySpark Pandas (Koalas),\n",
    "    with automatic conversion to Pandas DataFrame.\n",
    "\n",
    "    This class facilitates loading CSV or Parquet files into memory using\n",
    "    `pyspark.pandas` (Koalas) and converts them into standard `pandas.DataFrame`\n",
    "    for further analysis or modeling.\n",
    "\n",
    "    Attributes:\n",
    "        file_location (str): Path to the data file.\n",
    "        dataframe (Optional[pd.DataFrame]): Loaded dataset in Pandas format.\n",
    "    \"\"\"\n",
    "    # Init\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_location: str,\n",
    "        dataframe: ps.DataFrame = None,\n",
    "    ):\n",
    "        try:\n",
    "\n",
    "            if dataframe is not None:\n",
    "                self.dataframe = dataframe\n",
    "\n",
    "            self.file_location = file_location\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to initialize dataframe and path: {e}')\n",
    "\n",
    "    # Load Data\n",
    "    def load_data(\n",
    "        self,\n",
    "        file_type: str = 'parquet',\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads data from a file and stores it as a Pandas DataFrame.\n",
    "\n",
    "        This method uses PySpark Pandas (`pyspark.pandas`, formerly Koalas) to read\n",
    "        the specified file (CSV or Parquet) and then converts it into a `pandas.DataFrame`.\n",
    "\n",
    "        Args:\n",
    "            file_type (str): The file format to load. Options are:\n",
    "                - 'parquet': Loads a Parquet file.\n",
    "                - 'csv': Loads a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            Optional[pd.DataFrame]: The loaded data in Pandas format, or None if an error occurs.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the provided file_type is unsupported.\n",
    "            Exception: For any unexpected issues during file loading.\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            if file_type == 'parquet':\n",
    "                dataframe = ps.read_parquet(self.file_location)\n",
    "            \n",
    "            elif file_type == 'csv':\n",
    "                dataframe = ps.read_csv(self.file_location)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type '{file_type}'. Use 'csv' or parquet.\")\n",
    "\n",
    "            print(f'âœ… File loaded successfully from: {self.file_location}')\n",
    "            \n",
    "            # Adjusting the dataframe to the pandas as pd type\n",
    "            self.dataframe = dataframe.to_pandas()\n",
    "            return self.dataframe \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR]  Error loading file '{self.file_location}': {str(e)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ae7feb7-3e8a-4f03-b1d8-071e6efe3c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf99bdd-b45a-4550-a99e-99e67667f571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GraphicsData:\n",
    "\n",
    "    # Init\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: pd.DataFrame,\n",
    "        ):\n",
    "\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if data.empty:\n",
    "                raise ValueError('The provided DataFrame is empty.')\n",
    "\n",
    "            self.data = data\n",
    "\n",
    "        except Exception  as e:\n",
    "            print(f'[Error] Failed to load Dataframe : {str(e)}')\n",
    "    \n",
    "\n",
    "    ###_initializer_subplot_grid Function ###\n",
    "    def _initializer_subplot_grid(\n",
    "        self, \n",
    "        num_columns, \n",
    "        figsize_per_row\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes and returns a standardized matplotlib subplot grid layout.\n",
    "\n",
    "        This utility method calculates the required number of rows based on \n",
    "        the number of variables in the dataset and the desired number of \n",
    "        columns per row. It then creates a grid of subplots accordingly and \n",
    "        applies a consistent styling.\n",
    "\n",
    "        Args:\n",
    "            num_columns (int): Number of subplots per row.\n",
    "            figsize_per_row (int): Vertical size (height) per row in the final figure.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - fig (matplotlib.figure.Figure): The full matplotlib figure object.\n",
    "                - ax (np.ndarray of matplotlib.axes._subplots.AxesSubplot): Flattened array of subplot axes.\n",
    "        \"\"\"\n",
    "        num_vars = len(self.data.columns)\n",
    "        num_rows = (num_vars + num_columns - 1) // num_columns\n",
    "\n",
    "        plt.rc('font', size = 12)\n",
    "        fig, ax = plt.subplots(num_rows, num_columns, figsize = (30, num_rows * figsize_per_row))\n",
    "        ax = ax.flatten()\n",
    "        sns.set(style = 'whitegrid')\n",
    "\n",
    "        return fig, ax\n",
    "\n",
    "    ###_finalize_subplot_layout Function ###\n",
    "    def _finalize_subplot_layout(\n",
    "        self,\n",
    "        fig,\n",
    "        ax,\n",
    "        i: int,\n",
    "        title: str = None,\n",
    "        fontsize: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Finalizes and displays a matplotlib figure by adjusting layout and removing unused subplots.\n",
    "\n",
    "        This method is used after plotting multiple subplots to:\n",
    "        - Remove any unused axes in the grid.\n",
    "        - Set a central title for the entire figure.\n",
    "        - Automatically adjust spacing and layout for better readability.\n",
    "        - Display the resulting plot.\n",
    "\n",
    "        Args:\n",
    "            fig (matplotlib.figure.Figure): The matplotlib figure object containing the subplots.\n",
    "            ax (np.ndarray of matplotlib.axes.Axes): Array of axes (flattened) for all subplots.\n",
    "            i (int): Index of the last used subplot (all subplots after this will be removed).\n",
    "            title (str, optional): Title to be displayed at the top of the entire figure.\n",
    "            fontsize (int, optional): Font size of the overall title. Default is 30.\n",
    "        \"\"\"\n",
    "        for j in range(i + 1, len(ax)):\n",
    "                fig.delaxes(ax[j])\n",
    "        \n",
    "        plt.suptitle(title, fontsize = fontsize, fontweight = 'bold')\n",
    "        plt.tight_layout(rect = [0, 0, 1, 0.97])\n",
    "        plt.show()\n",
    "    \n",
    "    ###_format_single_ax Function ###\n",
    "    def _format_single_ax(\n",
    "        self, \n",
    "        ax,\n",
    "        title: str = None,\n",
    "        fontsize: int = 20,\n",
    "        linewidth: float = 0.9\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Applies standard formatting to a single subplot axis.\n",
    "\n",
    "        This method configures a single axis by:\n",
    "        - Setting the title with specified font size and bold style.\n",
    "        - Hiding the x and y axis labels.\n",
    "        - Adding dashed grid lines for both axes with configurable line width.\n",
    "\n",
    "        Args:\n",
    "            ax (matplotlib.axes.Axes): The axis to be formatted.\n",
    "            title (str, optional): Title text for the axis. Defaults to None.\n",
    "            fontsize (int, optional): Font size for the title. Defaults to 20.\n",
    "            linewidth (float, optional): Width of the dashed grid lines. Defaults to 0.9.\n",
    "        \"\"\"\n",
    "        ax.set_title(title, fontsize = fontsize, fontweight = 'bold')\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_ylabel(None)\n",
    "        ax.grid(axis = 'y', which = 'major', linestyle = '--', linewidth = linewidth)\n",
    "        ax.grid(axis = 'x', which = 'major', linestyle = '--', linewidth = linewidth)\n",
    "\n",
    "    ### Numerical histograms Function ###\n",
    "    def numerical_histograms(\n",
    "        self, \n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        color: str = '#a2bffe',\n",
    "        hue: str = None,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        title: str = 'Histograms of Numerical Variables',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots histograms with KDE (Kernel Density Estimation) for all numerical columns in the dataset.\n",
    "\n",
    "        Optionally groups the histograms by a categorical target variable using different colors (hue).\n",
    "        Useful for visualizing the distribution of numerical features and how they differ between groups.\n",
    "\n",
    "        Args:\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height of each row in inches (controls vertical spacing).\n",
    "            color (str): Default color for histograms when `hue` is not specified.\n",
    "            hue (str, optional): Name of the column used for grouping (e.g., 'churn_target'). Must be categorical.\n",
    "            palette (list): List of colors for hue levels. Only used if `hue` is provided.\n",
    "            title (str): Title of the entire figure layout.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If plotting fails due to missing columns, incorrect types, or rendering errors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "                sns.histplot(\n",
    "                    data = self.data,\n",
    "                    x = column,\n",
    "                    kde = True,\n",
    "                    hue = hue,\n",
    "                    palette = palette if hue else None,\n",
    "                    edgecolor = 'black',\n",
    "                    alpha = 0.4 if hue else 0.7,\n",
    "                    color = None if hue else color,\n",
    "                    ax = ax[i],\n",
    "                )\n",
    "                # Config Ax's\n",
    "                self._format_single_ax(ax[i], title = f'Histogram of variable: {column}')\n",
    "                \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e:\n",
    "            print(f'[Error] Failed to generate numeric histograms: {str(e)}')\n",
    "\n",
    "    ### Numerical Boxplots Function ###\n",
    "    def numerical_boxplots(\n",
    "        self, \n",
    "        hue: str = None, \n",
    "        num_columns: int = 3,\n",
    "        figsize_per_row: int = 6,\n",
    "        palette: list = ['#b0ff9d', '#db5856'],\n",
    "        color: str = '#a2bffe',\n",
    "        showfliers: bool = False,\n",
    "        title: str = 'Boxplots of Numerical Variables',\n",
    "        legend: list = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots boxplots for each numerical variable in the dataset.\n",
    "\n",
    "        Optionally groups the boxplots by a categorical hue variable (e.g., churn target), \n",
    "        allowing for comparison of distributions between groups. Helps identify outliers, \n",
    "        skewness, and variability in each feature.\n",
    "\n",
    "        Args:\n",
    "            hue (str, optional): Column name to group the boxplots (e.g., 'churn_target').\n",
    "                                If None, individual boxplots are created without grouping.\n",
    "            num_columns (int): Number of plots per row in the subplot grid.\n",
    "            figsize_per_row (int): Height (in inches) of each row of plots.\n",
    "            palette (list): Color palette to use when `hue` is provided.\n",
    "            color (str): Single color to use when `hue` is not specified.\n",
    "            showfliers (bool): Whether to display outlier points in the boxplots (default: False).\n",
    "            title (str): Overall title for the subplot grid.\n",
    "            legend (list): Custom legend labels to replace default tick labels when `hue` is present.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the hue column is not found in the DataFrame.\n",
    "            Exception: If plotting fails due to unexpected issues.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Entry checks\n",
    "            if hue and hue not in self.data.columns:\n",
    "                raise ValueError(f\"Column '{hue}' not in the DataFrame.\")\n",
    "\n",
    "            numeric_cols = self.data.select_dtypes(include = 'number').columns.tolist()\n",
    "            if hue and hue in numeric_cols:\n",
    "                numeric_cols.remove(hue)\n",
    "\n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "\n",
    "            for i, column in enumerate(numeric_cols):\n",
    "                    sns.boxplot(\n",
    "                        data = self.data,\n",
    "                        x = hue if hue else column,\n",
    "                        y = column if hue else None,\n",
    "                        hue = hue if hue else None,\n",
    "                        palette = palette if hue else None,\n",
    "                        color = None if hue else color,\n",
    "                        showfliers = showfliers,\n",
    "                        legend = False,\n",
    "                        ax = ax[i]\n",
    "                    )\n",
    "\n",
    "                    # Config Ax's\n",
    "                    if len(legend) > 0:\n",
    "                        ax[i].set_xticks([l for l in range(0, len(legend))])\n",
    "                        ax[i].set_xticklabels(legend, fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "                    self._format_single_ax(ax[i], f'Box plot of variable: {column}')\n",
    "                    ax[i].set_yticklabels([])\n",
    "                    sns.despine(ax = ax[i], top = True, right = True, left = True, bottom = True)\n",
    "            \n",
    "            # Show Graphics\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)\n",
    "        except Exception as e: \n",
    "            print(f'[ERROR] Failed to generate numerical boxplots: {str(e)}')\n",
    "    \n",
    "    ### Models Performance Barplots Function ###\n",
    "    def models_performance_barplots(\n",
    "        self,\n",
    "        models_col: str = None,\n",
    "        palette = None,\n",
    "        title: str = 'Models Performance Comparison',\n",
    "        num_columns: int = 1,\n",
    "        figsize_per_row: int = 9\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates bar plots to compare the performance of multiple models across different metrics.\n",
    "\n",
    "        Args:\n",
    "            models_col (str, optional): Column name containing model identifiers.\n",
    "            palette (list or seaborn color palette, optional): Color palette for the bar plots.\n",
    "                Defaults to a 'viridis' palette if None.\n",
    "            title (str, optional): Main title for the figure. Defaults to 'Models Performance Comparison'.\n",
    "            num_columns (int, optional): Number of subplot columns. Defaults to 1.\n",
    "            figsize_per_row (int, optional): Height of each subplot row in inches. Defaults to 9.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error generating the bar plots.\n",
    "\n",
    "        Notes:\n",
    "            - The method expects `self.data` to contain one column for model names\n",
    "            and one or more columns with numeric performance metrics.\n",
    "            - Each subplot will represent a different metric.\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            # Define palette\n",
    "            if palette is None:\n",
    "                palette = sns.color_palette('viridis', len(self.data[models_col].unique()))\n",
    "            \n",
    "            # Define AX and Fig\n",
    "            fig, ax = self._initializer_subplot_grid(num_columns, figsize_per_row)\n",
    "            # Ax Flatten\n",
    "            ax = ax.flatten()\n",
    "\n",
    "            # Iterate over metrics (excluding 'Model')\n",
    "            for i, column in enumerate(self.data.drop(columns = models_col).columns):\n",
    "\n",
    "                barplot = sns.barplot(\n",
    "                    data = self.data,\n",
    "                    x = models_col,\n",
    "                    y = column,\n",
    "                    hue = models_col,\n",
    "                    dodge = False,\n",
    "                    edgecolor = 'white',\n",
    "                    saturation = 1,\n",
    "                    palette = palette,\n",
    "                    ax = ax[i]\n",
    "                )\n",
    "\n",
    "                # Formatting axis\n",
    "                self._format_single_ax(ax[i], title = column, fontsize = 25)\n",
    "                ax[i].tick_params(axis = 'x', labelsize = 20)\n",
    "                ax[i].set_yticklabels([])\n",
    "                sns.set(style = 'whitegrid')\n",
    "\n",
    "                # Add values on bars\n",
    "                for v in barplot.patches:\n",
    "                    barplot.annotate(\n",
    "                        f'{v.get_height():.4f}',\n",
    "                        (v.get_x() + v.get_width() / 2., v.get_height() / 1.06),\n",
    "                        ha = 'center',\n",
    "                        va = 'top',\n",
    "                        xytext = (0, 0),\n",
    "                        textcoords = 'offset points',\n",
    "                        fontsize = 20,\n",
    "                        fontweight = 'bold',\n",
    "                        color = 'white'\n",
    "                    )\n",
    "\n",
    "            # Finalize plot\n",
    "            self._finalize_subplot_layout(fig, ax, i, title = title)     \n",
    "        \n",
    "        except Exception as e:\n",
    "                print(f'[ERROR] Failed to generate model performance barplots: {str(e)}.')\n",
    "                \n",
    "    ### Plot KDE Predictions Function ###\n",
    "    def plot_kde_predictions(\n",
    "        self,\n",
    "        palette: list = ['#12e193', '#feb308'],\n",
    "        predictions: str = None,\n",
    "        labels: str = None,\n",
    "        title: str = 'Prediction Probabilities'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the probability distributions of predictions using Kernel Density Estimation (KDE).\n",
    "\n",
    "        Args:\n",
    "            palette (list, optional): List of colors for each class. Defaults to ['#12e193', '#feb308'].\n",
    "            predictions (str, optional): Column name containing the predicted probabilities.\n",
    "            labels (str, optional): Column name containing the true labels.\n",
    "            title (str, optional): Title for the plot. Defaults to 'Prediction Probabilities'.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error generating the KDE plot.\n",
    "\n",
    "        Notes:\n",
    "            - The method expects `self.data` to contain the prediction probabilities and true labels.\n",
    "            - KDE plots are useful for visualizing class separation in probabilistic predictions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            # Creating figures and setting font size\n",
    "            plt.rc('font', size = 10)\n",
    "            fig, ax = plt.subplots(figsize = (12, 4))\n",
    "\n",
    "            sns.kdeplot(\n",
    "                data = self.data,\n",
    "                x = predictions,\n",
    "                hue = labels,\n",
    "                fill = True,\n",
    "                alpha = 0.4,\n",
    "                bw_adjust = 1,\n",
    "                palette = palette,\n",
    "                linewidth = 1,\n",
    "                ax = ax\n",
    "            )\n",
    "\n",
    "            # Axis and title adjustments\n",
    "            ax.set_title(title, fontsize = 14)\n",
    "            ax.set_xlabel(None)\n",
    "            ax.set_ylabel(None)\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "            ## Grade and style\n",
    "            ax.grid(axis = 'y', linestyle = '--', linewidth = 0.3)\n",
    "            ax.grid(axis = 'x', linestyle = '--', linewidth = 0.3)\n",
    "            sns.set(style = 'whitegrid')\n",
    "            sns.despine(ax = ax, top = True, right = True, left = True, bottom = False)\n",
    "\n",
    "            # Show Graphics\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate prediction probability graph {str(e)}.')\n",
    "\n",
    "    ### Plot ROC and Precision Curves Function ###\n",
    "    def plot_roc_pr_curves(\n",
    "        preds, \n",
    "        labels\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the Receiver Operating Characteristic (ROC) curve and the Precision-Recall (PR) curve\n",
    "        for a binary classification model, along with their respective AUC metrics.\n",
    "\n",
    "        Args:\n",
    "            preds (Tensor or array-like): Predicted probabilities for the positive class.\n",
    "            labels (Tensor or array-like): True binary labels (0 or 1).\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error generating or plotting the curves.\n",
    "\n",
    "        Notes:\n",
    "            - Computes and plots:\n",
    "                * ROC curve with AUROC (Area Under ROC Curve).\n",
    "                * Precision-Recall curve with AUPRC (Area Under Precision-Recall Curve).\n",
    "            - Uses TorchMetrics for metric computation.\n",
    "            - Useful for evaluating model performance, especially in imbalanced datasets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Metrics\n",
    "\n",
    "            # ROC\n",
    "            fpr, tpr, _ = BinaryROC()(preds, labels)\n",
    "            auroc_value = BinaryAUROC()(preds, labels).item()\n",
    "\n",
    "            # Precision-Recall\n",
    "            precision_vals, recall_vals, _ = BinaryPrecisionRecallCurve()(preds, labels)\n",
    "            auprc_value = BinaryAveragePrecision()(preds, labels).item()\n",
    "\n",
    "            # Ax and Fig\n",
    "            fig, axes = plt.subplots(1, 2, figsize = (12, 5))\n",
    "\n",
    "            # ROC Curve\n",
    "            axes[0].plot(fpr, tpr, color = '#1f77b4', lw = 2, label = f'ROC (AUROC = {auroc_value:.3f})')\n",
    "            axes[0].plot([0, 1], [0, 1], color = 'gray', linestyle = '--', label = 'Random (AUROC = 0.5)')\n",
    "            axes[0].set_xlabel('False Positive Rate', fontsize = 12)\n",
    "            axes[0].set_ylabel('True Positive Rate', fontsize = 12)\n",
    "            axes[0].set_title('ROC Curve', fontsize = 14, fontweight = 'bold')\n",
    "            axes[0].legend(loc = 'lower right', fontsize = 10)\n",
    "\n",
    "            # PR Curve\n",
    "            axes[1].plot(recall_vals, precision_vals, color = 'darkorange', lw = 2, label = f'PR Curve (AUPRC = {auprc_value:.3f})')\n",
    "            axes[1].set_xlabel('Recall', fontsize = 12)\n",
    "            axes[1].set_ylabel('Precision', fontsize = 12)\n",
    "            axes[1].set_title('Precision-Recall Curve', fontsize = 14, fontweight = 'bold')\n",
    "            axes[1].legend(loc='upper right', fontsize = 10)\n",
    "\n",
    "            # Grid\n",
    "            for ax in axes:\n",
    "                ax.grid(True, linestyle = '--', linewidth = 0.5)\n",
    "\n",
    "            # Show Graphics\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Failed to generate ROC and PR curves: {str(e)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be6aa27-d8e5-4ba5-9792-627aeeb8aec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cross Validation Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb1f056-c765-46d7-b2a5-1929e645b2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation_ml(\n",
    "    models,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 33,\n",
    "    shuffle: bool = True,\n",
    "    scoring: str = 'roc_auc',  \n",
    "):\n",
    "    \"\"\"\n",
    "    Performs stratified K-fold cross-validation on a list of machine learning models.\n",
    "\n",
    "    This function receives multiple models (e.g., pipelines), trains and evaluates them\n",
    "    using Stratified K-Fold cross-validation, and prints the mean and standard deviation \n",
    "    of the chosen scoring metric for each model.\n",
    "\n",
    "    Args:\n",
    "        models (list): A list of tuples with (model_name, model_pipeline), where each \n",
    "            model_pipeline follows the scikit-learn API (i.e., implements .fit and .predict).\n",
    "        x_train (array-like): Feature set used for training and validation.\n",
    "        y_train (array-like): Target labels corresponding to x_train.\n",
    "        n_splits (int, optional): Number of folds for cross-validation. Defaults to 5.\n",
    "        random_state (int, optional): Random seed for reproducibility. Defaults to 33.\n",
    "        shuffle (bool, optional): Whether to shuffle the data before splitting. Defaults to True.\n",
    "        scoring (str, optional): Scoring metric to use (e.g., 'roc_auc', 'accuracy', 'f1'). \n",
    "            Defaults to 'roc_auc'.\n",
    "\n",
    "    Prints:\n",
    "        For each model:\n",
    "            - Name of the model\n",
    "            - Mean cross-validation score\n",
    "            - Standard deviation of the score across folds\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        # Cross Validation\n",
    "        cv = StratifiedKFold(n_splits = n_splits, random_state = random_state, shuffle = shuffle)\n",
    "\n",
    "        # Metrics Cross validation\n",
    "        results, names = [], []\n",
    "\n",
    "        for name, pipeline in models:\n",
    "\n",
    "            cv_results = cross_val_score(\n",
    "                pipeline,\n",
    "                x_train, \n",
    "                y_train,\n",
    "                cv = cv,\n",
    "                scoring = scoring\n",
    "\n",
    "            )\n",
    "            results.append(cv_results)\n",
    "            names.append(name)\n",
    "\n",
    "            print('#' * 30)\n",
    "            print(f'ðŸŽ¯ {name}: ')\n",
    "            print(f'ðŸ”µ AUC ROC Mean: {cv_results.mean():.6f} ')\n",
    "            print(f'ðŸŸ£ STD Metrics: {cv_results.std():.6f}\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'[ERROR] Failed to run ml cross validation {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9eac17e-d688-40a3-aae7-bbb86c7cd4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9990243-cedf-4027-bc85-651322467737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PyTorch:\n",
    "\n",
    "    ### Init ###\n",
    "    def __init__(\n",
    "        self, \n",
    "        name: str = 'PyTorch_object'\n",
    "    ):\n",
    "        self.name = name\n",
    "\n",
    "    ### Dataset Pytorch Class ###\n",
    "    class Dataset(torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset class for handling tabular data with separate indices\n",
    "        for categorical features, numerical features, and labels.\n",
    "\n",
    "        This class enables flexible extraction of different feature types from\n",
    "        a preprocessed dataset (e.g., NumPy array) and returns them as PyTorch tensors,\n",
    "        which is useful for deep learning models that treat categorical and numerical\n",
    "        features differently (e.g., when using embeddings).\n",
    "\n",
    "        Attributes:\n",
    "            data (array-like): The full dataset (e.g., NumPy array) containing all features and labels.\n",
    "            cat_idx (list): A list with two elements [start, end] indicating the range of categorical columns.\n",
    "            num_idx (list): A list with two elements [start, end] indicating the range of numerical columns.\n",
    "            label_idx (list): A list with two elements [start, end] indicating the range of label columns.\n",
    "\n",
    "        Methods:\n",
    "            __len__(): Returns the total number of samples in the dataset.\n",
    "            __getitem__(idx): Returns a tuple (categorical_data, numerical_data, labels) for a given index.\n",
    "        \"\"\"\n",
    "        # Initializing Attributes\n",
    "        def __init__(\n",
    "            self, \n",
    "            dataset,\n",
    "            cat_idx: list = [],\n",
    "            num_idx: list = [],\n",
    "            label_idx: list = [],\n",
    "        ):\n",
    "            try:\n",
    "\n",
    "                # Loading data\n",
    "                self.data = dataset\n",
    "                self.cat_idx = cat_idx\n",
    "                self.num_idx = num_idx\n",
    "                self.label_idx = label_idx\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to load dataset: {str(e)}.')\n",
    "        \n",
    "        # Len function Torch\n",
    "        def __len__(\n",
    "            self,\n",
    "        ):\n",
    "            try:\n",
    "\n",
    "                return len(self.data)  \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to len data: {str(e)}.')\n",
    "\n",
    "        # Get item \n",
    "        def __getitem__(\n",
    "            self,\n",
    "            idx: int,\n",
    "        ):\n",
    "            try:\n",
    "\n",
    "                # Split of categorical, numerical and label variables\n",
    "                categorical_data = self.data[idx][self.cat_idx[0] : self.cat_idx[1]]\n",
    "                numerical_data = self.data[idx][self.num_idx[0] : self.num_idx[1]]\n",
    "                labels = self.data[idx][self.label_idx[0] :  self.label_idx[1]]\n",
    "\n",
    "                # Transform to tensors\n",
    "                categorical_data = torch.from_numpy(categorical_data.astype(np.int64))\n",
    "                numerical_data = torch.from_numpy(numerical_data.astype(np.float32))\n",
    "                labels = torch.from_numpy(labels.astype(np.float32))\n",
    "\n",
    "                return categorical_data, numerical_data, labels\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to find the indices and transform the data into tensors: {str(e)}')\n",
    "\n",
    "    ### Neural NetWork Class ###\n",
    "    class Net(nn.Module):\n",
    "        \"\"\"\n",
    "        Neural network for binary classification using both categorical and numerical features.\n",
    "\n",
    "        This architecture uses embedding layers for categorical features and dense layers \n",
    "        for numerical features. Both feature types are combined and passed through a \n",
    "        deep neural network with batch normalization, dropout, and LeakyReLU activations.\n",
    "        \"\"\"\n",
    "        # Initializing network parameters and attributes\n",
    "        def __init__(\n",
    "            self, \n",
    "            l1: int = 256,\n",
    "            l2: int = 128,\n",
    "            l3: int = 64,\n",
    "            dropout_rate: float = 0.5,\n",
    "            classes_per_cat: list = [2, 4, 7, 6, 4],  # Number of classes per categorical variable\n",
    "            num_numerical_features: int = 13,\n",
    "            prior_minoritary_class: float = 0,\n",
    "            negative_slope: float = 0.01\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initializes the network architecture and parameters.\n",
    "\n",
    "            Args:\n",
    "                l1 (int): Number of units in the first dense layer for numerical features.\n",
    "                l2 (int): Number of units in the first combined layer (numerical + categorical).\n",
    "                l3 (int): Number of units in the second combined layer.\n",
    "                dropout_rate (float): Dropout rate applied after the first combined layer.\n",
    "                classes_per_cat (list): List with the number of classes per categorical feature.\n",
    "                num_numerical_features (int): Number of input numerical features.\n",
    "                prior_minoritary_class (float): Proportion of the positive (minority) class; used to adjust output layer bias.\n",
    "                negative_slope (float): Negative slope used in the LeakyReLU activation function.\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                # Running __init__ of the nn.Module class\n",
    "                super().__init__()\n",
    "                \n",
    "                # Embedding dims\n",
    "                embedding_dims = [int(min(50, np.sqrt(n))) for n in  classes_per_cat]\n",
    "\n",
    "                # Creating embeddings dynamically\n",
    "                self.embeddings = nn.ModuleList(\n",
    "                    [nn.Embedding(num_embeddings, emb_dim) for num_embeddings, emb_dim in zip(classes_per_cat, embedding_dims)]\n",
    "                )\n",
    "\n",
    "                # Total dimensions of the embedding layers\n",
    "                self.total_embedding_dim = sum(embedding_dims)\n",
    "\n",
    "                # Layer for numeric variables\n",
    "                self.numerical_layer = nn.Linear(num_numerical_features, l1)\n",
    "                self.bn_num = nn.BatchNorm1d(l1)\n",
    "                \n",
    "                # Combined the layers\n",
    "                # Layer 1\n",
    "                self.combined_layer_1 = nn.Linear(self.total_embedding_dim + l1, l2)\n",
    "                self.bn1 = nn.BatchNorm1d(l2)\n",
    "\n",
    "                # Layer 2\n",
    "                self.combined_layer_2 = nn.Linear(l2, l3)\n",
    "                self.bn2 = nn.BatchNorm1d(l3)\n",
    "\n",
    "                # Dropout Layer\n",
    "                self.dropout_layer = nn.Dropout(dropout_rate)\n",
    "\n",
    "                # Output Layer\n",
    "                self.output_layer = nn.Linear(l3, 1)\n",
    "                \n",
    "                # Activation LeakyReLU\n",
    "                self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "\n",
    "                # Bias adjustment according to the probability of the negative class\n",
    "                self.prior_minoritary_class = prior_minoritary_class\n",
    "\n",
    "                # Initialization weights\n",
    "                self._init_weights()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERRO] Failed to load network attributes: {str(e)}')\n",
    "        \n",
    "        # Initialization Weights\n",
    "        def _init_weights(\n",
    "            self,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initializes the weights of the network layers.\n",
    "\n",
    "            - Dense layers are initialized using Kaiming Normal initialization.\n",
    "            - Embedding layers are initialized using Xavier Uniform initialization.\n",
    "            - The output layer bias is set based on the prior probability of the minority class (if provided).\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Linear Layers\n",
    "                for layer in [self.numerical_layer, self.combined_layer_1, self.combined_layer_2]:\n",
    "                    \n",
    "                    # Weights Linear Layers\n",
    "                    if isinstance(layer, nn.Linear):\n",
    "                        nn.init.kaiming_normal_(layer.weight, mode = 'fan_out', nonlinearity = 'leaky_relu')\n",
    "                        # Bias Linear Layers\n",
    "                        if layer.bias is not None:\n",
    "                            nn.init.zeros_(layer.bias)\n",
    "                \n",
    "                # Embbedings\n",
    "                for embedding in self.embeddings:\n",
    "                    nn.init.xavier_uniform_(embedding.weight)\n",
    "                \n",
    "                # Special adjustment for output layer\n",
    "                nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "                # Initializing the output layer bios for imbalanced data \n",
    "                # Replacing with the actual proportion of the positive class\n",
    "                if self.prior_minoritary_class > 0:\n",
    "                    eps = 1e-6\n",
    "                    prior_adjusted = max(min(self.prior_minoritary_class, 1 - eps), eps)\n",
    "                    nn.init.constant_(\n",
    "                        self.output_layer.bias, \n",
    "                        torch.log(torch.tensor(prior_adjusted / (1 - prior_adjusted), dtype = torch.float))\n",
    "                    )\n",
    "                else:\n",
    "                    nn.init.zeros_(self.output_layer.bias)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to perform weight initialization: {str(e)}.')\n",
    "\n",
    "        # Forward\n",
    "        def forward(\n",
    "            self,\n",
    "            cat_data, \n",
    "            num_data\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Forward pass of the neural network.\n",
    "\n",
    "            Args:\n",
    "                cat_data (torch.Tensor): Tensor of categorical feature indices.\n",
    "                    Shape: (batch_size, num_categorical_features)\n",
    "                num_data (torch.Tensor): Tensor of numerical feature values.\n",
    "                    Shape: (batch_size, num_numerical_features)\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Logits output by the network.\n",
    "                    Shape: (batch_size, 1)\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                # Processing categorical variables with embeddings\n",
    "                embedded_features = [embedding(cat_data[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "                combined_embeddings = torch.cat(embedded_features, dim = 1)\n",
    "\n",
    "                # Processing of numerical variables\n",
    "                numerical_out = self.numerical_layer(num_data)\n",
    "                numerical_out = self.bn_num(numerical_out)\n",
    "                numerical_out = self.leaky_relu(numerical_out)\n",
    "\n",
    "                # Combining embeddings with numerical data\n",
    "                combined = torch.cat([numerical_out, combined_embeddings], dim = 1)\n",
    "\n",
    "                # Passage through the neural network\n",
    "                # combined_layer_1\n",
    "                x = self.combined_layer_1(combined)\n",
    "                x = self.bn1(x)\n",
    "                x = self.leaky_relu(x)\n",
    "                x = self.dropout_layer(x)\n",
    "\n",
    "                # combined_layer_2\n",
    "                x = self.combined_layer_2(x)\n",
    "                x = self.bn2(x)\n",
    "                x = self.leaky_relu(x)\n",
    "\n",
    "                # Logits\n",
    "                logits = self.output_layer(x)\n",
    "\n",
    "                return logits\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to execute neural network forward: {str(e)}')\n",
    "    \n",
    "    \n",
    "    ### Focal Loss Class ###\n",
    "    class FocalLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        Focal Loss for binary classification tasks.\n",
    "\n",
    "        This loss function addresses class imbalance by down-weighting easy examples and focusing \n",
    "        training on hard negatives using a modulating factor.\n",
    "\n",
    "        Attributes:\n",
    "            gamma (float): Focusing parameter that reduces the relative loss for well-classified examples.\n",
    "            reduction (str): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n",
    "            alpha (torch.Tensor): Class weights for positive and negative samples.\n",
    "        \"\"\"\n",
    "        # Initializing attributes\n",
    "        def __init__(\n",
    "            self,\n",
    "            alpha: list = [1.0, 1.0],\n",
    "            gamma: float = 2,\n",
    "            reduction: str = 'mean'\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initializes the FocalLoss module.\n",
    "\n",
    "            Args:\n",
    "                alpha (list): Class weighting factors [alpha_negative, alpha_positive].\n",
    "                gamma (float): Focusing parameter to reduce the impact of easy examples.\n",
    "                reduction (str): Specifies the reduction to apply to the output:\n",
    "                    - 'none': no reduction\n",
    "                    - 'mean': average of the loss\n",
    "                    - 'sum': sum of the loss\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Running __init__ of the FocalLoss class\n",
    "                super().__init__()\n",
    "                self.gamma = gamma\n",
    "                self.reduction = reduction\n",
    "                self.register_buffer('alpha', torch.tensor(alpha, dtype = torch.float))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed load FocalLoss atributes: {str(e)}')\n",
    "\n",
    "        # Foward Focal Loss\n",
    "        def forward(\n",
    "            self,\n",
    "            inputs,\n",
    "            targets\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Computes the focal loss between the predictions and targets.\n",
    "\n",
    "            Args:\n",
    "                inputs (torch.Tensor): Raw model outputs (logits) of shape (batch_size,).\n",
    "                targets (torch.Tensor): Ground truth binary labels of shape (batch_size,).\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: The computed loss. The shape depends on the reduction method:\n",
    "                    - 'none': returns loss per sample\n",
    "                    - 'mean': returns scalar average loss\n",
    "                    - 'sum': returns scalar summed loss\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Predicted probabilities  \n",
    "                probs = torch.sigmoid(inputs)\n",
    "                probs = torch.clamp(probs, min = 1e-7, max = 1-1e-7)\n",
    "\n",
    "                # Targets\n",
    "                targets = targets.float()\n",
    "\n",
    "                # BCE Loss\n",
    "                bce_loss = F.binary_cross_entropy(probs, targets, reduction = 'none')\n",
    "                \n",
    "                # Focusing factor (1 - p_t)^Î³\n",
    "                focal_factor = (1 - probs).pow(self.gamma) * targets + probs.pow(self.gamma) * (1 - targets)\n",
    "                \n",
    "                # Adjust the alpha weight for each class\n",
    "                alpha_factor = self.alpha[1] * targets + self.alpha[0] * (1 - targets)\n",
    "                \n",
    "                # Loss\n",
    "                loss = alpha_factor * focal_factor * bce_loss\n",
    "\n",
    "                # Reduction\n",
    "                if self.reduction == 'mean':\n",
    "                    return loss.mean()\n",
    "\n",
    "                elif self.reduction == 'sum':\n",
    "                    return loss.sum()\n",
    "                \n",
    "                elif self.reduction == 'none':\n",
    "                    return loss\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to execute forward in Focal Loss function: {str(e)}')\n",
    "    \n",
    "    ### Early Sopping Class ###\n",
    "    class EarlyStopping:\n",
    "        \n",
    "        \"\"\"\n",
    "        Implements early stopping to terminate training when a monitored metric stops improving.\n",
    "\n",
    "        Attributes:\n",
    "            patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "            min_delta (float): Minimum change in the monitored score to qualify as an improvement.\n",
    "            mode (str): One of ['min', 'max']. In 'min' mode, training stops when the score increases;\n",
    "                in 'max' mode, training stops when the score decreases.\n",
    "            save_path (str or Path): Path to save the best model.\n",
    "            tempfile_save (bool): Whether to save using a temporary file.\n",
    "            verbose (bool): If True, prints messages during training.\n",
    "            best_score (float): Best score observed so far.\n",
    "            counter (int): Number of epochs since the last improvement.\n",
    "            early_stop (bool): Whether early stopping was triggered.\n",
    "            best_epoch (int): Epoch number at which the best score was achieved.\n",
    "        \"\"\"\n",
    "        # Initializing attributes\n",
    "        def __init__(\n",
    "            self,\n",
    "            patience: int = 15,\n",
    "            min_delta: float = 1e-4,\n",
    "            mode: str = 'max',\n",
    "            save_path = None,\n",
    "            tempfile_save = False,\n",
    "            verbose: bool = True,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Initializes the EarlyStopping object.\n",
    "\n",
    "            Args:\n",
    "                patience (int): Number of epochs to wait for improvement before stopping.\n",
    "                min_delta (float): Minimum score improvement to reset the patience counter.\n",
    "                mode (str): 'max' for maximizing the metric, 'min' for minimizing.\n",
    "                save_path (str or Path): File path to save the best model.\n",
    "                tempfile_save (bool): Use temporary file handling for saving the model.\n",
    "                verbose (bool): If True, logs progress and stopping messages.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                self.patience = patience\n",
    "                self.min_delta = min_delta\n",
    "                self.mode = mode\n",
    "                self.save_path = save_path\n",
    "                self.verbose = verbose\n",
    "                self.tempfile_save = tempfile_save\n",
    "                self.best_score = None\n",
    "                self.counter = 0\n",
    "                self.early_stop = False\n",
    "                self.best_epoch = None\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to initialize attributes of Early Stopping class: {str(e)}.')\n",
    "        \n",
    "        # Call\n",
    "        def __call__(\n",
    "            self,\n",
    "            epoch: int,\n",
    "            score: float, \n",
    "            model = None,\n",
    "        ):  \n",
    "            \"\"\"\n",
    "            Evaluates whether the model has improved and saves the best model if applicable.\n",
    "\n",
    "            Args:\n",
    "                epoch (int): Current epoch number.\n",
    "                score (float): The value of the monitored metric at the current epoch.\n",
    "                model (torch.nn.Module, optional): The model to save if improvement is detected.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # If the model as been improved\n",
    "                improved = False\n",
    "\n",
    "                # First score\n",
    "                if self.best_score is None:\n",
    "                    improved = True\n",
    "\n",
    "                # Max Mode\n",
    "                elif self.mode == 'max' and score > self.best_score + self.min_delta:\n",
    "                    improved = True\n",
    "                \n",
    "                # Min Mode\n",
    "                elif self.mode == 'min' and score < self.best_score - self.min_delta:\n",
    "                    improved = True\n",
    "                \n",
    "                # Improved Score\n",
    "                if improved:\n",
    "                    \n",
    "                    self.best_score = score\n",
    "                    self.counter = 0\n",
    "                    self.early_stop = False\n",
    "                    self.best_epoch = epoch if epoch is not None else 0\n",
    "\n",
    "                    # Saving the model and the path\n",
    "                    if model and self.save_path:\n",
    "                        \n",
    "                        # Tempfile\n",
    "                        if self.tempfile_save:\n",
    "                            model_to_save = model.module if isinstance(model, nn.DataParallel) else model \n",
    "                            torch.save(model_to_save.state_dict(), self.save_path.name)\n",
    "\n",
    "                        # No Tempfile\n",
    "                        else:\n",
    "                            model_to_save = model.module if isinstance(model, nn.DataParallel) else model \n",
    "                            torch.save(model_to_save.state_dict(), self.save_path)\n",
    "                        \n",
    "                        if self.verbose:\n",
    "                            print(f'âœ… Model Improvement (Epoch: {self.best_epoch}, Score: {self.best_score:.5f})')\n",
    "                \n",
    "                # No Improvement\n",
    "                else:\n",
    "                    # Counter\n",
    "                    self.counter += 1\n",
    "                    if self.verbose:\n",
    "                        print(f'â³ EarlyStopping: {self.counter}/{self.patience} no improvement (Current Score: {score:.5f})')\n",
    "                    \n",
    "                    # Early Stopping\n",
    "                    if self.counter >= self.patience:\n",
    "                        self.early_stop = True\n",
    "                        if self.verbose:\n",
    "                            print(f'ðŸ›‘ Stopping training by early stopping (no improvement after: {self.patience} epochs.)')\n",
    "                            print(f\"âœ… Best Model saved in: '{self.save_path}' (Epoch: {self.best_epoch}, Score: {self.best_score:.5f}).\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to execute Early stopping: {str(e)}')\n",
    "\n",
    "        # Reset Atributes\n",
    "        def reset(\n",
    "            self\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Resets the internal state of the EarlyStopping instance.\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                self.best_score = None\n",
    "                self.counter = 0\n",
    "                self.early_stop = False\n",
    "                self.best_epoch = 0\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to reset Early Stooping class attributes: {str(e)}.')\n",
    "    \n",
    "    ### PyTorch Flow Class ###\n",
    "    class PyTorchFlow():\n",
    "\n",
    "        \"\"\"\n",
    "        Wrapper class for managing the training pipeline of a PyTorch model,\n",
    "        including data loading, architecture parameters, training configuration,\n",
    "        early stopping, and evaluation with k-fold cross-validation.\n",
    "\n",
    "        Attributes:\n",
    "            trainset (Dataset): Training dataset.\n",
    "            testset (Dataset): Test dataset.\n",
    "            l1 (int): Number of units in the first hidden layer.\n",
    "            l2 (int): Number of units in the second hidden layer.\n",
    "            l3 (int): Number of units in the third hidden layer.\n",
    "            dropout_rate (float): Dropout rate for regularization.\n",
    "            num_workers (int): Number of subprocesses for data loading.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "            lr (float): Learning rate for the optimizer.\n",
    "            weight_decay (float): Weight decay (L2 penalty) for regularization.\n",
    "            max_epochs (int): Maximum number of training epochs.\n",
    "            early_stopping_p (int): Patience for early stopping.\n",
    "            early_stopping_mode (str): Mode for early stopping, either 'min' or 'max'.\n",
    "            save_path_model (str): File path to save the best model.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "            k_fold (int): Number of folds for cross-validation.\n",
    "            target_score (str): Metric used to determine model performance (e.g., 'roc').\n",
    "        \"\"\"\n",
    "        # Initialize Atributes\n",
    "        def __init__(\n",
    "            self,\n",
    "            trainset = None, \n",
    "            testset = None,\n",
    "            l1: int = 256,\n",
    "            l2: int = 128,\n",
    "            l3: int = 64,\n",
    "            dropout_rate: float = 0.5,\n",
    "            num_workers: int = 2,\n",
    "            batch_size: int = 128,\n",
    "            lr: float = 1e-3,\n",
    "            weight_decay: float = 1e-5,\n",
    "            max_epochs: int = 200,\n",
    "            early_stopping_p: int = 15,\n",
    "            early_stopping_mode: str = 'max',\n",
    "            save_path_model: str = '/best_model.pt',\n",
    "            seed: int = 33,\n",
    "            k_fold: int = 5,\n",
    "            target_score: str = 'roc',\n",
    "            \n",
    "        ):\n",
    "            try:\n",
    "\n",
    "                self.trainset = trainset\n",
    "                self.testset = testset\n",
    "                self.l1 = l1\n",
    "                self.l2 = l2\n",
    "                self.l3 = l3\n",
    "                self.dropout_rate = dropout_rate\n",
    "                self.num_workers = num_workers\n",
    "                self.batch_size = batch_size\n",
    "                self.lr = lr\n",
    "                self.weight_decay = weight_decay\n",
    "                self.max_epochs = max_epochs\n",
    "                self.early_stopping_p = early_stopping_p\n",
    "                self.early_stopping_mode = early_stopping_mode\n",
    "                self.save_path_model = save_path_model\n",
    "                self.seed = seed\n",
    "                self.k_fold = k_fold\n",
    "                self.target_score = target_score\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to load PytorchFlow class attributes {str(e)} .')\n",
    "        \n",
    "        ### Device Function ###\n",
    "        def _device(\n",
    "            self,\n",
    "            net,\n",
    "            device: str = None,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Moves the given PyTorch model to the appropriate device (CPU or GPU).\n",
    "            If multiple GPUs are available, wraps the model using `nn.DataParallel`.\n",
    "\n",
    "            Args:\n",
    "                net (nn.Module): The PyTorch model to be moved.\n",
    "                device (str, optional): Device identifier (e.g., 'cpu', 'cuda:0'). \n",
    "                    If None, it will be automatically selected based on availability.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[nn.Module, str]: A tuple containing the model moved to the device \n",
    "                    and the device identifier string.\n",
    "\n",
    "            Raises:\n",
    "                Exception: If the model fails to move to the specified device.\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                # Automatically detect device if not provided\n",
    "                if device is None:\n",
    "                    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "                \n",
    "                # If more than one GPU is available, apply DataParallel\n",
    "                if torch.cuda.device_count() > 1 and device.startswith('cuda'):\n",
    "                    net = nn.DataParallel(net)\n",
    "                \n",
    "                # Move the model to the device\n",
    "                net.to(device)\n",
    "\n",
    "                return net, device\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to move network to device: {str(e)}.')\n",
    "\n",
    "        ### Create Kfolds Function ###\n",
    "        def _kfolds(\n",
    "            self\n",
    "        ):  \n",
    "            \"\"\"\n",
    "            Splits the training dataset into k folds for cross-validation.\n",
    "\n",
    "            This method creates `k_fold` random subsets (folds) of the training dataset, \n",
    "            ensuring reproducibility using a fixed random seed. The final fold may be slightly \n",
    "            larger if the dataset size is not perfectly divisible by `k_fold`.\n",
    "\n",
    "            Returns:\n",
    "                List[Subset]: A list of PyTorch `Subset` objects representing each fold.\n",
    "\n",
    "            Raises:\n",
    "                Exception: If the dataset fails to split into folds.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Spliting folds for cross validation\n",
    "                fold_size = len(self.trainset) // self.k_fold\n",
    "                fold_sizes = [fold_size] * (self.k_fold - 1) + [len(self.trainset) - fold_size * (self.k_fold - 1)]\n",
    "                folds = torch.utils.data.random_split(self.trainset, fold_sizes, generator = torch.Generator().manual_seed(self.seed))\n",
    "                \n",
    "                return folds\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to create k folds of trainset: {str(e)}.')\n",
    "        \n",
    "        ### OverSampling Function ###\n",
    "        def _oversampling(\n",
    "            self, \n",
    "            trainset,\n",
    "        ):  \n",
    "            \"\"\"\n",
    "            Applies oversampling to balance class distribution in the training dataset.\n",
    "\n",
    "            This method calculates class weights based on the frequency of each class and creates \n",
    "            a `WeightedRandomSampler` to allow oversampling of the minority class during training. \n",
    "            It also returns the proportion of the positive class in the dataset.\n",
    "\n",
    "            Args:\n",
    "                trainset (Dataset): The training dataset, where each item returns a tuple \n",
    "                    (features, ..., label). The label must be the last element and compatible \n",
    "                    with `torch.int64`.\n",
    "\n",
    "            Returns:\n",
    "                Tuple[WeightedRandomSampler, float]: \n",
    "                    - A PyTorch `WeightedRandomSampler` to be used in a DataLoader.\n",
    "                    - The proportion of the positive class (class 1) in the dataset.\n",
    "\n",
    "            Raises:\n",
    "                Exception: If there is an error while computing class weights or creating the sampler.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                \n",
    "                # Compute class weights for imbalance handling\n",
    "                eps = 1e-6\n",
    "                all_class = torch.cat([labels for _, _, labels in trainset]).to(torch.int64)\n",
    "                class_counts = torch.bincount(all_class)\n",
    "                total_samples = len(all_class)\n",
    "                num_classes = len(class_counts)\n",
    "                class_weights = total_samples / (class_counts.float() + eps)\n",
    "\n",
    "                # Create Weighted Sampler\n",
    "                sample_weights = class_weights[all_class]\n",
    "                sampler = torch.utils.data.WeightedRandomSampler(\n",
    "                    weights = sample_weights, \n",
    "                    num_samples = len(sample_weights), \n",
    "                    replacement = True\n",
    "                )\n",
    "                # Calculate minority class proportion\n",
    "                prop_class_positive = class_counts[1].item() / total_samples\n",
    "\n",
    "                return sampler, prop_class_positive\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to create sampler and calculate class distributions: {str(e)}.')\n",
    "        \n",
    "        ### Plot Metrics Function ###\n",
    "        def _plot_metrics(\n",
    "            self,\n",
    "            avg_loss_t: list = None,\n",
    "            avg_loss_v: list = None,\n",
    "            avg_accuracy_t: list = None,\n",
    "            avg_accuracy_v: list = None,\n",
    "            avg_precision_t: list = None, \n",
    "            avg_precision_v: list = None,\n",
    "            avg_npv_t: list = None, \n",
    "            avg_npv_v: list = None,\n",
    "            avg_recall_t: list = None, \n",
    "            avg_recall_v: list = None,\n",
    "            avg_auc_t: list = None, \n",
    "            avg_auc_v: list = None\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Plots training and validation metrics over epochs for model convergence visualization.\n",
    "\n",
    "            Args:\n",
    "                avg_loss_t (list, optional): List of average training loss values per epoch.\n",
    "                avg_loss_v (list, optional): List of average validation loss values per epoch.\n",
    "                avg_accuracy_t (list, optional): List of average training accuracy values per epoch.\n",
    "                avg_accuracy_v (list, optional): List of average validation accuracy values per epoch.\n",
    "                avg_precision_t (list, optional): List of average training precision values per epoch.\n",
    "                avg_precision_v (list, optional): List of average validation precision values per epoch.\n",
    "                avg_npv_t (list, optional): List of average training negative predictive value per epoch.\n",
    "                avg_npv_v (list, optional): List of average validation negative predictive value per epoch.\n",
    "                avg_recall_t (list, optional): List of average training recall values per epoch.\n",
    "                avg_recall_v (list, optional): List of average validation recall values per epoch.\n",
    "                avg_auc_t (list, optional): List of average training AUC-ROC values per epoch.\n",
    "                avg_auc_v (list, optional): List of average validation AUC-ROC values per epoch.\n",
    "\n",
    "            Raises:\n",
    "                Exception: If an error occurs during plotting.\n",
    "\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # List of metrics\n",
    "                metrics, train_metrics, val_metrics = [], [], []\n",
    "\n",
    "                # Dynamically grouping metrics\n",
    "                if avg_loss_t and avg_loss_v:\n",
    "                    metrics.append('Loss')\n",
    "                    train_metrics.append(avg_loss_t)\n",
    "                    val_metrics.append(avg_loss_v)\n",
    "\n",
    "                if avg_accuracy_t and avg_accuracy_v:\n",
    "                    metrics.append('Accuracy')\n",
    "                    train_metrics.append(avg_accuracy_t)\n",
    "                    val_metrics.append(avg_accuracy_v)\n",
    "\n",
    "                if avg_precision_t and avg_precision_v :\n",
    "                    metrics.append('Precision')\n",
    "                    train_metrics.append(avg_precision_t)\n",
    "                    val_metrics.append(avg_precision_v)\n",
    "\n",
    "                if avg_npv_t and avg_npv_v:\n",
    "                    metrics.append('NPV')\n",
    "                    train_metrics.append(avg_npv_t)\n",
    "                    val_metrics.append(avg_npv_v)\n",
    "\n",
    "                if avg_recall_t and avg_recall_v:\n",
    "                    metrics.append('Recall')\n",
    "                    train_metrics.append(avg_recall_t)\n",
    "                    val_metrics.append(avg_recall_v)\n",
    "\n",
    "                if avg_auc_t and avg_auc_v:\n",
    "                    metrics.append('AUC-ROC')\n",
    "                    train_metrics.append(avg_auc_t)\n",
    "                    val_metrics.append(avg_auc_v)\n",
    "\n",
    "                # Total number of metrics\n",
    "                num_metrics = len(train_metrics)\n",
    "\n",
    "                num_cols = 3\n",
    "                num_rows = (num_metrics + num_cols - 1) // num_cols\n",
    "\n",
    "                plt.rc('font', size = 10)\n",
    "                fig, axes = plt.subplots(num_rows, num_cols, figsize = (6 * num_cols, 4 * num_rows))\n",
    "                axes = axes.flatten() if num_metrics > 1 else [axes]\n",
    "\n",
    "                for i in range(num_metrics):\n",
    "                    ax = axes[i]\n",
    "                    ax.plot(train_metrics[i], label = 'Training')\n",
    "                    ax.plot(val_metrics[i], label = 'Validation')\n",
    "                    ax.set_title(f'Model Convergence - {metrics[i]}')\n",
    "                    ax.set_xlabel('Epochs')\n",
    "                    ax.set_ylabel(metrics[i])\n",
    "                    ax.legend()\n",
    "                    ax.grid(True, alpha = 0.6, linestyle = 'dotted')\n",
    "\n",
    "                # Remove extra subplots, if any\n",
    "                for j in range(num_metrics, len(axes)):\n",
    "                    fig.delaxes(axes[j])\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to generate graphs of metrics during epochs: {str(e)}.')\n",
    "\n",
    "        ### Confusion Matrix Function ###\n",
    "        def _confusion_matrix(\n",
    "            self,\n",
    "            preds,\n",
    "            labels,\n",
    "            title: str = 'Confusion Matrix',\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Plots the confusion matrix for binary classification predictions.\n",
    "\n",
    "            Args:\n",
    "                preds (Tensor or array-like): Predicted labels or probabilities from the model.\n",
    "                labels (Tensor or array-like): True labels corresponding to the predictions.\n",
    "                title (str, optional): Title for the confusion matrix plot. Default is 'Confusion Matrix'.\n",
    "\n",
    "            Raises:\n",
    "                Exception: If an error occurs during the plotting process.\n",
    "\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                # Confusion Matrix\n",
    "                plt.rc('font', size = 10)\n",
    "                fig, ax= plt.subplots(figsize = (8, 4))\n",
    "                ax.grid(False)\n",
    "                \n",
    "                metric = BinaryConfusionMatrix()\n",
    "                metric(preds, labels)\n",
    "                fig_, ax_ = metric.plot(cmap = 'viridis', ax = ax)\n",
    "                ax_.set_title(title)\n",
    "                plt.show()\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] failed to plot confusion matrix: {str(e)}.')\n",
    "\n",
    "        ### Flow Cross Validation Function ###\n",
    "        def CrossValidation(\n",
    "            self,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Perform k-fold cross-validation training and evaluation of the PyTorch model.\n",
    "\n",
    "            This method performs the following steps for each fold:\n",
    "            - Splits the training data into training and validation sets.\n",
    "            - Applies oversampling to handle class imbalance.\n",
    "            - Creates data loaders for training and validation.\n",
    "            - Initializes the neural network, loss function, optimizer, and learning rate scheduler.\n",
    "            - Trains the model for a maximum number of epochs or until early stopping triggers.\n",
    "            - Evaluates the model on validation data after training.\n",
    "            - Collects and prints various metrics (Loss, Accuracy, Precision, NPV, Recall, AUC-ROC).\n",
    "            - Plots training and validation metric curves and the confusion matrix.\n",
    "            - Aggregates metrics across folds and prints mean and standard deviation.\n",
    "\n",
    "            Attributes used:\n",
    "                self.trainset: Dataset used for training and cross-validation splits.\n",
    "                self.k_fold (int): Number of folds for cross-validation.\n",
    "                self.batch_size (int): Batch size for data loaders.\n",
    "                self.num_workers (int): Number of workers for data loading.\n",
    "                self.l1, self.l2, self.l3 (int): Neural network layer sizes.\n",
    "                self.dropout_rate (float): Dropout rate in the network.\n",
    "                self.lr (float): Learning rate for optimizer.\n",
    "                self.weight_decay (float): Weight decay for optimizer.\n",
    "                self.max_epochs (int): Maximum number of epochs for training.\n",
    "                self.early_stopping_p (int): Patience parameter for early stopping.\n",
    "                self.early_stopping_mode (str): Mode ('max' or 'min') for early stopping.\n",
    "                self.save_path_model (str): File path to save the best model.\n",
    "                self.target_score (str): Metric used for early stopping ('roc', 'accuracy', etc.).\n",
    "\n",
    "            Raises:\n",
    "                Exception: Prints error message if any exception occurs during the cross-validation process.\n",
    "\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                folds = self._kfolds()\n",
    "                # Defining list to store fold metrics\n",
    "                accuracy_kfolds, recall_kfolds, auc_kfolds, loss_kfolds, precision_kfolds, npv_kfolds = [], [], [], [], [], []\n",
    "\n",
    "                # Initialize Metrics for binary classification\n",
    "\n",
    "                # Training\n",
    "                accuracy_train = BinaryAccuracy()\n",
    "                recall_train = BinaryRecall()\n",
    "                auc_train = BinaryAUROC(thresholds = None)\n",
    "                precision_train = BinaryPrecision()\n",
    "                npv_train = BinaryNegativePredictiveValue()\n",
    "\n",
    "                # Validation\n",
    "                accuracy_val = BinaryAccuracy()\n",
    "                recall_val = BinaryRecall()\n",
    "                auc_val = BinaryAUROC(thresholds = None)\n",
    "                precision_val = BinaryPrecision()\n",
    "                npv_val = BinaryNegativePredictiveValue()\n",
    "\n",
    "                # Score target\n",
    "                if self.target_score == 'acuracy':\n",
    "                    target_score = BinaryAccuracy()\n",
    "                \n",
    "                elif self.target_score == 'recall':\n",
    "                    target_score = BinaryRecall()\n",
    "                \n",
    "                elif self.target_score == 'roc':\n",
    "                    target_score = BinaryAUROC(thresholds = None)\n",
    "                \n",
    "                elif self.target_score == 'precision':\n",
    "                    target_score = BinaryPrecision()\n",
    "\n",
    "                elif self.target_score == 'npv':\n",
    "                    target_score = BinaryNegativePredictiveValue()\n",
    "\n",
    "                for i in tqdm(range(self.k_fold), desc = '\\nCross - Validation Progress', leave = False):\n",
    "\n",
    "                    # Separating training and validation data\n",
    "                    # Fold for validation\n",
    "                    val_set = folds[i] \n",
    "\n",
    "                    # All training folds except validation fold\n",
    "                    train_sets = [folds[j] for j in range(self.k_fold) if j != i]\n",
    "                    train_set = torch.utils.data.ConcatDataset(train_sets)\n",
    "\n",
    "                    sampler, prop_class_positive =  self._oversampling(trainset = train_set)\n",
    "                    \n",
    "                    # Train Loader\n",
    "                    trainloader = torch.utils.data.DataLoader(\n",
    "                        train_set, \n",
    "                        batch_size = self.batch_size, \n",
    "                        sampler = sampler, \n",
    "                        num_workers = self.num_workers,\n",
    "                        drop_last = True,\n",
    "                    )\n",
    "\n",
    "                    # Val Loader\n",
    "                    valloader = torch.utils.data.DataLoader(\n",
    "                        val_set, \n",
    "                        batch_size = self.batch_size, \n",
    "                        shuffle = False, \n",
    "                        num_workers = self.num_workers, \n",
    "                        drop_last = False,\n",
    "                    )\n",
    "                    \n",
    "\n",
    "            \n",
    "                    # Loading Net\n",
    "                    net = PyTorch.Net(\n",
    "                        l1 = self.l1,\n",
    "                        l2 = self.l2, \n",
    "                        l3 = self.l3, \n",
    "                        dropout_rate = self.dropout_rate,\n",
    "                        prior_minoritary_class = prop_class_positive,  \n",
    "                    )\n",
    "                    # Moving the network to the device\n",
    "                    net, device = self._device(net)\n",
    "\n",
    "                    # Criterion\n",
    "                    criterion = PyTorch().FocalLoss().to(device)\n",
    "\n",
    "                    # Optimizer\n",
    "                    optimizer = optim.AdamW(net.parameters(), lr = self.lr, weight_decay = self.weight_decay) \n",
    "\n",
    "                    # Scheduler\n",
    "                    # Warmup (linear from 1e-5 to 0.001)\n",
    "                    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                        optimizer,\n",
    "                        start_factor = 0.01,\n",
    "                        total_iters = 10,\n",
    "                    )\n",
    "                    # Cosine Annealing after warmup\n",
    "                    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                        optimizer,\n",
    "                        T_max = self.max_epochs - 10,\n",
    "                        eta_min = 1e-6,\n",
    "                    )\n",
    "                    # Composition: 10 warmup epochs + (max_epochs cosine - 10 warmup)\n",
    "                    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "                        optimizer, \n",
    "                        schedulers = [warmup_scheduler, cosine_scheduler],\n",
    "                        milestones = [10],\n",
    "                    )\n",
    "\n",
    "                    # Adjusting error caused by scheduler\n",
    "                    warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "                    # Info Cross-Validation\n",
    "                    print(f'\\n\\nTraining fold sample set:')\n",
    "                    print(f'################# [ K-Fold {i+1} ] #################')\n",
    "                    \n",
    "                    # Early Stopping\n",
    "                    early_stopping = PyTorch().EarlyStopping(\n",
    "                        patience = self.early_stopping_p, \n",
    "                        mode = self.early_stopping_mode, \n",
    "                        save_path = self.save_path_model\n",
    "                    )\n",
    "                    \n",
    "                    # Metrics for epochs\n",
    "                    avg_loss_t, avg_accuracy_t, avg_recall_t, avg_auc_t, avg_precision_t, avg_npv_t = [], [], [], [], [], []\n",
    "                    avg_loss_v, avg_accuracy_v, avg_recall_v, avg_auc_v, avg_precision_v, avg_npv_v = [], [], [], [], [], []\n",
    "                    # Epochs\n",
    "                    for epoch in range(0, self.max_epochs):\n",
    "\n",
    "                    # Checking the learning rate according to the epochs:\n",
    "                    #print(f'\\nEpoch {epoch + 1} -- Kfold: {i + 1}\\n--------------------------------------------------------------')\n",
    "                    #print(f\"Epoch {epoch + 1} -- Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "                        # Metrics Training\n",
    "                        train_loss = 0.0 \n",
    "                        train_steps = 0 \n",
    "                        accuracy_train.reset()\n",
    "                        recall_train.reset()\n",
    "                        auc_train.reset()\n",
    "                        precision_train.reset()\n",
    "                        npv_train.reset()\n",
    "\n",
    "                        # Metrics Validation\n",
    "                        val_loss = 0.0\n",
    "                        val_steps = 0\n",
    "                        accuracy_val.reset()\n",
    "                        recall_val.reset()\n",
    "                        auc_val.reset()\n",
    "                        precision_val.reset()\n",
    "                        npv_val.reset()\n",
    "\n",
    "                        # Target Score\n",
    "                        target_score.reset()\n",
    "\n",
    "                        # Save preds and labels\n",
    "                        preds_val, labels_val = [], []\n",
    "                        \n",
    "                        # Training\n",
    "                        net.train()\n",
    "\n",
    "                        for cat_input, num_input, labels in (trainloader):\n",
    "                \n",
    "                            # Inputs + Labels to(device)    \n",
    "                            cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                            \n",
    "                            # Zero the parameter gradients\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            # Foward Pass\n",
    "                            outputs = net(cat_input, num_input)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                            # Backward + optimize\n",
    "                            loss.backward()\n",
    "                            # Optimizer\n",
    "                            optimizer.step()\n",
    "\n",
    "                            # Accumulating Loss\n",
    "                            train_loss += loss.item()\n",
    "                            train_steps += 1\n",
    "\n",
    "                            # Updating metrics\n",
    "                            accuracy_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                            recall_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                            auc_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                            precision_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                            npv_train.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                        # Evaluation\n",
    "                        net.eval()\n",
    "\n",
    "                        # Disabling gradient calculations\n",
    "                        with torch.no_grad():\n",
    "                            # Get the inputs; data is a list of [inputs, labels]\n",
    "                            for cat_input, num_input, labels in (valloader):\n",
    "                                    \n",
    "                                    # Inputs + Labels to(device)\n",
    "                                    cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                    \n",
    "                                    # Eval net\n",
    "                                    outputs = net(cat_input, num_input)\n",
    "                                    loss = criterion(outputs, labels)\n",
    "                                    \n",
    "                                    # Accumulating Loss\n",
    "                                    val_loss += loss.item()\n",
    "                                    val_steps += 1\n",
    "\n",
    "                                    # Updating metrics\n",
    "                                    accuracy_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    recall_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    auc_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    precision_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    npv_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    \n",
    "                                    # Target Score\n",
    "                                    target_score.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                        # Saving metrics by epoch\n",
    "                        # Train\n",
    "                        avg_accuracy_t.append(accuracy_train.compute().item())\n",
    "                        avg_recall_t.append(recall_train.compute().item())\n",
    "                        avg_auc_t.append(auc_train.compute().item())\n",
    "                        avg_precision_t.append(precision_train.compute().item())\n",
    "                        avg_npv_t.append(npv_train.compute().item())\n",
    "                        avg_loss_t.append(train_loss / train_steps)\n",
    "                        # Validation\n",
    "                        avg_accuracy_v.append(accuracy_val.compute().item())\n",
    "                        avg_recall_v.append(recall_val.compute().item())\n",
    "                        avg_auc_v.append(auc_val.compute().item())\n",
    "                        avg_precision_v.append(precision_val.compute().item())\n",
    "                        avg_npv_v.append(npv_val.compute().item())\n",
    "                        avg_loss_v.append(val_loss / val_steps)\n",
    "                                    \n",
    "                        # Scheduler step\n",
    "                        scheduler.step()\n",
    "\n",
    "                        # Early_stopping step\n",
    "                        early_stopping(score = target_score.compute().item(), model = net, epoch = epoch)\n",
    "                        # Stopping \n",
    "                        if early_stopping.early_stop:\n",
    "                            print(f'>>>>>>> Finished Training K-Fold {i+1}.')\n",
    "                        \n",
    "                            # Final Validation Score Model\n",
    "                            # Load the Best Model\n",
    "                            net.load_state_dict(torch.load(self.save_path_model))\n",
    "                            \n",
    "\n",
    "                            # Metrics Validation\n",
    "                            val_loss = 0.0\n",
    "                            val_steps = 0\n",
    "                            accuracy_val.reset()\n",
    "                            recall_val.reset()\n",
    "                            auc_val.reset()\n",
    "                            precision_val.reset()\n",
    "                            npv_val.reset()\n",
    "                            \n",
    "                            # Evaluation\n",
    "                            net.eval()\n",
    "                            # Disabling gradient calculations\n",
    "                            with torch.no_grad():\n",
    "                                # Get the inputs; data is a list of [inputs, labels]\n",
    "                                for cat_input, num_input, labels in (valloader):\n",
    "                                        \n",
    "                                        # Inputs + Labels to(device)\n",
    "                                        cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                        \n",
    "                                        # Eval net\n",
    "                                        outputs = net(cat_input, num_input)\n",
    "                                        loss = criterion(outputs, labels)\n",
    "                                        \n",
    "                                        # Accumulating Loss\n",
    "                                        val_loss += loss.item()\n",
    "                                        val_steps += 1\n",
    "\n",
    "                                        # Updating metrics\n",
    "                                        accuracy_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                        recall_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                        auc_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                        precision_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                        npv_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                        \n",
    "                                        # Accumulating Predictions\n",
    "                                        preds_val.append(torch.sigmoid(outputs).detach().cpu())\n",
    "                                        labels_val.append(labels.detach().cpu())\n",
    "                        \n",
    "                            # Concatenates all batches\n",
    "                            preds_val = torch.cat(preds_val).float()\n",
    "                            labels_val = torch.cat(labels_val).long()\n",
    "\n",
    "                            break\n",
    "                    \n",
    "                    # Metrics out\n",
    "                    print('\\nTrain Metrics:') \n",
    "                    print(f'Loss: {train_loss / train_steps:.3f}')   \n",
    "                    print(f'Accuracy: {accuracy_train.compute().item() * 100:> 0.1f}%') \n",
    "                    print(f'Precision: {precision_train.compute().item() * 100:> 0.1f}%')\n",
    "                    print(f'NPV: {npv_train.compute().item() * 100:> 0.1f}%')\n",
    "                    print(f'Recall: {recall_train.compute().item() *100:> 0.1f}%') \n",
    "                    print(f'AUC-ROC: {auc_train.compute().item() *100:> 0.1f}%') \n",
    "\n",
    "                    print('\\nValidation Metrics:')\n",
    "                    print(f'Loss: {val_loss / val_steps:.3f}')\n",
    "                    print(f'Accuracy: {accuracy_val.compute().item() * 100:> 0.1f}%')\n",
    "                    print(f'Precision: {precision_val.compute().item() * 100:> 0.1f}%')\n",
    "                    print(f'NPV: {npv_val.compute().item() * 100:> 0.1f}%')\n",
    "                    print(f'Recall: {recall_val.compute().item() * 100:> 0.1f}%')\n",
    "                    print(f'AUC-ROC: {auc_val.compute().item() * 100:> 0.1f}%')\n",
    "                    \n",
    "                    # Graphics for Training and Validation\n",
    "            \n",
    "                    self._plot_metrics(\n",
    "                        avg_loss_t = avg_loss_t, \n",
    "                        avg_loss_v = avg_loss_v,\n",
    "                        avg_accuracy_t = avg_accuracy_t, \n",
    "                        avg_accuracy_v = avg_accuracy_v,\n",
    "                        avg_precision_t = avg_precision_t, \n",
    "                        avg_precision_v = avg_precision_v,\n",
    "                        avg_npv_t = avg_npv_t, \n",
    "                        avg_npv_v = avg_npv_v,\n",
    "                        avg_recall_t = avg_recall_t, \n",
    "                        avg_recall_v = avg_recall_v,\n",
    "                        avg_auc_t = avg_auc_t, \n",
    "                        avg_auc_v = avg_auc_v\n",
    "                    )\n",
    "\n",
    "                    self._confusion_matrix(\n",
    "                        preds = preds_val,\n",
    "                        labels = labels_val,\n",
    "                        title = f'Confusion Matrix is K-Fold: {i + 1}'\n",
    "                    )\n",
    "\n",
    "                    # Time sleep\n",
    "                    time.sleep(5)\n",
    "\n",
    "                    # Calculating metrics per fold\n",
    "                    accuracy_kfolds.append(accuracy_val.compute().item())\n",
    "                    recall_kfolds.append(recall_val.compute().item())\n",
    "                    auc_kfolds.append(auc_val.compute().item())\n",
    "                    precision_kfolds.append(precision_val.compute().item())\n",
    "                    npv_kfolds.append(npv_val.compute().item())\n",
    "                    loss_kfolds.append(val_loss / val_steps)\n",
    "\n",
    "                print('\\n\\nâœ…### Cross validation Metrics ### :')\n",
    "\n",
    "                print(f'\\nðŸ”´ Loss: {np.mean(loss_kfolds):.3f}') \n",
    "                print(f'â˜‘ï¸ Standard Deviation - Loss: {np.std(loss_kfolds):.6f}')\n",
    "\n",
    "                print(f'\\nðŸŸ  Accuracy: {(np.mean(accuracy_kfolds) * 100):.2f}%')\n",
    "                print(f'â˜‘ï¸ Standard Deviation - Accuracy: {np.std(accuracy_kfolds):.6f}')\n",
    "\n",
    "                print(f'\\nðŸ”µ Precision: {(np.mean(precision_kfolds) * 100):.2f}%')\n",
    "                print(f'â˜‘ï¸ Standard Deviation - Precision: {np.std(precision_kfolds):.6f}')\n",
    "\n",
    "                print(f'\\nðŸ”µ NPV: {(np.mean(npv_kfolds) * 100):.2f}%')\n",
    "                print(f'â˜‘ï¸ Standard Deviation - NPV: {np.std(npv_kfolds):.6f}')\n",
    "\n",
    "                print(f'\\nâš ï¸ Recall: {(np.mean(recall_kfolds) * 100):.2f}%')\n",
    "                print(f'â˜‘ï¸ Standard Deviation - Recall: {np.std(recall_kfolds):.6f}')\n",
    "\n",
    "                print(f'\\nðŸŽ¯ AUC-ROC: {(np.mean(auc_kfolds) * 100):.2f}%')\n",
    "                print(f'â˜‘ï¸ Standard Deviation - AUC-ROC: {np.std(auc_kfolds):.6f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Cross validation flow execution failed: {str(e)}.')\n",
    "\n",
    "        ### Flow Hyper Tunning ###\n",
    "        ### Cross Tunning Function ###\n",
    "        def _cross_tunning(\n",
    "            self,\n",
    "            config,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Perform hyperparameter tuning with k-fold cross-validation for a PyTorch model.\n",
    "\n",
    "            This method integrates k-fold cross-validation with oversampling, early stopping, \n",
    "            and learning rate scheduling to evaluate model performance across different \n",
    "            hyperparameter configurations. It is designed to work with Ray Tune for distributed \n",
    "            hyperparameter optimization.\n",
    "\n",
    "            The process includes:\n",
    "                - Splitting the dataset into k folds.\n",
    "                - Using each fold once for validation and the remaining folds for training.\n",
    "                - Applying oversampling to balance class distribution in training data.\n",
    "                - Initializing the neural network architecture from the given configuration.\n",
    "                - Setting up the optimizer, loss function, and composite learning rate scheduler.\n",
    "                - Training with early stopping to avoid overfitting.\n",
    "                - Evaluating performance metrics on the validation set.\n",
    "                - Aggregating results and reporting them to Ray Tune along with a model checkpoint.\n",
    "\n",
    "            Args:\n",
    "                config (dict):\n",
    "                    A dictionary containing hyperparameters for the training process. Keys include:\n",
    "                        - 'batch_size' (int): Batch size for training and validation.\n",
    "                        - 'l1', 'l2', 'l3' (int): Number of units in each layer of the network.\n",
    "                        - 'lr' (float): Learning rate for the optimizer.\n",
    "                        - 'weight_decay' (float): Weight decay (L2 regularization) for the optimizer.\n",
    "\n",
    "            Attributes Used:\n",
    "                self.trainset: The training dataset for k-fold splitting.\n",
    "                self.k_fold (int): Number of folds for cross-validation.\n",
    "                self.num_workers (int): Number of workers for data loading.\n",
    "                self.dropout_rate (float): Dropout probability for the network.\n",
    "                self.target_score (str): Metric used for early stopping ('accuracy', 'recall', 'roc', 'precision', or 'npv').\n",
    "                self.max_epochs (int): Maximum number of training epochs.\n",
    "                self.early_stopping_p (int): Patience value for early stopping.\n",
    "                self.early_stopping_mode (str): Mode for early stopping ('max' or 'min').\n",
    "\n",
    "            Metrics Computed per Fold:\n",
    "                - Accuracy\n",
    "                - Recall\n",
    "                - AUC-ROC\n",
    "                - Precision\n",
    "                - Negative Predictive Value (NPV)\n",
    "                - Loss\n",
    "\n",
    "            Ray Tune Reports:\n",
    "                - Mean loss across folds\n",
    "                - Mean accuracy, precision, NPV, recall, and AUC-ROC\n",
    "                - Standard deviation of AUC-ROC\n",
    "                - Checkpoint containing model and optimizer states\n",
    "\n",
    "            Raises:\n",
    "                Exception: Logs an error message if any step in the tuning process fails.\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                folds = self._kfolds()\n",
    "                # Defining list to store fold metrics\n",
    "                accuracy_kfolds, recall_kfolds, auc_kfolds, loss_kfolds, precision_kfolds, npv_kfolds = [], [], [], [], [], []\n",
    "\n",
    "                # Initialize Metrics for binary classification\n",
    "\n",
    "                # Validation\n",
    "                accuracy_val = BinaryAccuracy()\n",
    "                recall_val = BinaryRecall()\n",
    "                auc_val = BinaryAUROC(thresholds = None)\n",
    "                precision_val = BinaryPrecision()\n",
    "                npv_val = BinaryNegativePredictiveValue()\n",
    "\n",
    "                # Score target\n",
    "                if self.target_score == 'acuracy':\n",
    "                    target_score = BinaryAccuracy()\n",
    "                \n",
    "                elif self.target_score == 'recall':\n",
    "                    target_score = BinaryRecall()\n",
    "                \n",
    "                elif self.target_score == 'roc':\n",
    "                    target_score = BinaryAUROC(thresholds = None)\n",
    "                \n",
    "                elif self.target_score == 'precision':\n",
    "                    target_score = BinaryPrecision()\n",
    "\n",
    "                elif self.target_score == 'npv':\n",
    "                    target_score = BinaryNegativePredictiveValue()\n",
    "\n",
    "                for i in range(self.k_fold):\n",
    "\n",
    "                    # Separating training and validation data\n",
    "                    # Fold for validation\n",
    "                    val_set = folds[i] \n",
    "\n",
    "                    # All training folds except validation fold\n",
    "                    train_sets = [folds[j] for j in range(self.k_fold) if j != i]\n",
    "                    train_set = torch.utils.data.ConcatDataset(train_sets)\n",
    "\n",
    "                    sampler, prop_class_positive =  self._oversampling(trainset = train_set)\n",
    "                    \n",
    "                    # Train Loader\n",
    "                    trainloader = torch.utils.data.DataLoader(\n",
    "                        train_set, \n",
    "                        batch_size = int(config['batch_size']), \n",
    "                        sampler = sampler, \n",
    "                        num_workers = self.num_workers,\n",
    "                        drop_last = True,\n",
    "                    )\n",
    "\n",
    "                    # Val Loader\n",
    "                    valloader = torch.utils.data.DataLoader(\n",
    "                        val_set, \n",
    "                        batch_size = int(config['batch_size']), \n",
    "                        shuffle = False, \n",
    "                        num_workers = self.num_workers, \n",
    "                        drop_last = False,\n",
    "                    )\n",
    "                    \n",
    "\n",
    "            \n",
    "                    # Loading Net\n",
    "                    net = PyTorch.Net(\n",
    "                        l1 = config['l1'],\n",
    "                        l2 = config['l2'], \n",
    "                        l3 = config['l3'], \n",
    "                        dropout_rate = self.dropout_rate,\n",
    "                        prior_minoritary_class = prop_class_positive,  \n",
    "                    )\n",
    "                    # Moving the network to the device\n",
    "                    net, device = self._device(net)\n",
    "\n",
    "                    # Criterion\n",
    "                    criterion = PyTorch().FocalLoss().to(device)\n",
    "\n",
    "                    # Optimizer\n",
    "                    optimizer = optim.AdamW(net.parameters(), lr = config['lr'], weight_decay = config['weight_decay']) \n",
    "\n",
    "                    # Scheduler\n",
    "                    # Warmup (linear from 1e-5 to 0.001)\n",
    "                    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                        optimizer,\n",
    "                        start_factor = 0.01,\n",
    "                        total_iters = 10,\n",
    "                    )\n",
    "                    # Cosine Annealing after warmup\n",
    "                    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                        optimizer,\n",
    "                        T_max = self.max_epochs - 10,\n",
    "                        eta_min = 1e-6,\n",
    "                    )\n",
    "                    # Composition: 10 warmup epochs + (max_epochs cosine - 10 warmup)\n",
    "                    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "                        optimizer, \n",
    "                        schedulers = [warmup_scheduler, cosine_scheduler],\n",
    "                        milestones = [10],\n",
    "                    )\n",
    "\n",
    "                    # Adjusting error caused by scheduler\n",
    "                    warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "                    # Saving the model temporarily with early stopping\n",
    "                    with tempfile.NamedTemporaryFile(delete = False) as temp_model_file:  \n",
    "                        \n",
    "                        # Early Stopping\n",
    "                        early_stopping = PyTorch().EarlyStopping(\n",
    "                            patience = self.early_stopping_p, \n",
    "                            mode = self.early_stopping_mode, \n",
    "                            save_path = temp_model_file,\n",
    "                            tempfile_save = True,\n",
    "                            verbose = False\n",
    "                        )\n",
    "                        \n",
    "                        # Epochs\n",
    "                        for epoch in range(0, self.max_epochs):\n",
    "\n",
    "                            # Target Score\n",
    "                            target_score.reset()\n",
    "                            \n",
    "                            # Training\n",
    "                            net.train()\n",
    "\n",
    "                            for cat_input, num_input, labels in (trainloader):\n",
    "                    \n",
    "                                # Inputs + Labels to(device)    \n",
    "                                cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                \n",
    "                                # Zero the parameter gradients\n",
    "                                optimizer.zero_grad()\n",
    "\n",
    "                                # Foward Pass\n",
    "                                outputs = net(cat_input, num_input)\n",
    "                                loss = criterion(outputs, labels)\n",
    "\n",
    "                                # Backward + optimize\n",
    "                                loss.backward()\n",
    "                                # Optimizer\n",
    "                                optimizer.step()\n",
    "\n",
    "                            # Evaluation\n",
    "                            net.eval()\n",
    "\n",
    "                            # Disabling gradient calculations\n",
    "                            with torch.no_grad():\n",
    "                                # Get the inputs; data is a list of [inputs, labels]\n",
    "                                for cat_input, num_input, labels in (valloader):\n",
    "                                        \n",
    "                                        # Inputs + Labels to(device)\n",
    "                                        cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                        \n",
    "                                        # Eval net\n",
    "                                        outputs = net(cat_input, num_input)\n",
    "                                        loss = criterion(outputs, labels)\n",
    "                                        \n",
    "                                        # Target Score\n",
    "                                        target_score.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                            # Scheduler step\n",
    "                            scheduler.step()\n",
    "\n",
    "                            # Early_stopping step\n",
    "                            early_stopping(score = target_score.compute().item(), model = net, epoch = epoch)\n",
    "                            # Stopping \n",
    "                            if early_stopping.early_stop:\n",
    "                    \n",
    "                                # Final Validation Score Model\n",
    "                                # Load the Best Model\n",
    "                                net.load_state_dict(torch.load(temp_model_file.name))\n",
    "                                os.remove(temp_model_file.name)\n",
    "\n",
    "                                # Metrics Validation\n",
    "                                val_loss = 0.0\n",
    "                                val_steps = 0\n",
    "                                accuracy_val.reset()\n",
    "                                recall_val.reset()\n",
    "                                auc_val.reset()\n",
    "                                precision_val.reset()\n",
    "                                npv_val.reset()\n",
    "                                \n",
    "                                # Evaluation\n",
    "                                net.eval()\n",
    "                                # Disabling gradient calculations\n",
    "                                with torch.no_grad():\n",
    "                                    # Get the inputs; data is a list of [inputs, labels]\n",
    "                                    for cat_input, num_input, labels in (valloader):\n",
    "                                            \n",
    "                                            # Inputs + Labels to(device)\n",
    "                                            cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                            \n",
    "                                            # Eval net\n",
    "                                            outputs = net(cat_input, num_input)\n",
    "                                            loss = criterion(outputs, labels)\n",
    "                                            \n",
    "                                            # Accumulating Loss\n",
    "                                            val_loss += loss.item()\n",
    "                                            val_steps += 1\n",
    "\n",
    "                                            # Updating metrics\n",
    "                                            accuracy_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                            recall_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                            auc_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                            precision_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                            npv_val.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                                break\n",
    "                            \n",
    "                    # Calculating metrics per fold\n",
    "                    accuracy_kfolds.append(accuracy_val.compute().item())\n",
    "                    recall_kfolds.append(recall_val.compute().item())\n",
    "                    auc_kfolds.append(auc_val.compute().item())\n",
    "                    precision_kfolds.append(precision_val.compute().item())\n",
    "                    npv_kfolds.append(npv_val.compute().item())\n",
    "                    loss_kfolds.append(val_loss / val_steps)\n",
    "                \n",
    "                # Checkpoint\n",
    "                checkpoint_data = {\n",
    "                    'net_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }\n",
    "                with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                    data_path = Path(checkpoint_dir) / 'data.pkl'\n",
    "                    with open(data_path, 'wb') as fp:\n",
    "                        pickle.dump(checkpoint_data, fp)\n",
    "                        checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "                        \n",
    "                        # Report for Ray\n",
    "                        tune.report(\n",
    "                            {\n",
    "                                'loss_mean': np.mean(loss_kfolds),\n",
    "                                'accuracy_mean': np.mean(accuracy_kfolds),\n",
    "                                'precision_mean': np.mean(precision_kfolds),\n",
    "                                'npv_mean': np.mean(npv_kfolds),\n",
    "                                'recall_mean': np.mean(recall_kfolds),\n",
    "                                'auc_roc_mean': np.mean(auc_kfolds),\n",
    "                                'auc_roc_std': np.std(auc_kfolds),\n",
    "                            },\n",
    "                            checkpoint = checkpoint\n",
    "                        )\n",
    "\n",
    "                print('âœ…Finished Training')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to execute cross adjustment flow: {str(e)}.')\n",
    "\n",
    "        ### HyperTunning Function ###\n",
    "        def HyperTunning(\n",
    "            self,\n",
    "            n_samples: int = 10,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Run hyperparameter search using Ray Tune and Optuna.\n",
    "\n",
    "            This method configures and executes a hyperparameter tuning experiment for \n",
    "            the neural network model using Ray Tune's `Tuner` class. It searches across \n",
    "            a predefined parameter space, evaluating configurations with k-fold cross-validation \n",
    "            implemented in the `_cross_tunning` method.\n",
    "\n",
    "            The tuning process uses `OptunaSearch` as the search algorithm and optimizes \n",
    "            for the highest mean AUC-ROC score across folds.\n",
    "\n",
    "            Args:\n",
    "                n_samples (int, optional):\n",
    "                    Number of hyperparameter configurations to evaluate. \n",
    "                    Defaults to 10.\n",
    "\n",
    "            Parameter Space:\n",
    "                - l1 (int): Number of units in the first hidden layer. Choices: powers of 2 from 1 to 256.\n",
    "                - l2 (int): Number of units in the second hidden layer. Choices: powers of 2 from 1 to 256.\n",
    "                - l3 (int): Number of units in the third hidden layer. Choices: powers of 2 from 1 to 256.\n",
    "                - lr (float): Learning rate for the optimizer. Log-uniform range: [1e-3, 1e-2].\n",
    "                - batch_size (int): Batch size for training and validation. Choices: [128, 256, 512].\n",
    "                - weight_decay (float): Weight decay (L2 regularization). Choices: [5e-4, 1e-5].\n",
    "\n",
    "            Tuning Configuration:\n",
    "                - Metric: 'auc_roc_mean'\n",
    "                - Mode: 'max'\n",
    "                - Search Algorithm: OptunaSearch\n",
    "                - Number of Samples: n_samples\n",
    "                - Experiment Name: 'Cross Hyper Tunning'\n",
    "\n",
    "            Output:\n",
    "                Prints the configuration and metrics of the best trial, including:\n",
    "                    - Loss mean\n",
    "                    - Accuracy mean\n",
    "                    - Precision mean\n",
    "                    - Negative Predictive Value (NPV) mean\n",
    "                    - Recall mean\n",
    "                    - AUC-ROC mean\n",
    "                    - Standard deviation of AUC-ROC\n",
    "\n",
    "            Raises:\n",
    "                Exception: If the tuning process fails, an error message is printed.\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                tunner = Tuner(\n",
    "                    self._cross_tunning,\n",
    "                    param_space = {\n",
    "                        'l1': tune.choice([2 ** i for i in range(9)]),\n",
    "                        'l2': tune.choice([2 ** i for i in range(9)]),\n",
    "                        'l3': tune.choice([2 ** i for i in range(9)]),\n",
    "                        'lr': tune.loguniform(1e-3, 1e-2),\n",
    "                        'batch_size': tune.choice([128, 256, 512]),\n",
    "                        'weight_decay': tune.choice([5e-4, 1e-5]),\n",
    "                    },\n",
    "\n",
    "                    tune_config = TuneConfig(\n",
    "                        metric = 'auc_roc_mean',\n",
    "                        mode = 'max', \n",
    "                        num_samples = n_samples,\n",
    "                        search_alg = OptunaSearch(),\n",
    "                    ),\n",
    "                    run_config = RunConfig(name = 'Cross Hyper Tunning')\n",
    "                )\n",
    "\n",
    "                results = tunner.fit()\n",
    "\n",
    "                # Printing results\n",
    "                best_result = results.get_best_result(metric= 'auc_roc_mean', mode = 'max')\n",
    "                print(f'\\nâœ… Best trial config: \\n{best_result.config}')\n",
    "                print(f'\\nâœ… Best Trial Final Validation Metrics:')\n",
    "                print(f\"ðŸ”´ Loss: {best_result.metrics['loss_mean']}\")\n",
    "                print(f\"ðŸŸ  Accuracy: {best_result.metrics['accuracy_mean']}\")\n",
    "                print(f\"ðŸ”µ Precision: {best_result.metrics['precision_mean']}\")\n",
    "                print(f\"ðŸ”µ NPV: {best_result.metrics['npv_mean']}\")\n",
    "                print(f\"âš ï¸ Recall: {best_result.metrics['recall_mean']}\")\n",
    "                print(f\"ðŸŽ¯ AUC-ROC: {best_result.metrics['auc_roc_mean']}\")\n",
    "                print(f\"â˜‘ï¸ Standard Deviation AUC-ROC: {best_result.metrics['auc_roc_std']}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to run hypertuning tests in config space: {str(e)}.')\n",
    "\n",
    "        ### Final Training Function ###\n",
    "        def FinalTraining(\n",
    "            self,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Executes the final training routine for the neural network, including \n",
    "            data preparation, training, validation, early stopping, and metric \n",
    "            reporting.\n",
    "\n",
    "            This method:\n",
    "            - Splits the dataset into training and validation sets.\n",
    "            - Performs oversampling to handle class imbalance.\n",
    "            - Initializes the model, loss function, optimizer, and learning rate schedulers.\n",
    "            - Trains the model with early stopping based on a selected target score.\n",
    "            - Evaluates the model on a validation set.\n",
    "            - Plots metrics and the final confusion matrix.\n",
    "\n",
    "            The training loop collects accuracy, precision, recall, NPV, AUC, and loss\n",
    "            for both training and validation sets across epochs. Learning rate scheduling \n",
    "            uses a warm-up phase followed by cosine annealing.\n",
    "\n",
    "            Args:\n",
    "                self: \n",
    "                    An instance of the training class containing:\n",
    "                        - trainset (Dataset): The dataset used for training and validation split.\n",
    "                        - target_score (str): The metric used for early stopping ('accuracy', 'recall', 'roc', 'precision', or 'npv').\n",
    "                        - batch_size (int): Batch size for training and validation data loaders.\n",
    "                        - num_workers (int): Number of worker threads for data loading.\n",
    "                        - seed (int): Random seed for reproducibility in splitting.\n",
    "                        - l1, l2, l3 (int): Sizes of the hidden layers in the network.\n",
    "                        - dropout_rate (float): Dropout probability for regularization.\n",
    "                        - lr (float): Initial learning rate for the optimizer.\n",
    "                        - weight_decay (float): Weight decay (L2 regularization) for the optimizer.\n",
    "                        - max_epochs (int): Maximum number of training epochs.\n",
    "                        - early_stopping_p (int): Patience for early stopping.\n",
    "                        - early_stopping_mode (str): Mode for early stopping ('min' or 'max').\n",
    "                        - save_path_model (str): File path to save the best model weights.\n",
    "\n",
    "            Raises:\n",
    "                Exception: If any part of the training process fails, an error message \n",
    "                will be printed containing the exception details.\n",
    "\n",
    "            Prints:\n",
    "                - Class imbalance ratio in the training set.\n",
    "                - Final metrics for training and validation sets (Loss, Accuracy, Precision, \n",
    "                NPV, Recall, AUC-ROC).\n",
    "\n",
    "            Side Effects:\n",
    "                - Saves the best-performing model to disk.\n",
    "                - Generates and displays plots for metrics and the final confusion matrix.\n",
    "            \"\"\"\n",
    "            try:\n",
    "\n",
    "                # Initialize Metrics for binary classification\n",
    "\n",
    "                # Training\n",
    "                accuracy_train = BinaryAccuracy()\n",
    "                recall_train = BinaryRecall()\n",
    "                auc_train = BinaryAUROC(thresholds = None)\n",
    "                precision_train = BinaryPrecision()\n",
    "                npv_train = BinaryNegativePredictiveValue()\n",
    "\n",
    "                # Validation\n",
    "                accuracy_val = BinaryAccuracy()\n",
    "                recall_val = BinaryRecall()\n",
    "                auc_val = BinaryAUROC(thresholds = None)\n",
    "                precision_val = BinaryPrecision()\n",
    "                npv_val = BinaryNegativePredictiveValue()\n",
    "\n",
    "                # Score target\n",
    "                if self.target_score == 'acuracy':\n",
    "                    target_score = BinaryAccuracy()\n",
    "                \n",
    "                elif self.target_score == 'recall':\n",
    "                    target_score = BinaryRecall()\n",
    "                \n",
    "                elif self.target_score == 'roc':\n",
    "                    target_score = BinaryAUROC(thresholds = None)\n",
    "                \n",
    "                elif self.target_score == 'precision':\n",
    "                    target_score = BinaryPrecision()\n",
    "\n",
    "                elif self.target_score == 'npv':\n",
    "                    target_score = BinaryNegativePredictiveValue()\n",
    "\n",
    "                \n",
    "\n",
    "                # Separating training and validation data\n",
    "                    # Spliting trainset and validationset\n",
    "\n",
    "                train_size = int(len(self.trainset) * 0.8)\n",
    "                train_set, val_set = torch.utils.data.random_split(\n",
    "                    self.trainset, [train_size, len(self.trainset) - train_size],\n",
    "                    generator = torch.Generator().manual_seed(self.seed)\n",
    "                )\n",
    "\n",
    "                sampler, prop_class_positive =  self._oversampling(trainset = train_set)\n",
    "                \n",
    "                # Train Loader\n",
    "                trainloader = torch.utils.data.DataLoader(\n",
    "                    train_set, \n",
    "                    batch_size = self.batch_size, \n",
    "                    sampler = sampler, \n",
    "                    num_workers = self.num_workers,\n",
    "                    drop_last = True,\n",
    "                )\n",
    "\n",
    "                # Val Loader\n",
    "                valloader = torch.utils.data.DataLoader(\n",
    "                    val_set, \n",
    "                    batch_size = self.batch_size, \n",
    "                    shuffle = False, \n",
    "                    num_workers = self.num_workers, \n",
    "                    drop_last = False,\n",
    "                )\n",
    "                \n",
    "\n",
    "        \n",
    "                # Loading Net\n",
    "                net = PyTorch.Net(\n",
    "                    l1 = self.l1,\n",
    "                    l2 = self.l2, \n",
    "                    l3 = self.l3, \n",
    "                    dropout_rate = self.dropout_rate,\n",
    "                    prior_minoritary_class = prop_class_positive,  \n",
    "                )\n",
    "                # Moving the network to the device\n",
    "                net, device = self._device(net)\n",
    "\n",
    "                # Criterion\n",
    "                criterion = PyTorch.FocalLoss().to(device)\n",
    "\n",
    "                # Optimizer\n",
    "                optimizer = optim.AdamW(net.parameters(), lr = self.lr, weight_decay = self.weight_decay) \n",
    "\n",
    "                # Scheduler\n",
    "                # Warmup (linear from 1e-5 to 0.001)\n",
    "                warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "                    optimizer,\n",
    "                    start_factor = 0.01,\n",
    "                    total_iters = 10,\n",
    "                )\n",
    "                # Cosine Annealing after warmup\n",
    "                cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer,\n",
    "                    T_max = self.max_epochs - 10,\n",
    "                    eta_min = 1e-6,\n",
    "                )\n",
    "                # Composition: 10 warmup epochs + (max_epochs cosine - 10 warmup)\n",
    "                scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "                    optimizer, \n",
    "                    schedulers = [warmup_scheduler, cosine_scheduler],\n",
    "                    milestones = [10],\n",
    "                )\n",
    "\n",
    "                # Adjusting error caused by scheduler\n",
    "                warnings.filterwarnings('ignore', category = UserWarning)\n",
    "                \n",
    "                # Early Stopping\n",
    "                early_stopping = PyTorch().EarlyStopping(\n",
    "                    patience = self.early_stopping_p, \n",
    "                    mode = self.early_stopping_mode, \n",
    "                    save_path = self.save_path_model\n",
    "                )\n",
    "                \n",
    "                # Metrics for epochs\n",
    "                avg_loss_t, avg_accuracy_t, avg_recall_t, avg_auc_t, avg_precision_t, avg_npv_t = [], [], [], [], [], []\n",
    "                avg_loss_v, avg_accuracy_v, avg_recall_v, avg_auc_v, avg_precision_v, avg_npv_v = [], [], [], [], [], []\n",
    "                \n",
    "                # Epochs\n",
    "                for epoch in range(0, self.max_epochs):\n",
    "\n",
    "                    # Metrics Training\n",
    "                    train_loss = 0.0 \n",
    "                    train_steps = 0 \n",
    "                    accuracy_train.reset()\n",
    "                    recall_train.reset()\n",
    "                    auc_train.reset()\n",
    "                    precision_train.reset()\n",
    "                    npv_train.reset()\n",
    "\n",
    "                    # Metrics Validation\n",
    "                    val_loss = 0.0\n",
    "                    val_steps = 0\n",
    "                    accuracy_val.reset()\n",
    "                    recall_val.reset()\n",
    "                    auc_val.reset()\n",
    "                    precision_val.reset()\n",
    "                    npv_val.reset()\n",
    "\n",
    "                    # Target Score\n",
    "                    target_score.reset()\n",
    "\n",
    "                    # Save preds and labels\n",
    "                    preds_val, labels_val = [], []\n",
    "                    \n",
    "                    # Training\n",
    "                    net.train()\n",
    "\n",
    "                    for cat_input, num_input, labels in (trainloader):\n",
    "            \n",
    "                        # Inputs + Labels to(device)    \n",
    "                        cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                        \n",
    "                        # Zero the parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Foward Pass\n",
    "                        outputs = net(cat_input, num_input)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward + optimize\n",
    "                        loss.backward()\n",
    "                        # Optimizer\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # Accumulating Loss\n",
    "                        train_loss += loss.item()\n",
    "                        train_steps += 1\n",
    "\n",
    "                        # Updating metrics\n",
    "                        accuracy_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                        recall_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                        auc_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                        precision_train.update(torch.sigmoid(outputs), labels.int())\n",
    "                        npv_train.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                    # Evaluation\n",
    "                    net.eval()\n",
    "\n",
    "                    # Disabling gradient calculations\n",
    "                    with torch.no_grad():\n",
    "                        # Get the inputs; data is a list of [inputs, labels]\n",
    "                        for cat_input, num_input, labels in (valloader):\n",
    "                                \n",
    "                                # Inputs + Labels to(device)\n",
    "                                cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                \n",
    "                                # Eval net\n",
    "                                outputs = net(cat_input, num_input)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                                \n",
    "                                # Accumulating Loss\n",
    "                                val_loss += loss.item()\n",
    "                                val_steps += 1\n",
    "\n",
    "                                # Updating metrics\n",
    "                                accuracy_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                recall_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                auc_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                precision_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                npv_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                \n",
    "                                # Target Score\n",
    "                                target_score.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                    # Saving metrics by epoch\n",
    "                    # Train\n",
    "                    avg_accuracy_t.append(accuracy_train.compute().item())\n",
    "                    avg_recall_t.append(recall_train.compute().item())\n",
    "                    avg_auc_t.append(auc_train.compute().item())\n",
    "                    avg_precision_t.append(precision_train.compute().item())\n",
    "                    avg_npv_t.append(npv_train.compute().item())\n",
    "                    avg_loss_t.append(train_loss / train_steps)\n",
    "                    # Validation\n",
    "                    avg_accuracy_v.append(accuracy_val.compute().item())\n",
    "                    avg_recall_v.append(recall_val.compute().item())\n",
    "                    avg_auc_v.append(auc_val.compute().item())\n",
    "                    avg_precision_v.append(precision_val.compute().item())\n",
    "                    avg_npv_v.append(npv_val.compute().item())\n",
    "                    avg_loss_v.append(val_loss / val_steps)\n",
    "                                \n",
    "                    # Scheduler step\n",
    "                    scheduler.step()\n",
    "\n",
    "                    # Early_stopping step\n",
    "                    early_stopping(score = target_score.compute().item(), model = net, epoch = epoch)\n",
    "                    # Stopping \n",
    "                    if early_stopping.early_stop:\n",
    "                        print(f'>>>>>>> Finished Training.')\n",
    "                    \n",
    "                        # Final Validation Score Model\n",
    "                        # Load the Best Model\n",
    "                        net.load_state_dict(torch.load(self.save_path_model))\n",
    "                        \n",
    "\n",
    "                        # Metrics Validation\n",
    "                        val_loss = 0.0\n",
    "                        val_steps = 0\n",
    "                        accuracy_val.reset()\n",
    "                        recall_val.reset()\n",
    "                        auc_val.reset()\n",
    "                        precision_val.reset()\n",
    "                        npv_val.reset()\n",
    "                        \n",
    "                        # Evaluation\n",
    "                        net.eval()\n",
    "                        # Disabling gradient calculations\n",
    "                        with torch.no_grad():\n",
    "                            # Get the inputs; data is a list of [inputs, labels]\n",
    "                            for cat_input, num_input, labels in (valloader):\n",
    "                                    \n",
    "                                    # Inputs + Labels to(device)\n",
    "                                    cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                                    \n",
    "                                    # Eval net\n",
    "                                    outputs = net(cat_input, num_input)\n",
    "                                    loss = criterion(outputs, labels)\n",
    "                                    \n",
    "                                    # Accumulating Loss\n",
    "                                    val_loss += loss.item()\n",
    "                                    val_steps += 1\n",
    "\n",
    "                                    # Updating metrics\n",
    "                                    accuracy_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    recall_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    auc_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    precision_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    npv_val.update(torch.sigmoid(outputs), labels.int())\n",
    "                                    \n",
    "                                    # Accumulating Predictions\n",
    "                                    preds_val.append(torch.sigmoid(outputs).detach().cpu())\n",
    "                                    labels_val.append(labels.detach().cpu())\n",
    "                    \n",
    "                        # Concatenates all batches\n",
    "                        preds_val = torch.cat(preds_val).float()\n",
    "                        labels_val = torch.cat(labels_val).long()\n",
    "\n",
    "                        break\n",
    "\n",
    "                # Distribution of the minority class\n",
    "                print(f'\\nâš–ï¸ The Distribution of the minority class (prop_class_positive): {prop_class_positive}')\n",
    "\n",
    "                # Metrics out\n",
    "                print('\\nâœ… Train Metrics:') \n",
    "                print(f'Loss: {train_loss / train_steps:.3f}')   \n",
    "                print(f'Accuracy: {accuracy_train.compute().item() * 100:> 0.1f}%') \n",
    "                print(f'Precision: {precision_train.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'NPV: {npv_train.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'Recall: {recall_train.compute().item() *100:> 0.1f}%') \n",
    "                print(f'AUC-ROC: {auc_train.compute().item() *100:> 0.1f}%') \n",
    "\n",
    "                print('\\nâ˜‘ï¸ Validation Metrics:')\n",
    "                print(f'Loss: {val_loss / val_steps:.3f}')\n",
    "                print(f'Accuracy: {accuracy_val.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'Precision: {precision_val.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'NPV: {npv_val.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'Recall: {recall_val.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'AUC-ROC: {auc_val.compute().item() * 100:> 0.1f}%')\n",
    "                \n",
    "                # Graphics for Training and Validation\n",
    "                self._plot_metrics(\n",
    "                    avg_loss_t = avg_loss_t, \n",
    "                    avg_loss_v = avg_loss_v,\n",
    "                    avg_accuracy_t = avg_accuracy_t, \n",
    "                    avg_accuracy_v = avg_accuracy_v,\n",
    "                    avg_precision_t = avg_precision_t, \n",
    "                    avg_precision_v = avg_precision_v,\n",
    "                    avg_npv_t = avg_npv_t, \n",
    "                    avg_npv_v = avg_npv_v,\n",
    "                    avg_recall_t = avg_recall_t, \n",
    "                    avg_recall_v = avg_recall_v,\n",
    "                    avg_auc_t = avg_auc_t, \n",
    "                    avg_auc_v = avg_auc_v\n",
    "                )\n",
    "\n",
    "                self._confusion_matrix(\n",
    "                    preds = preds_val,\n",
    "                    labels = labels_val,\n",
    "                    title = 'Confusion Matrix Final Validation'\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to execute final training flow: {str(e)}.')\n",
    "        \n",
    "        ### Final Test Function ###\n",
    "        def FinalTest(\n",
    "            self,\n",
    "            net,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Executes the final evaluation of a trained neural network model on the test dataset,\n",
    "            computing multiple performance metrics and displaying a confusion matrix.\n",
    "\n",
    "            This method:\n",
    "                - Loads the trained model onto the appropriate device (CPU or GPU).\n",
    "                - Iterates over the test dataset without gradient computation.\n",
    "                - Calculates the loss and multiple binary classification metrics.\n",
    "                - Stores predictions and ground truth labels for later analysis.\n",
    "                - Displays the final evaluation results and a confusion matrix.\n",
    "\n",
    "            Args:\n",
    "                net (torch.nn.Module):\n",
    "                    The trained PyTorch neural network model to be evaluated.\n",
    "\n",
    "            Returns:\n",
    "                tuple:\n",
    "                    - preds_test (torch.Tensor): Tensor containing all predicted probabilities for the test dataset.\n",
    "                    - labels_test (torch.Tensor): Tensor containing all ground truth labels for the test dataset.\n",
    "\n",
    "            Raises:\n",
    "                Exception:\n",
    "                    If any error occurs during the evaluation process.\n",
    "\n",
    "            Notes:\n",
    "                - This method uses a `FocalLoss` criterion for evaluation.\n",
    "                - Metrics computed:\n",
    "                    * Binary Accuracy\n",
    "                    * Binary Precision\n",
    "                    * Binary Negative Predictive Value (NPV)\n",
    "                    * Binary Recall\n",
    "                    * Binary Area Under the ROC Curve (AUC-ROC)\n",
    "                - Predictions are probability scores obtained from `torch.sigmoid(outputs)`.\n",
    "                - The confusion matrix is generated via the `_confusion_matrix` method.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                \n",
    "                # Net Loading\n",
    "                net, device = self._device(net)\n",
    "\n",
    "                # Test Loader\n",
    "                testloader = torch.utils.data.DataLoader(\n",
    "                    self.testset, batch_size = 4, shuffle = False, num_workers = 2, \n",
    "                    drop_last = False,\n",
    "                )\n",
    "                \n",
    "                # Criterion\n",
    "                criterion = PyTorch.FocalLoss().to(device)\n",
    "\n",
    "                # Evaluation\n",
    "                net.eval()\n",
    "\n",
    "                # Metrics Testing\n",
    "                test_loss = 0.0 \n",
    "                test_steps = 0 \n",
    "                \n",
    "                accuracy_test = BinaryAccuracy()\n",
    "                precision_test = BinaryPrecision()\n",
    "                npv_test = BinaryNegativePredictiveValue()\n",
    "                recall_test = BinaryRecall()\n",
    "                auc_test = BinaryAUROC(thresholds = None)\n",
    "                \n",
    "                # Save preds and labels\n",
    "                preds_test, labels_test = [], []\n",
    "\n",
    "                # Disabling gradient calculations\n",
    "                with torch.no_grad():\n",
    "                    # Get the inputs; data is a list of [inputs, labels]\n",
    "                    for cat_input, num_input, labels in (testloader):\n",
    "                            \n",
    "                            # Inputs + Labels to(device)\n",
    "                            cat_input, num_input, labels = cat_input.to(device), num_input.to(device), labels.to(device)\n",
    "                            \n",
    "                            # Eval net\n",
    "                            outputs = net(cat_input, num_input)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            # Updating Metrics\n",
    "                            accuracy_test.update(torch.sigmoid(outputs), labels.int())\n",
    "                            precision_test.update(torch.sigmoid(outputs), labels.int())\n",
    "                            npv_test.update(torch.sigmoid(outputs), labels.int())\n",
    "                            recall_test.update(torch.sigmoid(outputs), labels.int())\n",
    "                            auc_test.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "                            # Accumulating Loss\n",
    "                            test_loss += loss.item()\n",
    "                            test_steps += 1\n",
    "                            # Accumulating Predictions\n",
    "                            preds_test.append(torch.sigmoid(outputs).detach().cpu())\n",
    "                            labels_test.append(labels.detach().cpu())\n",
    "\n",
    "                # Concatenates all batches\n",
    "                preds_test = torch.cat(preds_test).float()\n",
    "                labels_test = torch.cat(labels_test).long()\n",
    "                \n",
    "                # Metrics Out\n",
    "                print('\\nâœ… Test Metrics:')\n",
    "                print(f'Loss: {test_loss / test_steps:.3f}')\n",
    "                print(f'Accuracy: {accuracy_test.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'Precision: {precision_test.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'NPV: {npv_test.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'Recall: {recall_test.compute().item() * 100:> 0.1f}%')\n",
    "                print(f'AUC-ROC: {auc_test.compute().item() * 100:> 0.1f}%')\n",
    "\n",
    "                self._confusion_matrix(\n",
    "                    preds = preds_test,\n",
    "                    labels = labels_test,\n",
    "                    title = 'Confusion Matrix Final Test'\n",
    "                )\n",
    "                # Return Preds\n",
    "                return preds_test, labels_test\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'[ERROR] Failed to execute test flow: {str(e)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1475345-4428-4a97-b345-4dcd0126c1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6b8893-ca1e-4b2b-886d-1f0633b9ecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pandas:\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fd6f6b-7dfd-4da2-b041-0d0de43a3dda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 4. Modeling\n",
    "---  \n",
    "In this stage, **I will test classical machine learning models** to evaluate their performance on the training data. The approach will be **intentionally simple** (without complex hyperparameter tuning or advanced preprocessing techniques), as algorithms like **Random Forest, Logistic Regression, and SVM** typically perform better with straightforward data transformations.  \n",
    "\n",
    "---  \n",
    "\n",
    "After this initial analysis, **I will prioritize the projectâ€™s main model**: a **neural network developed in PyTorch**. This architecture was chosen due to its:  \n",
    "\n",
    "- **Ability to identify complex patterns** in non-linear data.  \n",
    "- **Flexibility to adapt to class imbalances** (e.g., the observed 84%-16% class distribution).  \n",
    "- **Generalization capability** (Highly efficient with unseen data).  \n",
    "\n",
    "However, neural networks require **specific preprocessing**, particularly to address:  \n",
    "1. **High-cardinality categorical variables** (e.g., unique identifiers).  \n",
    "2. **Asymmetric distributions** (identified during the EDA phase).  \n",
    "3. **Data noise** (such as outliers in numerical variables).  \n",
    "\n",
    "To address these, I will apply:  \n",
    "- **Embedding layers** for categorical variables.  \n",
    "- **Cross-validation** to verify and adjust data across different partitions.  \n",
    "- **Regularization techniques** (e.g., *dropout*) to prevent *overfitting*.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Modeling Split into Two Phases \n",
    "#### **Phase 1: Classical Machine Learning Models**  \n",
    "| **Objective** | **Tools** | **Metric** |  \n",
    "|---------------|------------|-------------|  \n",
    "| Establish a performance baseline for future comparison. | Scikit-learn (Decision Trees, SVM, Logistic Regression). | AUC-ROC. |  \n",
    "\n",
    "#### **Phase 2: PyTorch Neural Network**  \n",
    "| **Objective** | **Tools** | **Metric** |  \n",
    "|---------------|------------|-------------|  \n",
    "| Achieve better generalization on unseen data. | PyTorch, Torchmetrics, Ray Tune. | AUC-ROC, Recall, Acurracy. |  \n",
    "\n",
    "---  \n",
    "\n",
    "### Evaluation Metric Choice: AUC-ROC \n",
    "#### Why AUC-ROC?  \n",
    "| **Criterion** | **Explanation** | **Business Impact** |  \n",
    "|---------------|------------------|----------------------|  \n",
    "| **Class imbalance** | Balances *recall* (capturing churning customers) and *specificity* (avoiding unnecessary actions on loyal customers). | Reduces operational costs by prioritizing high-risk customers. |  \n",
    "| **Asymmetric cost sensitivity** | False negatives (missing churn) are more critical than false positives. | Improves retention campaign efficacy (e.g., personalized offers). |  \n",
    "| **Universal interpretability** | Scores above **0.85** indicate strong predictive power for binary classification. | Simplifies communication with non-technical stakeholders. |  \n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3128cbf-0eb6-4347-bc0f-e3456672a821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Loading data train and data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ce2c13-f0d2-4b2b-8376-b9899204b271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and file type -- train\n",
    "file_location  = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Gold/train'\n",
    "train = DataSparkPS(file_location = file_location).load_data(file_type = 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bb30dd-d475-4d91-8ab5-6c3e8458f40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and file type -- train\n",
    "file_location  = 'dbfs:/FileStore/DS_Credit-Card_Churn_Analysis/Datasets/Gold/test'\n",
    "test = DataSparkPS(file_location = file_location).load_data(file_type = 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947695d3-d3c1-4386-8157-83911551d9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e8c19a-b9f9-469c-8064-22955468da3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c09457-c5a0-4d1e-8d6b-a86fd1df750a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Separating features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934a12c0-85e7-459b-a33a-75b6092cdbd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train = train.drop(columns = 'churn_target')\n",
    "y_train = train['churn_target'].copy()\n",
    "# Test \n",
    "X_test = test.drop(columns = 'churn_target')\n",
    "y_test = test['churn_target'].copy()\n",
    "\n",
    "# Checking the dimensions of the training and test data\n",
    "print(f'The Train features dataset shape: {X_train.shape}')\n",
    "print(f'The Train labels dataset shape: {y_train.shape}')\n",
    "print(f'\\nThe Test features dataset shape: {X_test.shape}')\n",
    "print(f'The Test labels dataset shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d0431c6-97b8-4e08-8b4c-d432b2c62c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa52cbf-75a7-442b-8475-c0c638abfda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating lists of numeric variables\n",
    "continuos_numerical = X_train.select_dtypes('float64').columns.tolist()\n",
    "discrete_numerical = X_train.select_dtypes('int32').columns.tolist()\n",
    "\n",
    "numerical_features = continuos_numerical\n",
    "numerical_features.extend(discrete_numerical)\n",
    "\n",
    "# Printing the quantity and numeric columns\n",
    "print(f'There are {len(numerical_features)} numerical features.')\n",
    "print('\\nThere are:')\n",
    "numerical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2be6cc7-1c78-412f-bd3c-d7423268b39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f35be0-a5a1-4b41-b21f-ce2b12862335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating lists of categorical variables\n",
    "categorical_features = X_train.select_dtypes('object').columns.tolist()\n",
    "\n",
    "# Printing the quantity and categorical columns\n",
    "print(f'There are {len(categorical_features)} categorical features.')\n",
    "print('\\nThere are:')\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85203b04-9a4f-476b-aec6-3a870582d395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "  print(feature)\n",
    "  print('-' * 40)\n",
    "  print(f'There are {X_train[feature].nunique()} unique values there are: ')\n",
    "  print(X_train[feature].value_counts(normalize = True))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51f318a9-9aa4-4e03-afd7-b70eb557f98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b6f700-f717-44c5-98cd-ed4bb54bbf52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  print('churn_target')\n",
    "  print('-' * 40)\n",
    "  print(f'There are {y_train.nunique()} unique values there are for churn_target: ')\n",
    "  print(y_train.value_counts(normalize = True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9988eefe-bf87-4855-8701-4c74caf6fd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training Classics Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042705a9-07df-4b5c-89c1-73ad4b764aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98384a6c-019d-464f-8caa-1d5940d5b73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard Scaler Features\n",
    "std_scaler_features = [\n",
    "    'credit_limit', 'total_amt_chng_q4_q1', 'total_ct_chng_q4_q1','avg_utilization_ratio','customer_age','dependent_count','months_on_book', 'total_relationship_count', 'months_inactive_12_mon', 'contacts_count_12_mon', 'total_revolving_bal', 'total_trans_amt', 'total_trans_ct'\n",
    "]\n",
    "\n",
    "# One Hot Encoder Features\n",
    "one_hot_features = [\n",
    "    'gender', 'marital_status',\n",
    "]\n",
    "\n",
    "# Ordinal Features\n",
    "ordinal_features = ['education_level', 'income_category', 'card_category']\n",
    "ordinal_features_order = {\n",
    "    'education_level': ['Unknown', 'Uneducated', 'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate'],\n",
    "    'income_category': ['Unknown', 'Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n",
    "    'card_category': ['Blue', 'Silver', 'Gold', 'Platinum']\n",
    "}\n",
    "\n",
    "# Pipeline Ordinal Features\n",
    "ordinal_pipeline = Pipeline(\n",
    "    steps = [\n",
    "        ('ordinal_encoder', OrdinalEncoder(categories = [\n",
    "            ordinal_features_order['education_level'], \n",
    "            ordinal_features_order['income_category'],\n",
    "            ordinal_features_order['card_category']\n",
    "        ])),\n",
    "        ('standard_scaler', StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "classic_ml_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('one_hot_features', OneHotEncoder(), one_hot_features),\n",
    "        ('ordinal_features', ordinal_pipeline, ordinal_features),\n",
    "        ('std_scaler_features', StandardScaler(), std_scaler_features),\n",
    "        \n",
    "    ], \n",
    "    remainder = 'passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "583e9176-36eb-40f0-b5c1-a3c7a5e93bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking data train preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c3df38-f273-4504-b2e0-c0be5ce5536a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_preprocessed = classic_ml_preprocessor.fit_transform(X_train)\n",
    "pd.DataFrame(X_train_preprocessed, columns = classic_ml_preprocessor.get_feature_names_out(X_train.columns)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15b21634-2f55-487b-951f-bfab2ac5d441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Shape of train preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cdf68b5-7cfe-43c8-adca-02b9620f2f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04f47912-2d4c-4216-841b-754a4f30b8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Applying Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4556771-5685-4c34-860b-03268e8af59a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "\n",
    "    ('Logistic Regression', \n",
    "     Pipeline([('model', LogisticRegression(\n",
    "         random_state = 33, \n",
    "         class_weight = 'balanced',\n",
    "    ))])),\n",
    "\n",
    "    ('Decision Tree Classifier', \n",
    "     Pipeline([('model', DecisionTreeClassifier(\n",
    "         random_state = 33, \n",
    "         class_weight = 'balanced',\n",
    "         max_depth = 5,\n",
    "         criterion = 'gini',\n",
    "         min_impurity_decrease = 0.001,\n",
    "    ))])),\n",
    "\n",
    "\n",
    "    ('Random Forest Classifier', \n",
    "     Pipeline([('model', RandomForestClassifier(\n",
    "        random_state = 33,\n",
    "        class_weight = 'balanced',\n",
    "        n_estimators = 100,\n",
    "        max_depth = 10,\n",
    "        min_samples_split = 2,\n",
    "        \n",
    "    ))])),\n",
    "     \n",
    "    ('KNeighbors Classifier', \n",
    "     Pipeline([('model', KNeighborsClassifier(\n",
    "        n_neighbors = 5,\n",
    "        weights = 'distance',\n",
    "        metric = 'minkowski'\n",
    "    ))])),\n",
    "\n",
    "    ('Suport Vector Machine Classifier', \n",
    "     Pipeline([('model', SVC(\n",
    "        random_state = 33,\n",
    "        class_weight = 'balanced',\n",
    "        C = 1.0,\n",
    "        kernel = 'rbf',\n",
    "        gamma = 'scale',\n",
    "        probability = True, \n",
    "    ))])),\n",
    "\n",
    "    ('Gradient Boosting Classifier', \n",
    "     Pipeline([('model', GradientBoostingClassifier(\n",
    "        random_state = 33,\n",
    "        n_estimators = 200,\n",
    "        max_depth = 3,\n",
    "        learning_rate = 0.1,\n",
    "        subsample = 0.7\n",
    "    ))])),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3ce01a-4761-4343-ae2d-a52c2f6a9f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cross_validation_ml(\n",
    "    models = models,\n",
    "    x_train = X_train_preprocessed,\n",
    "    y_train = y_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57de380d-e9cc-429b-a4ed-0146f6ec3639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Training PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b5f596-9330-4318-b516-80aa6661fec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Preprocessing for PyTorch \n",
    "Neural networks benefit from **transformations and scaling that preserve the natural distribution of the data**, ensuring stability during training. For this, I adopted the following strategy:  \n",
    "\n",
    "#### 1. Numerical Variables \n",
    "| **Technique** | **Applied Variables** | **Justification** |  \n",
    "|-------------|--------------------------|-------------------|  \n",
    "| **MinMaxScaler** | Most variables (e.g., age, number of transactions). | Preserves the original scale within [0, 1] intervals. |  \n",
    "| **Robust Scaler** | `credit_limit`, `total_trans_amt`. | Minimizes the impact of **outliers** and wide amplitude (e.g., values between $500 and $50,000), maintaining statistical robustness. |  \n",
    "\n",
    "**Prior Checks**:  \n",
    "- Distribution analysis (histograms and boxplots).  \n",
    "- Initial model performance tests with Robustscaler.  \n",
    "\n",
    "---  \n",
    "\n",
    "#### 2. Categorical Variables  \n",
    "For categorical variables, I used **embedding layers** in PyTorch because:  \n",
    "- **Learning specific patterns**: Dense representations capture hierarchical relationships (e.g., \"Blue\" â†’ \"Gold\" â†’ \"Platinum\").  \n",
    "- **Handling high cardinality**: Reduces dimensionality of variables.  \n",
    "- **Direct network integration**: Avoids one-hot encoding, which increases sparsity and computational cost.  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df6ce41-91eb-4b82-abe9-c27e7437fa1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "data_ax = train[numerical_features]\n",
    "# Initialize Graphics\n",
    "numerical_graphics = GraphicsData(data_ax)\n",
    "# Numerical Histograms\n",
    "numerical_graphics.numerical_histograms()\n",
    "# Numerical Boxplots\n",
    "numerical_graphics.numerical_boxplots(showfliers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1df5a34-c2fd-4021-9f8a-810c5ccc1743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nominal features\n",
    "nominal_features = [\n",
    "    'gender', 'marital_status'\n",
    "]\n",
    "\n",
    "\n",
    "# Ordinal features\n",
    "ordinal_features = ['education_level', 'income_category', 'card_category']\n",
    "ordinal_features_order = {\n",
    "    'education_level': ['Unknown', 'Uneducated', 'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate'],\n",
    "    'income_category': ['Unknown', 'Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n",
    "    'card_category': ['Blue', 'Silver', 'Gold', 'Platinum']\n",
    "}\n",
    "\n",
    "# Pipeline ordinal features\n",
    "ordinal_pipeline = Pipeline(\n",
    "    steps = [\n",
    "        ('ordinal_encoder', OrdinalEncoder(categories = [\n",
    "            ordinal_features_order['education_level'], \n",
    "            ordinal_features_order['income_category'],\n",
    "            ordinal_features_order['card_category']\n",
    "        ])),\n",
    "        \n",
    "    ]\n",
    ") \n",
    "\n",
    "# Robust features\n",
    "robust_scaler_features = [\n",
    "    'credit_limit', 'total_trans_amt', \n",
    "]\n",
    "\n",
    "# Standard Scaler features\n",
    "std_scaler_features = []\n",
    "\n",
    "# MixMax features\n",
    "minmax_scaler_features = [\n",
    "    'months_on_book', 'customer_age', 'dependent_count',\n",
    "    'total_relationship_count', 'months_inactive_12_mon',\n",
    "    'contacts_count_12_mon', 'total_revolving_bal', 'avg_utilization_ratio', \n",
    "    'total_amt_chng_q4_q1', 'total_ct_chng_q4_q1', 'total_trans_ct', \n",
    "]\n",
    "\n",
    "# Final Preprocessor\n",
    "pytorch_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('nominal_features', OrdinalEncoder(), nominal_features),\n",
    "        ('ordinal_features', ordinal_pipeline, ordinal_features),\n",
    "        ('minmax_scaler_features', MinMaxScaler(), minmax_scaler_features),\n",
    "        ('robust_scaler_features', RobustScaler(), robust_scaler_features),        \n",
    "    ], \n",
    "    remainder = 'passthrough'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3071e3-945b-42c1-9852-fd02c19b487c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_train = pytorch_preprocessor.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5800f291-6a81-4e09-93ae-93811327348e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocessed_train, columns = pytorch_preprocessor.get_feature_names_out(train.columns)).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a771806c-bb56-46d0-b0b9-0fa684124eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "221afcd9-86d4-4e5d-81bd-f0343560507c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Dataset Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9d65b0-5419-48e6-b04b-13ca371e2283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set = PyTorch.Dataset(\n",
    "    dataset = preprocessed_train,\n",
    "    cat_idx = [0, 5],\n",
    "    num_idx = [5, 18],\n",
    "    label_idx = [18, 19],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e0836f6-d10a-460b-93ca-1f79fc6c6399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checking the transformations of data into tensors and their dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b607566d-8d19-4cc8-bf79-d70d10ea0682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical, numerical, label = train_set[0]\n",
    "print(f'The Categorical tensor {categorical}')\n",
    "print(f'\\nThe Numerical tensor {numerical}')\n",
    "print(f'\\nThe Labels {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219336b8-edcc-42e2-9833-d6ed970aa942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for cat, num, label in train_set:\n",
    "    print(f'Shape of categorical train: {cat.shape} {cat.dtype}')\n",
    "    print(f'Shape of numerical train: {num.shape} {num.dtype}')\n",
    "    print(f'Shape of label train: {label.shape} {label.dtype}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4799a3c1-048b-4446-9d40-891235eac527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d93869-5d0a-4601-bed3-bc5f9eb4b650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "exemple_model = net = PyTorch.Net()\n",
    "exemple_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "204d7b8d-972d-4200-9ccd-27111a4b72f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b20b8d6-5b0e-47e7-b531-fe14f7d977f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "criterion = PyTorch.FocalLoss()\n",
    "criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f56d634-382c-498a-8297-6909705ef689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8424b2-77b2-4860-8bc2-c326fcc3c3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = PyTorch.EarlyStopping()\n",
    "early_stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd218883-8f1c-48ef-b0dc-f243f1773778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cross Validation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d71f9db1-12f5-4341-bae0-eaa269413b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27a368d-70ac-43c5-ae2b-9dd92073f4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PyTorch.PyTorchFlow(\n",
    "    trainset = train_set, \n",
    "    l1 = 256,\n",
    "    l2 = 128,\n",
    "    l3 = 64,\n",
    "    dropout_rate = 0.5,\n",
    "    num_workers = 2,\n",
    "    batch_size = 128,\n",
    "    lr = 1e-3,\n",
    "    weight_decay = 1e-5,\n",
    "    max_epochs = 200,\n",
    "    early_stopping_p = 15,\n",
    "    early_stopping_mode = 'max',\n",
    "    save_path_model = '/best_model.pt',\n",
    "    k_fold = 5,\n",
    "    target_score = 'roc',\n",
    "    seed = 33\n",
    ").CrossValidation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "448ebb6c-8cda-4bbb-a34d-056a63682543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### PyTorch Training Architecture\n",
    "The training process was adapted to the projectâ€™s specific characteristics (limited and imbalanced data) using the following strategies:  \n",
    "\n",
    "#### 1. Cross-Validation for Limited Data  \n",
    "| **Strategy** | **Details** | **Benefit** |  \n",
    "|----------------|--------------|----------------|  \n",
    "| **5-Fold Cross-Validation** | - Training data split into **5 folds**.<br>- Each fold: **4 parts for training** + **1 for validation**. | Maximizes the use of **8120 available training records**, reducing overfitting and evaluation bias. |  \n",
    "\n",
    "---  \n",
    "\n",
    "#### 2. Handling Class Imbalance  \n",
    "| **Technique** | **Implementation** | **Impact** |  \n",
    "|-------------|--------------------|--------------|  \n",
    "| **WeightedRandomSampler** | - Adjusts batch sampling to prioritize the minority class (*churn=1*). | Balances class contributions during training. |  \n",
    "| **Bias-Weight Initialization** | - Output layer bias adjusted according to class distribution (16% *churn* vs. 84% *non-churn*). | Reduces initial bias toward the majority class. |  \n",
    "| **Focal Loss** (Î³=2.0) | - Penalizes errors in minority class examples (*hard samples*). | Focuses on complex *churn* patterns, improving recall without sacrificing precision. |  \n",
    "\n",
    "---  \n",
    "\n",
    "#### 3. Regularization and Training Stability \n",
    "| **Component** | **Configuration** | **Objective** |  \n",
    "|----------------|-------------------|---------------|  \n",
    "| **BatchNorm** | - Applied to intermediate dense layers.<br>- Batch size â‰¥ 64. | Reduces scale dependency and accelerates convergence for asymmetric variables (e.g., `total_trans_amt`). |  \n",
    "| **Dropout** (p = 0.5) | - Hidden layers after BatchNorm. | Forces the network to learn redundant patterns, preventing overfitting. |  \n",
    "| **AdamW** | - *Weight decay* = 5e-4. | Decouples L2 regularization from gradient adaptation, improving generalization. |  \n",
    "\n",
    "---  \n",
    "\n",
    "#### 4. Learning Rate Management  \n",
    "| **Scheduler** | **Behavior** | **Advantage** |  \n",
    "|---------------|--------------------|---------------|  \n",
    "| **LinearLR** | - Increases learning rate from 1% â†’ 100% over **100 iterations**. | Smoothens training initiation, avoiding abrupt oscillations. |  \n",
    "| **ReduceLROnPlateau** | - Halves the learning rate after **5 epochs without validation AUC-ROC improvement**. | Dynamically adapts the learning rate to performance plateaus. |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00c47225-45b9-4832-a23f-3e1fbea438b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 5 - Evaluation  \n",
    "In this stage, I analyze the comparative performance of **classical machine learning models** and the **PyTorch neural model**, focusing on technical and business criteria. I will evaluate and compare the top 3 models based on their performance during **cross-validation**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance  \n",
    "| **Model**             | **AUC-ROC** | **Standard Deviation (AUC-ROC)** | **Highlight** |  \n",
    "|-----------------------|-------------|----------------------------------|---------------|  \n",
    "| **Random Forest**      | 98.24%      | Â±0.001276                        | Good generalization and satisfactory AUC-ROC. |  \n",
    "| **Gradient Boosting**  | 99.13%      | Â±0.002317                        | Best AUC-ROC, but higher variance compared to PyTorch. |  \n",
    "| **Neural Network (PyTorch)** | 98.96% | **Â±0.001296**                    | Superior generalization and lower sensitivity to data partitions. |  \n",
    "\n",
    "---\n",
    "\n",
    "### Why Prioritize the PyTorch Model?  \n",
    "1. **Robust Generalization**:  \n",
    "   - Lower standard deviation indicates **consistency across diverse data scenarios**.  \n",
    "   - Techniques like *dropout (p=0.5)* and *L2 regularization* reduce dependency on specific variables.  \n",
    "\n",
    "2. **Adaptability to New Data**:  \n",
    "   - The neural architecture is more effective for **unseen data** (e.g., new clients with unusual patterns), thanks to its ability to learn complex non-linear relationships.  \n",
    "\n",
    "3. **Business Costs**:  \n",
    "   - **False negatives** (failing to identify *churn*) cost **5-7x more** than false positives (estimated cost: R$ 2,000 per lost client vs. R$ 400 in unnecessary offers).  \n",
    "   - PyTorch allows adjustments to *thresholds* and loss functions (e.g., Focal Loss) to prioritize **recall** (capturing a larger share of *churners*).  \n",
    "\n",
    "---\n",
    "\n",
    "### Next Optimization Steps  \n",
    "1. **Hyperparameter Tuning**:  \n",
    "   - Adjust hyperparameters using cross-validation and test these adjustments with unseen test data to verify the modelâ€™s generalization.  \n",
    "   - Low Bias\n",
    "   - Low Variance \n",
    "2. **Recall Prioritization**:  \n",
    "   - Adjust classification thresholds to maximize *churner* detection.  \n",
    "3. **Production Validation**:  \n",
    "   - Monitor business metrics:  \n",
    "     - *Churn* reduction (>15%).  \n",
    "     - Retention campaign ROI (>200%).  \n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion  \n",
    "The choice of PyTorch is justified not only by technical performance but also by **operational flexibility**:  \n",
    "\n",
    "- **Scalability**: Adapts to new variables (e.g., real-time transaction data) without full retraining.  \n",
    "- **Robustness**: The slight difference in AUC-ROC (98.96% vs. 99.13%) is offset by the neural modelâ€™s robustness in dynamic scenarios, aligning with the strategic goal of reducing customer acquisition costs.  \n",
    "\n",
    "---  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980f190f-4562-4062-889a-7616526c1f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_scores = {\n",
    "    'Model': [\n",
    "        'Logistic Regression', 'Decision Tree', 'Random Forest', \n",
    "        'KNeighbors', 'Suport Vector Machine', 'Gradient Boosting', 'Pytorch'\n",
    "    ], \n",
    "    'AUC-ROC Validation': [0.9231, 0.94562, 0.9824, 0.8980, 0.9643, 0.9913, 0.9902], \n",
    "    'Standard Deviation': [0.012404, 0.005276, 0.001276, 0.012466,0.003249, 0.002317, 0.002035], \n",
    "}\n",
    "df_scores = pd.DataFrame(data_scores)\n",
    "df_scores.sort_values('AUC-ROC Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddb91f9-439c-4a71-aee6-36607f980218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(df_scores.sort_values('AUC-ROC Validation')).models_performance_barplots(models_col = 'Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce01ab70-5c34-4bb3-8b54-3cad7cb39631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hypertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69390a6f-32de-4720-a331-77b8f246443d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PyTorch.PyTorchFlow(\n",
    "    trainset = train_set, \n",
    "    dropout_rate = 0.5,\n",
    "    num_workers = 2,\n",
    "    max_epochs = 200,\n",
    "    early_stopping_p = 15,\n",
    "    early_stopping_mode = 'max',\n",
    "    k_fold = 5,\n",
    "    target_score = 'roc',\n",
    "    seed = 33\n",
    ").HyperTunning(n_samples = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a862c742-0f24-4995-815d-e5d5b4d251d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e42997d-9926-4e57-81ad-d1d2cd287838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PyTorch.PyTorchFlow(\n",
    "    trainset = train_set, \n",
    "    l1 = 128,\n",
    "    l2 = 128,\n",
    "    l3 = 64,\n",
    "    dropout_rate = 0.5,\n",
    "    num_workers = 2,\n",
    "    batch_size = 256,\n",
    "    lr = 0.009832484484875212,\n",
    "    weight_decay = 1e-5,\n",
    "    max_epochs = 200,\n",
    "    early_stopping_p = 15,\n",
    "    early_stopping_mode = 'max',\n",
    "    save_path_model = '/main_final_model.pt',\n",
    "    target_score = 'roc',\n",
    "    seed = 33\n",
    ").FinalTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1eceae5-fc65-42da-8a21-f9ce93065ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Final Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271f13b0-d36b-4a1a-96e0-21d2125620d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading Net\n",
    "net = PyTorch.Net(l1 = 128, l2 = 128, l3 = 64, prior_minoritary_class = 0.16071428571428573)\n",
    "net.load_state_dict(torch.load('/main_final_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c2e263-b987-4b9c-b413-3df3af287720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salving the model\n",
    "torch.save(net.state_dict(), '/main_final_model.pth')\n",
    "print('Saved Pytorch Model State to main_final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dacbb098-ead7-46f4-9730-75cd4c46fe63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing Test data\n",
    "preprocessed_test = pytorch_preprocessor.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696e563b-5459-4965-9624-8377a16c122b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salving the pipeline of preprocessing\n",
    "with open('/pytorch_preprocessor.pkl', 'wb') as fp:\n",
    "    pickle.dump(pytorch_preprocessor, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431a7409-be1e-4865-9502-8b2fd25398af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Datset Pytoch\n",
    "test_set = PyTorch.Dataset(\n",
    "    dataset = preprocessed_test,\n",
    "    cat_idx = [0, 5],\n",
    "    num_idx = [5, 18],\n",
    "    label_idx = [18, 19],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b0363f-4517-4efb-b8ae-7e3ab0d6f81b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical, numerical, label = test_set[0]\n",
    "print(f'The Categorical tensor {categorical}')\n",
    "print(f'\\nThe Numerical tensor {numerical}')\n",
    "print(f'\\nThe Labels {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4116409f-184f-480e-935d-1a1b951f0681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds, labels = PyTorch.PyTorchFlow(\n",
    "    testset = test_set, \n",
    ").FinalTest(net = net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8629c27d-051f-48ea-b6ab-93a1b65d3976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metric_collection = MetricCollection([\n",
    "    BinaryAUROC(), \n",
    "    BinaryAccuracy(),\n",
    "    BinaryRecall(), \n",
    "    BinaryF1Score(),\n",
    "    BinaryPrecision(),\n",
    "])\n",
    "metric_collection(preds, labels)\n",
    "plt.rc('font', size = 12)\n",
    "fig, ax= plt.subplots(figsize = (10, 4))\n",
    "fig_ax_ = metric_collection.plot(together=True, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0a0e76d-52ab-4e02-b460-7bca859d4dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1633ca6-e8dc-4cec-ae61-e8f797d3b89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_preds = preds.flatten().tolist()\n",
    "list_labels = labels.flatten().tolist()\n",
    "data_preds = {\n",
    "    'Predictions': list_preds,\n",
    "    'Labels': list_labels,\n",
    "}\n",
    "df_preds = pd.DataFrame(data_preds)\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efa59a7-ef98-4ec0-a1f9-ddd50089ce65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data collect\n",
    "list_preds = preds.flatten().tolist()\n",
    "list_labels = labels.flatten().tolist()\n",
    "data_preds = {\n",
    "    'Predictions': list_preds,\n",
    "    'Labels': list_labels,\n",
    "}\n",
    "df_preds = pd.DataFrame(data_preds)\n",
    "df_preds.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2d694e-14d3-4394-a26e-3465a2b29e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Organizing data and renaming labels\n",
    "data_ax = df_preds\n",
    "data_ax['Labels'] = data_ax['Labels'].map({0: 'No Churn', 1: 'Churn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab05bc85-56c4-48be-b8b5-d2fb7595dc60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData(data_ax).plot_kde_predictions(\n",
    "    predictions = 'Predictions',\n",
    "    labels = 'Labels',\n",
    "    title = 'Prediction Probabilities - Churn and Non-Churn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bdf0ad-db7c-48a9-a9e4-af0446fb2915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GraphicsData.plot_roc_pr_curves(preds = preds, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f780abce-319d-423a-8241-d0c3408ca628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Models Aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8776ef22-af66-4c54-9371-e30dfe2201fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Precison Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acb8dd1-2d6f-441c-8567-8ab870469648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PyTorch.PyTorchFlow(\n",
    "    trainset = train_set, \n",
    "    l1 = 128,\n",
    "    l2 = 128,\n",
    "    l3 = 64,\n",
    "    dropout_rate = 0.5,\n",
    "    num_workers = 2,\n",
    "    batch_size = 256,\n",
    "    lr = 0.009832484484875212,\n",
    "    weight_decay = 1e-5,\n",
    "    max_epochs = 200,\n",
    "    early_stopping_p = 15,\n",
    "    early_stopping_mode = 'max',\n",
    "    save_path_model = '/main_final_model.pt',\n",
    "    target_score = 'precision',\n",
    "    seed = 33\n",
    ").FinalTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb7764b2-2f2b-46f8-82bb-75d14c478516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### NPV Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b83f169-324e-432e-b1f2-597cd92e8415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PyTorch.PyTorchFlow(\n",
    "    trainset = train_set, \n",
    "    l1 = 128,\n",
    "    l2 = 128,\n",
    "    l3 = 64,\n",
    "    dropout_rate = 0.5,\n",
    "    num_workers = 2,\n",
    "    batch_size = 256,\n",
    "    lr = 0.009832484484875212,\n",
    "    weight_decay = 1e-5,\n",
    "    max_epochs = 200,\n",
    "    early_stopping_p = 15,\n",
    "    early_stopping_mode = 'max',\n",
    "    save_path_model = '/main_final_model.pt',\n",
    "    target_score = 'npv',\n",
    "    seed = 33\n",
    ").FinalTraining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8009fe-35aa-451b-8eea-dd12cd25b946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6 - Deployment  \n",
    "\n",
    "- In this step, I will be developing a classifier based on the statistics and analyses obtained through EDA and with this information the classifier will be returning the probability of the customer becoming a churn and their characteristics to help the user make the appropriate decision about a decision when it comes to retaining this customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba23f28-d104-4415-9deb-9f671897bbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e5ca18-275c-42a4-abb8-566043a98400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Avg Utilization Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f049e92f-a102-4328-bc8e-1cd60e11d047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('churn_target')['avg_utilization_ratio'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "747d2b96-4157-4b22-b1a3-e39e9b2564af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Total Revolving Bal Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44d643b-70e1-4f66-a398-08c2d7121868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('churn_target')['total_revolving_bal'].mean().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9868da9-cd54-431f-8ac2-1307394b87b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Total Relationship Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6e8c0b-16b5-476e-935d-5837ae58a42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('churn_target')['total_relationship_count'].mean().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1462ad48-2392-4839-be39-16a1e1b40cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Contacts Count 12 Months Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991de5ce-9dd3-4296-b59e-20832d3ac735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('churn_target')['contacts_count_12_mon'].mean().round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83db4e7-579a-4792-9779-1cb27b00e83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Total Transfer Amount Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d3e5ba-a8b8-468b-a47e-94edd2278386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('churn_target')['total_trans_amt'].mean().round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c12208-739b-4c9e-9e77-86f5a429df31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Total Transaction Count Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89e7ff2-5667-4182-8bc0-b05fb60a1907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train.groupby('churn_target')['total_trans_ct'].mean().round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b20deb0-1fdf-4711-900c-795b8f498bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daed70f3-9a27-40e3-883a-e836665a4075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client_to_classify = test.drop(columns = 'churn_target')[10:11]\n",
    "client_to_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3103a50-d5ac-435e-a1de-9489884af807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## -- Final Classifier --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fd2b4a-ac9e-42db-8766-8cc01dfdf1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def final_classifier(client_to_classify, device = 'cpu'):\n",
    "\n",
    "    # Secure copy of data (avoids altering the original)\n",
    "    client_data = client_to_classify.copy()\n",
    "\n",
    "    # Add target column just for pipeline compatibility (will not be used)\n",
    "    if 'churn_target' not in client_data.columns:\n",
    "        client_data['churn_target'] = None  # dummy\n",
    "\n",
    "    # Loading preprocessing pipeline\n",
    "    with open('/pytorch_preprocessor.pkl', 'rb') as fp:\n",
    "        preprocessor_loaded = pickle.load(fp)\n",
    "\n",
    "    # Pre- porcessing\n",
    "    data_preprocessed = preprocessor_loaded.transform(client_data)\n",
    "\n",
    "    # Separation of categorical and numerical data\n",
    "    cat_input = torch.from_numpy(data_preprocessed[:, 0:5].astype(np.int64)).to(device)\n",
    "    num_input = torch.from_numpy(data_preprocessed[:, 5:18].astype(np.float32)).to(device)\n",
    "\n",
    "    # Loading the trained network\n",
    "    net = PyTorch.Net(l1 = 128, l2 = 128, l3 = 64, prior_minoritary_class = 0.16071428571428573).to(device)\n",
    "    net.load_state_dict(torch.load('/main_final_model.pt', map_location = device))\n",
    "    net.eval()\n",
    "\n",
    "    # Prediction\n",
    "    with torch.no_grad():\n",
    "        pred = net(cat_input, num_input)\n",
    "        prob = torch.sigmoid(pred).item()\n",
    "\n",
    "    # Decision threshold\n",
    "    threshold = 0.5\n",
    "    binary_pred = int(prob >= threshold)\n",
    "\n",
    "    # Classification Output\n",
    "    if binary_pred == 1:\n",
    "        print('\\nThis customer has been classified as a:\\nâ›” Potential Churner.')\n",
    "    else:\n",
    "        print('\\nThis customer has been classified as a:\\nâœ… No-Churner.')\n",
    "\n",
    "    print(f'ðŸ“Š With a rate of: [{prob * 100:.2f}%] Chance Of Churning.')\n",
    "\n",
    "    print('\\nðŸŽ¯We have some indicators for this customer that we can work on to prevent churn:')\n",
    "\n",
    "    # Interpretable indicators and analyses\n",
    "    avg_utilization_ratio = client_data['avg_utilization_ratio'].item()\n",
    "    print(f'\\nAverage credit card usage in the last 12 months: {avg_utilization_ratio * 100:.2f}%')\n",
    "    if avg_utilization_ratio <= 0.162929:\n",
    "        print('ðŸ”´ This customer has very low credit card usage. Recommend increasing above 16.30%.')\n",
    "    else:\n",
    "        print('ðŸŸ¢ Good credit card usage (GREATER THAN 16.30%).')\n",
    "\n",
    "    total_revolving_bal = client_data['total_revolving_bal'].item()\n",
    "    print(f'\\nRevolving balance: {total_revolving_bal}')\n",
    "    if total_revolving_bal <= 684.0:\n",
    "        print('ðŸ”´ Low revolving balance. Recommend increasing above 684.')\n",
    "    else:\n",
    "        print('ðŸŸ¢ Good revolving balance (GREATER THAN 684).')\n",
    "\n",
    "    total_relationship_count = client_data['total_relationship_count'].item()\n",
    "    print(f'\\nTotal products/services: {total_relationship_count}')\n",
    "    if total_relationship_count <= 3:\n",
    "        print('ðŸ”´ Less than 4 products/services. Recommend cross-sell to increase engagement.')\n",
    "    else:\n",
    "        print('ðŸŸ¢ Customer has more than 3 services. Positive indicator.')\n",
    "\n",
    "    total_trans_amt = client_data['total_trans_amt'].item()\n",
    "    print(f'\\nTotal transaction amount (12 months): {total_trans_amt}')\n",
    "    if total_trans_amt <= 3116.0:\n",
    "        print('ðŸ”´ Low transaction volume. Recommend incentive campaigns.')\n",
    "    else:\n",
    "        print('ðŸŸ¢ Healthy transaction amount (GREATER THAN 3116).')\n",
    "\n",
    "    total_trans_ct = client_data['total_trans_ct'].item()\n",
    "    print(f'\\nNumber of transactions (12 months): {total_trans_ct}')\n",
    "    if total_trans_ct <= 45:\n",
    "        print('ðŸ”´ Low activity. Recommend engaging offers to increase usage.')\n",
    "    else:\n",
    "        print('ðŸŸ¢ Active customer (GREATER THAN 45 transactions).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b81c791-9843-489c-9985-4ac7de65556a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_classifier(client_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c112e61-9c96-4f92-ac4a-bcf174b3058f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
